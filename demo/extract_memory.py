"""è®°å¿†æå–è„šæœ¬ - ä»å¯¹è¯æ•°æ®ä¸­æå– MemCell å’Œä¸ªäºº Profile

æœ¬è„šæœ¬æ”¯æŒä¸‰ç§è¿è¡Œæ¨¡å¼ï¼š
1. EXTRACT_ALL: å®Œæ•´æå–ï¼ˆMemCell + Profileï¼‰
2. EXTRACT_MEMCELL_ONLY: ä»…æå– MemCell
3. EXTRACT_PROFILE_ONLY: ä»…æå– Profileï¼ˆä»å·²æœ‰ MemCellï¼‰

ä¸»è¦åŠŸèƒ½ï¼š
- æ¶ˆæ¯å½’ä¸€åŒ–å’Œè¿‡æ»¤
- åŸºäº LLM çš„å¯¹è¯è¾¹ç•Œæ£€æµ‹
- MemCell æå–å’ŒæŒä¹…åŒ–åˆ° MongoDB
- åå°å¼‚æ­¥èšç±»
- ä¸ªäºº Profile æå–ï¼ˆä»ç¾¤ç»„ MemCell ä¸­ä¸ºæ¯ä¸ªå‚ä¸è€…æå–ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python extract_memory.py
"""

import asyncio
import time
import json
import os
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from dataclasses import dataclass

# ç¡®ä¿ src åŒ…å¯è¢«å‘ç°
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT / "src") not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT / "src"))
sys.path.insert(0, str(PROJECT_ROOT))

from memory_layer.memcell_extractor.base_memcell_extractor import RawData
from memory_layer.memcell_extractor.conv_memcell_extractor import (
    ConvMemCellExtractor,
    ConversationMemCellExtractRequest,
)
from src.memory_layer.llm.llm_provider import LLMProvider
from src.common_utils.datetime_utils import from_iso_format, get_now_with_timezone
from src.infra_layer.adapters.out.persistence.document.memory.memcell import (
    MemCell as DocMemCell,
    DataTypeEnum,
)
from src.memory_layer.memory_extractor.profile_memory_extractor import (
    ProfileMemoryExtractor,
    ProfileMemoryExtractRequest,
)
from demo.profile_extractor import ValueDiscriminatorCompanion, DiscriminatorCompanionConfig
from beanie import init_beanie
from motor.motor_asyncio import AsyncIOMotorClient
from dotenv import load_dotenv

# å¯¼å…¥å…±äº«é…ç½®å’Œå·¥å…·
from demo.memory_config import (
    RunMode,
    ScenarioType,
    ExtractModeConfig,
    LLMConfig,
    MongoDBConfig,
)
from demo.memory_utils import (
    ensure_mongo_beanie_ready,
    serialize_datetime,
    PerformanceMetrics,
    ProgressTracker,
    BatchMongoWriter,
)

load_dotenv()

# ============================================================================
# å…¨å±€æ€§èƒ½æŒ‡æ ‡ï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰
# ============================================================================

# å…¨å±€æ€§èƒ½æŒ‡æ ‡å¯¹è±¡
PERF_METRICS = PerformanceMetrics()


# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================

# ğŸ’¡ å¿«é€Ÿåˆ‡æ¢è¿è¡Œæ¨¡å¼ï¼š
# - RunMode.EXTRACT_ALL: å®Œæ•´æå–ï¼ˆMemCell + Profileï¼‰
# - RunMode.EXTRACT_MEMCELL_ONLY: ä»…æå– MemCell
# - RunMode.EXTRACT_PROFILE_ONLY: ä»…æå– Profileï¼ˆä»å·²æœ‰ MemCellï¼‰

# CURRENT_RUN_MODE = RunMode.EXTRACT_PROFILE_ONLY  # âœ… å®Œæ•´æå–ï¼šMemCell + Profile
CURRENT_RUN_MODE = RunMode.EXTRACT_ALL  # âœ… å®Œæ•´æå–ï¼šMemCell + Profile

# æå–é…ç½®
EXTRACT_CONFIG = ExtractModeConfig(
    scenario_type=ScenarioType.GROUP_CHAT,
    # scenario_type=ScenarioType.ASSISTANT,
    enable_profile_extraction=True,
)

# LLM é…ç½®
LLM_CONFIG = LLMConfig()

# MongoDB é…ç½®
MONGO_CONFIG = MongoDBConfig()


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================


def estimate_memcell_tokens(memcell) -> int:
    """ä¼°ç®—å•ä¸ª MemCell çš„ Token æ•°é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰

    ä¼°ç®—é€»è¾‘ï¼š
    - è‹±æ–‡ï¼šçº¦ 1 token = 4 å­—ç¬¦
    - ä¸­æ–‡ï¼šçº¦ 1 token = 1.5 å­—ç¬¦
    - æ ¼å¼åŒ–å¼€é”€ï¼šæ¯æ¡æ¶ˆæ¯é¢å¤– 20 tokens

    Args:
        memcell: MemCell å¯¹è±¡

    Returns:
        ä¼°ç®—çš„ token æ•°é‡
    """
    try:
        total_chars = 0
        message_count = 0

        # è·å– original_data ä¸­çš„æ¶ˆæ¯åˆ—è¡¨
        original_data = getattr(memcell, 'original_data', None)
        if original_data and isinstance(original_data, list):
            for msg in original_data:
                if isinstance(msg, dict):
                    # ç»Ÿè®¡ content å­—æ®µçš„å­—ç¬¦æ•°
                    content = msg.get('content', '')
                    if content:
                        total_chars += len(str(content))

                    # ç»Ÿè®¡ speaker_name å­—æ®µçš„å­—ç¬¦æ•°
                    speaker_name = msg.get('speaker_name', '')
                    if speaker_name:
                        total_chars += len(str(speaker_name))

                    message_count += 1

        # ç²—ç•¥ä¼°ç®—ï¼šå¹³å‡ 2.5 å­—ç¬¦ = 1 tokenï¼ˆæ··åˆä¸­è‹±æ–‡åœºæ™¯ï¼‰
        # + æ¯æ¡æ¶ˆæ¯çš„æ ¼å¼åŒ–å¼€é”€ï¼ˆæ—¶é—´æˆ³ã€conversation_id ç­‰ï¼‰
        estimated_tokens = int(total_chars / 2.5) + (message_count * 20)

        return estimated_tokens
    except Exception:
        # å¦‚æœä¼°ç®—å¤±è´¥ï¼Œè¿”å›ä¿å®ˆå€¼
        return 1000


def estimate_total_tokens(memcell_list: List) -> int:
    """ä¼°ç®— MemCell åˆ—è¡¨çš„æ€» Token æ•°é‡

    Args:
        memcell_list: MemCell å¯¹è±¡åˆ—è¡¨

    Returns:
        ä¼°ç®—çš„æ€» token æ•°é‡
    """
    if not memcell_list:
        return 2000  # åªè¿”å›åŸºç¡€ Prompt å¼€é”€

    total_tokens = 0
    for memcell in memcell_list:
        total_tokens += estimate_memcell_tokens(memcell)

    # æ·»åŠ  Prompt æ¨¡æ¿çš„åŸºç¡€å¼€é”€ï¼ˆçº¦ 2000 tokensï¼‰
    total_tokens += 2000

    return total_tokens


def split_memcells_into_batches(
    memcell_list: List, max_batch_size: int, strategy: str = "latest_first"
) -> List[List]:
    """å°† MemCell åˆ—è¡¨åˆ†æ‰¹ï¼Œæ”¯æŒå¤šç§ç­–ç•¥

    Args:
        memcell_list: MemCell å¯¹è±¡åˆ—è¡¨
        max_batch_size: æ¯æ‰¹æœ€å¤§ MemCell æ•°é‡ï¼ˆå¿…é¡» > 0ï¼‰
        strategy: åˆ†æ‰¹ç­–ç•¥
            - "latest_first": æœ€æ–°ä¼˜å…ˆï¼ˆæŒ‰æ—¶é—´æˆ³é™åºï¼Œæœ€æ–°çš„åœ¨å‰ï¼‰
            - "distributed": å‡åŒ€åˆ†å¸ƒï¼ˆå°½é‡è®©æ¯æ‰¹åŒ…å«ä¸åŒæ—¶æœŸçš„å¯¹è¯ï¼‰
            - "sequential": é¡ºåºåˆ†æ‰¹ï¼ˆæŒ‰åŸå§‹é¡ºåºï¼‰

    Returns:
        åˆ†æ‰¹åçš„ MemCell åˆ—è¡¨ï¼ˆäºŒç»´åˆ—è¡¨ï¼‰
    """
    # è¾¹ç•Œæ¡ä»¶æ£€æŸ¥
    if not memcell_list:
        return []

    if max_batch_size <= 0:
        raise ValueError(f"max_batch_size å¿…é¡» > 0ï¼Œå½“å‰å€¼: {max_batch_size}")

    # å¦‚æœæ•°é‡ä¸è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥è¿”å›
    if len(memcell_list) <= max_batch_size:
        return [memcell_list]

    # æ ¹æ®ç­–ç•¥æ’åº
    sorted_memcells = list(memcell_list)  # å¤åˆ¶åˆ—è¡¨ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®

    if strategy == "latest_first":
        # æŒ‰æ—¶é—´æˆ³é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        sorted_memcells.sort(key=lambda mc: getattr(mc, 'timestamp', ''), reverse=True)
    elif strategy == "distributed":
        # å…ˆæŒ‰æ—¶é—´æ’åºï¼Œç„¶åäº¤é”™åˆ†é…
        sorted_memcells.sort(key=lambda mc: getattr(mc, 'timestamp', ''))
        # ä½¿ç”¨è½®è¯¢æ–¹å¼åˆ†é…åˆ°å„æ‰¹æ¬¡ï¼Œç¡®ä¿å‡åŒ€åˆ†å¸ƒ
        total_batches = (len(sorted_memcells) + max_batch_size - 1) // max_batch_size
        distributed_batches = [[] for _ in range(total_batches)]
        for idx, memcell in enumerate(sorted_memcells):
            batch_idx = idx % total_batches
            distributed_batches[batch_idx].append(memcell)
        return [batch for batch in distributed_batches if batch]
    # strategy == "sequential" æ—¶ä¸æ’åºï¼Œä¿æŒåŸå§‹é¡ºåº

    # é¡ºåºåˆ†æ‰¹ï¼ˆlatest_first å’Œ sequential ç­–ç•¥ï¼‰
    batches = []
    for i in range(0, len(sorted_memcells), max_batch_size):
        batch = sorted_memcells[i : i + max_batch_size]
        batches.append(batch)

    return batches


def ensure_output_dir(output_dir: Path, clean_previous: bool = True) -> None:
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼Œå¯é€‰æ‹©æ¸…ç†ä¹‹å‰çš„ç»“æœ

    Args:
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        clean_previous: æ˜¯å¦æ¸…ç†ä¹‹å‰çš„ç»“æœæ–‡ä»¶
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    if clean_previous:
        for existing_file in output_dir.glob("memcell_*.json"):
            existing_file.unlink()


def load_events(path: Path) -> List[Dict[str, Any]]:
    """ä» JSON æ–‡ä»¶åŠ è½½å¯¹è¯äº‹ä»¶åˆ—è¡¨å¹¶è§„èŒƒåŒ–æ ¼å¼

    Args:
        path: JSON æ–‡ä»¶è·¯å¾„

    Returns:
        å¯¹è¯äº‹ä»¶å­—å…¸åˆ—è¡¨
    """
    with path.open("r", encoding="utf-8") as fp:
        data = json.load(fp)

    if isinstance(data, dict):
        conversation_list = data.get("conversation_list")
        if conversation_list is not None:
            if isinstance(conversation_list, list):
                return conversation_list
            raise ValueError("`conversation_list` å­—æ®µå¿…é¡»ä¸ºæ•°ç»„ï¼Œè¯·æ£€æŸ¥è¾“å…¥ JSONã€‚")

    if isinstance(data, list):
        return data

    raise ValueError(
        "ä¸æ”¯æŒçš„ç¾¤èŠæ•°æ®æ ¼å¼ï¼Œè¯·æä¾›åŒ…å« `conversation_list` çš„å¯¹è±¡æˆ–æ¶ˆæ¯åˆ—è¡¨ã€‚"
    )


def load_memcells_from_files(input_dir: Path) -> List[Any]:
    """ä» JSON æ–‡ä»¶åŠ è½½å·²æœ‰çš„ MemCell åˆ—è¡¨

    Args:
        input_dir: MemCell æ–‡ä»¶æ‰€åœ¨ç›®å½•

    Returns:
        MemCell å¯¹è±¡åˆ—è¡¨ï¼ˆåŒ…è£…ä¸ºç®€å•å¯¹è±¡ä»¥æ”¯æŒå±æ€§è®¿é—®ï¼‰
    """

    class MemCellWrapper:
        """ç®€å•çš„ MemCell åŒ…è£…ç±»ï¼Œæ”¯æŒå±æ€§è®¿é—®"""

        def __init__(self, data: dict):
            self._data = data
            # å°†å­—å…¸çš„é”®ä½œä¸ºå±æ€§
            for key, value in data.items():
                setattr(self, key, value)

        def __getattr__(self, name):
            return self._data.get(name)

    memcells = []
    memcell_files = sorted(input_dir.glob("memcell_*.json"))

    if not memcell_files:
        print(f"[LoadMemCell] âš ï¸ æœªæ‰¾åˆ° MemCell æ–‡ä»¶: {input_dir}")
        return []

    print(f"[LoadMemCell] å‘ç° {len(memcell_files)} ä¸ª MemCell æ–‡ä»¶")

    for file_path in memcell_files:
        try:
            with file_path.open("r", encoding="utf-8") as fp:
                memcell_data = json.load(fp)
                # åŒ…è£…ä¸ºå¯¹è±¡ä»¥æ”¯æŒå±æ€§è®¿é—®
                memcell_obj = MemCellWrapper(memcell_data)
                memcells.append(memcell_obj)
        except Exception as e:
            print(f"[LoadMemCell] âš ï¸ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            continue

    print(f"[LoadMemCell] âœ… æˆåŠŸåŠ è½½ {len(memcells)} ä¸ª MemCell")
    return memcells


def _is_supported_msg_type(entry: Dict[str, Any], supported_types: set) -> bool:
    """æ£€æŸ¥æ¶ˆæ¯ç±»å‹æ˜¯å¦æ”¯æŒ

    Args:
        entry: åŸå§‹æ¶ˆæ¯æ¡ç›®
        supported_types: æ”¯æŒçš„æ¶ˆæ¯ç±»å‹é›†åˆ

    Returns:
        æ˜¯å¦æ”¯æŒè¯¥æ¶ˆæ¯ç±»å‹
    """
    raw_msg_type = entry.get("type")
    if raw_msg_type is None:
        return True

    raw_msg_type = str(raw_msg_type).strip().lower()
    if not raw_msg_type:
        return True

    if supported_types is None:
        return True

    normalized_supported = {str(item).strip().lower() for item in supported_types}
    return raw_msg_type in normalized_supported


def normalize_entry(entry: Dict[str, Any]) -> Dict[str, Any] | None:
    """å°†åŸå§‹äº‹ä»¶ç»“æ„è½¬æ¢ä¸º ConvMemCellExtractor å‹å¥½çš„æ ¼å¼

    Args:
        entry: åŸå§‹æ¶ˆæ¯æ¡ç›®

    Returns:
        å½’ä¸€åŒ–åçš„æ¶ˆæ¯ payloadï¼Œå¦‚æœè§£æå¤±è´¥è¿”å› None
    """
    # 1. æå–å¹¶è§£ææ—¶é—´æˆ³ï¼ˆå…¼å®¹å¤šç§å­—æ®µåï¼‰
    timestamp = (
        entry.get("create_time")
        or entry.get("createTime")  # é©¼å³°å¼
        or entry.get("timestamp")
        or entry.get("created_at")
    )
    if timestamp is None:
        return None

    if isinstance(timestamp, str):
        try:
            timestamp_dt = from_iso_format(timestamp)
        except Exception:
            return None
    else:
        return None

    # 2. æå–å‘è¨€è€…åç§°ï¼ˆå…¼å®¹å¤šç§æ¥æºï¼‰
    speaker_name = entry.get("sender_name") or entry.get("sender")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä» origin å­—æ®µä¸­è·å–
    if not speaker_name:
        origin = entry.get("origin")
        if isinstance(origin, dict):
            speaker_name = origin.get("fullName") or origin.get("full_name")

    if not speaker_name:
        return None
    speaker_name = str(speaker_name)

    # 3. æå–å‘è¨€è€… IDï¼ˆå…¼å®¹å¤šç§å­—æ®µåï¼Œä¼˜å…ˆçº§ï¼šorigin.createBy > sender_id > senderï¼‰
    raw_speaker_id = None

    # ä¼˜å…ˆä» origin ä¸­è·å–ï¼ˆè¿™æ˜¯çœŸå®çš„ç”¨æˆ·IDï¼‰
    origin = entry.get("origin")
    if isinstance(origin, dict):
        raw_speaker_id = origin.get("createBy") or origin.get("create_by")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå†ä»é¡¶å±‚å­—æ®µè·å–
    if not raw_speaker_id:
        raw_speaker_id = entry.get("sender_id") or entry.get("sender")

    speaker_id = str(raw_speaker_id) if raw_speaker_id is not None else ""

    # 4. æå–æ¶ˆæ¯å†…å®¹
    content = entry.get("content", "")
    content = str(content)

    # 5. æå– @ æåŠåˆ—è¡¨
    refer_entries = (
        entry.get("refer_list")
        or entry.get("referList")
        or entry.get("references")
        or []
    )
    refer_list: List[dict] = []
    for refer in refer_entries:
        if isinstance(refer, dict):
            refer_id = (
                refer.get("message_id")
                or refer.get("id")
                or refer.get("_id")
                or refer.get("refer_id")
            )
            name = (
                refer.get("sender_name")
                or refer.get("name")
                or refer.get("full_name")
                or refer.get("display_name")
            )
            if not refer_id or not name:
                continue
            refer_list.append({"id": str(refer_id), "name": str(name)})
        elif isinstance(refer, str):
            refer_list.append({"id": refer, "name": refer})

    # 6. æå–æ¶ˆæ¯ç±»å‹å¹¶è½¬æ¢ä¸ºæ•°å­—æ ¼å¼ï¼ˆå…¼å®¹å¤šç§æ¥æºï¼‰
    msg_type = entry.get("type") or entry.get("msgType")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä» origin å­—æ®µä¸­è·å–
    if msg_type is None:
        origin = entry.get("origin")
        if isinstance(origin, dict):
            msg_type = origin.get("msgType") or origin.get("msg_type")

    if msg_type is not None:
        # æ¶ˆæ¯ç±»å‹æ˜ å°„è¡¨ï¼šå°†å­—ç¬¦ä¸²ç±»å‹è½¬æ¢ä¸ºæ•°å­—
        msg_type_mapping = {
            "text": 1,
            "image": 2,
            "file": 3,
            "audio": 4,
            "video": 5,
            "link": 6,
            "system": 7,
        }

        # å¦‚æœå·²ç»æ˜¯æ•°å­—ï¼Œç›´æ¥ä½¿ç”¨
        if isinstance(msg_type, int):
            pass  # ä¿æŒä¸å˜
        else:
            # å…ˆè½¬æ¢ä¸ºå°å†™å­—ç¬¦ä¸²
            msg_type_str = str(msg_type).strip().lower()

            # å¦‚æœæ˜¯æ•°å­—å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºæ•´æ•°ï¼›å¦åˆ™æŸ¥æ‰¾æ˜ å°„è¡¨
            if msg_type_str.isdigit():
                msg_type = int(msg_type_str)
            else:
                msg_type = msg_type_mapping.get(msg_type_str, 1)  # é»˜è®¤ä¸º TEXT

    # 7. æ„å»ºå¹¶è¿”å› payload
    payload: Dict[str, Any] = {
        "speaker_id": speaker_id,
        "speaker_name": speaker_name,
        "content": content,
        "timestamp": timestamp_dt,
        "referList": refer_list,
    }
    if msg_type is not None:
        payload["msgType"] = msg_type

    return payload


def write_memcell_to_file(memcell, index: int, output_dir: Path) -> None:
    """å°† MemCell ä¿å­˜ä¸º JSON æ–‡ä»¶

    Args:
        memcell: è¦ä¿å­˜çš„ MemCell å¯¹è±¡
        index: æ–‡ä»¶ç´¢å¼•ç¼–å·
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    """
    payload = memcell.to_dict()

    # å½’ä¸€åŒ–æšä¸¾ç±»å‹ä¸ºåŸå§‹å€¼
    memcell_type = payload.get("type")
    if memcell_type is not None and hasattr(memcell_type, "value"):
        payload["type"] = memcell_type.value

    # å°† original_data ä¸­çš„ datetime è½¬æ¢ä¸º ISO å­—ç¬¦ä¸²
    if "original_data" in payload and payload["original_data"]:
        for item in payload["original_data"]:
            if isinstance(item, dict) and "timestamp" in item:
                ts = item["timestamp"]
                if hasattr(ts, "isoformat"):
                    item["timestamp"] = ts.isoformat()

    # ä¿å­˜æ–‡ä»¶
    output_path = output_dir / f"memcell_{index:03d}.json"
    with output_path.open("w", encoding="utf-8") as fp:
        json.dump(payload, fp, ensure_ascii=False, indent=2)

    print(f"[Extract] ä¿å­˜ MemCell #{index} â†’ {output_path.name}")


async def _save_memcell_to_mongodb(memcell) -> None:
    """å°† MemCell ä¿å­˜åˆ° MongoDB

    Args:
        memcell: è¦ä¿å­˜çš„ MemCell å¯¹è±¡
    """
    try:
        # è§£ææ—¶é—´æˆ³
        ts = memcell.timestamp
        if isinstance(ts, str):
            ts_dt = from_iso_format(ts)
        elif isinstance(ts, (int, float)):
            tz = get_now_with_timezone().tzinfo
            ts_dt = datetime.fromtimestamp(float(ts), tz=tz)
        else:
            ts_dt = ts or get_now_with_timezone()

        # è·å–ä¸»ç”¨æˆ· ID
        primary_user = (
            memcell.user_id_list[0]
            if getattr(memcell, 'user_id_list', None)
            else "default"
        )

        # åˆ›å»ºæ–‡æ¡£å¹¶æ’å…¥
        doc = DocMemCell(
            user_id=primary_user,
            timestamp=ts_dt,
            summary=memcell.summary or "",
            group_id=getattr(memcell, 'group_id', None),
            participants=getattr(memcell, 'participants', None),
            type=DataTypeEnum.CONVERSATION,
            original_data=memcell.original_data,
            subject=getattr(memcell, 'subject', None),
            keywords=getattr(memcell, 'keywords', None),
            linked_entities=getattr(memcell, 'linked_entities', None),
            episode=getattr(memcell, 'episode', None),
            semantic_memories=getattr(memcell, 'semantic_memories', None),
            extend=getattr(memcell, 'extend', None),
        )
        await doc.insert()
    except Exception as e:
        print(f"[MongoDB] âš ï¸ ä¿å­˜ MemCell å¤±è´¥: {e}")


async def _save_clustering_snapshot(
    extractor: ConvMemCellExtractor, output_dir: Path, group_id: str
) -> None:
    """ä¿å­˜èšç±»å¿«ç…§åˆ°æ–‡ä»¶

    Args:
        extractor: MemCell æå–å™¨ï¼ˆåŒ…å« cluster_workerï¼‰
        output_dir: è¾“å‡ºç›®å½•
        group_id: ç¾¤ç»„ ID
    """
    try:
        # ç»™åå°èšç±» worker çŸ­æš‚çš„å¤„ç†æ—¶é—´
        await asyncio.sleep(0.1)

        # è·å–èšç±»åˆ†é…ç»“æœ
        assignments = extractor.cluster_worker.get_assignments()

        # ä¿å­˜æ€»èšç±»åˆ†é…æ–‡ä»¶
        out_path = output_dir / "cluster_assignments.json"
        with out_path.open("w", encoding="utf-8") as fp:
            json.dump(assignments, fp, ensure_ascii=False, indent=2)

        # ä¿å­˜æ¯ä¸ªç¾¤ç»„çš„èšç±»æ–‡ä»¶
        try:
            extractor.cluster_worker.dump_to_dir(str(output_dir))
        except Exception:
            pass

        # é¢å¤–ï¼šä¿å­˜èšç±»çŠ¶æ€ï¼ˆåŒ…å«ç°‡è´¨å¿ƒå’Œè®¡æ•°ï¼‰ï¼Œä¾¿äºä¸‹æ¬¡æ¢å¤
        try:
            extractor.cluster_worker.dump_state_to_dir(str(output_dir))
        except Exception:
            pass
    except Exception:
        pass


async def save_individual_profile_to_file(
    profile, user_id: str, output_dir: Path
) -> None:
    """ä¿å­˜ä¸ªäºº Profile åˆ°æ–‡ä»¶

    Args:
        profile: Profile Memory å¯¹è±¡
        user_id: ç”¨æˆ· IDï¼ˆå¦‚ "user_101"ï¼‰
        output_dir: è¾“å‡ºç›®å½•
    """
    try:
        # è½¬æ¢ä¸ºå­—å…¸ï¼ˆå¤„ç† timestamp å¯èƒ½æ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µï¼‰
        if hasattr(profile, 'to_dict'):
            try:
                payload = profile.to_dict()
            except (AttributeError, TypeError) as e:
                # å¦‚æœ to_dict() å¤±è´¥ï¼ˆé€šå¸¸æ˜¯å› ä¸º timestamp å·²ç»æ˜¯å­—ç¬¦ä¸²ï¼‰
                # ç›´æ¥ä½¿ç”¨ __dict__ å¹¶æ‰‹åŠ¨å¤„ç†
                error_msg = str(e).lower()
                if 'tzinfo' in error_msg or 'isoformat' in error_msg:
                    # timestamp ç±»å‹é”™è¯¯ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                    payload = profile.__dict__.copy()
                    # ç¡®ä¿ memory_type è½¬æ¢ä¸ºå­—ç¬¦ä¸²å€¼
                    if hasattr(payload.get('memory_type'), 'value'):
                        payload['memory_type'] = payload['memory_type'].value
                    # timestamp å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œä¿æŒä¸å˜
                    # å¦‚æœæ˜¯ datetimeï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    ts = payload.get('timestamp')
                    if ts is not None:
                        if hasattr(ts, 'isoformat'):
                            payload['timestamp'] = ts.isoformat()
                        elif not isinstance(ts, str):
                            # æ—¢ä¸æ˜¯ datetime ä¹Ÿä¸æ˜¯ strï¼Œå°è¯•è½¬æ¢
                            payload['timestamp'] = str(ts)
                else:
                    raise  # å…¶ä»–é”™è¯¯ç»§ç»­æŠ›å‡º
        elif hasattr(profile, '__dict__'):
            payload = profile.__dict__.copy()
            # å¤„ç†æšä¸¾ç±»å‹
            if hasattr(payload.get('memory_type'), 'value'):
                payload['memory_type'] = payload['memory_type'].value
            # å¤„ç† timestamp
            ts = payload.get('timestamp')
            if ts is not None:
                if hasattr(ts, 'isoformat'):
                    payload['timestamp'] = ts.isoformat()
                elif not isinstance(ts, str):
                    payload['timestamp'] = str(ts)
        else:
            payload = {}

        # åºåˆ—åŒ– datetimeï¼ˆæ·»åŠ æ›´å¥½çš„é”™è¯¯å¤„ç†ï¼‰
        try:
            payload = serialize_datetime(payload)
        except Exception as e:
            # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å½“å‰ payload
            # å·²ç»åœ¨ä¸Šé¢å¤„ç†äº† timestampï¼Œè¿™é‡Œé€šå¸¸ä¸ä¼šå‡ºé”™
            pass

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir.mkdir(parents=True, exist_ok=True)

        # æ–‡ä»¶åï¼šprofile_user_101.json
        output_path = output_dir / f"profile_{user_id}.json"

        # ä¿å­˜æ–‡ä»¶ï¼Œä½¿ç”¨ default=str å¤„ç†æ— æ³•åºåˆ—åŒ–çš„å¯¹è±¡
        with output_path.open("w", encoding="utf-8") as fp:
            json.dump(payload, fp, ensure_ascii=False, indent=2, default=str)

        print(f"[Profile] ä¿å­˜ {user_id} â†’ {output_path.name}")
    except Exception as e:
        print(f"[Profile] âŒ ä¿å­˜ {user_id} Profile å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


# ============================================================================
# Profile æå–é€»è¾‘ - ä¸ºæ¯ä¸ªå‚ä¸è€…æå–ä¸ªäºº Profile
# ============================================================================


async def extract_individual_profiles_from_group_memcells(
    memcell_list: List,
    profile_extractor: ProfileMemoryExtractor,
    group_id: str,
    group_name: Optional[str],
    output_dir: Path,
    extract_config: Optional[ExtractModeConfig] = None,
) -> int:
    """ä»ç¾¤ç»„ MemCell ä¸­ä¸ºæ‰€æœ‰å‚ä¸è€…æå–ä¸ªäºº Profileï¼ˆæ™ºèƒ½åˆ†æ‰¹ä¼˜åŒ–ç‰ˆï¼‰

    æ ¸å¿ƒé€»è¾‘ï¼š
    1. éªŒè¯è¾“å…¥ MemCell åˆ—è¡¨
    2. Token é¢„ä¼°å’Œæ™ºèƒ½åˆ†æ‰¹å†³ç­–
    3. åˆ†æ‰¹è°ƒç”¨ LLM æå– Profileï¼ˆæ¯æ‰¹ä½¿ç”¨å¢é‡æ›´æ–°ï¼‰
    4. æ™ºèƒ½åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
    5. ä¸ºæ¯ä¸ªç”¨æˆ·ä¿å­˜ç‹¬ç«‹çš„ profile_user_{user_id}.json

    é‡è¦æ”¹è¿›ï¼š
    - âœ… è‡ªåŠ¨æ£€æµ‹ MemCell æ•°é‡ï¼Œè¶…è¿‡é˜ˆå€¼æ—¶åˆ†æ‰¹å¤„ç†
    - âœ… Token é¢„ä¼°ï¼Œé¿å…è¶…å‡º LLM ä¸Šé™
    - âœ… å¢é‡æ›´æ–°æœºåˆ¶ï¼ˆå‰æ‰¹æ¬¡ç»“æœä½œä¸ºåæ‰¹æ¬¡çš„ old_memory_listï¼‰
    - âœ… æ”¯æŒå¤šç§åˆ†æ‰¹ç­–ç•¥ï¼ˆæœ€æ–°ä¼˜å…ˆ/å‡åŒ€åˆ†å¸ƒ/é¡ºåºåˆ†æ‰¹ï¼‰
    - âœ… è¯¦ç»†çš„è¿›åº¦æŠ¥å‘Šå’Œæ€§èƒ½è¿½è¸ª

    Args:
        memcell_list: ç¾¤ç»„çš„æ‰€æœ‰ MemCellï¼ˆåŒ…å«å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
        profile_extractor: ä¸ªäºº Profile æå–å™¨
        group_id: ç¾¤ç»„ ID
        group_name: ç¾¤ç»„åç§°
        output_dir: è¾“å‡ºç›®å½•
        extract_config: æå–é…ç½®ï¼ˆåŒ…å«åˆ†æ‰¹å‚æ•°ï¼‰

    Returns:
        æˆåŠŸæå–å¹¶ä¿å­˜çš„ Profile æ€»æ•°
    """

    # æ­¥éª¤ 1ï¼šéªŒè¯è¾“å…¥
    if not memcell_list:
        print("[ProfileExtract] âš ï¸ MemCell åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡ Profile æå–")
        return 0

    # æ­¥éª¤ 2ï¼šç»Ÿè®¡æ‰€æœ‰å‚ä¸è€…ï¼ˆç”¨äºéªŒè¯å’ŒæŠ¥å‘Šï¼‰
    all_users = set()
    for memcell in memcell_list:
        participants = getattr(memcell, 'participants', None)
        if participants and isinstance(participants, list):
            all_users.update(participants)

    if not all_users:
        print("[ProfileExtract] âš ï¸ æœªæ‰¾åˆ°å‚ä¸è€…ï¼Œè·³è¿‡ Profile æå–")
        return 0

    print(f"\n[ProfileExtract] ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - MemCell æ•°é‡: {len(memcell_list)}")
    print(f"  - å‚ä¸è€…æ•°é‡: {len(all_users)}")
    print(f"  - å‚ä¸è€…åˆ—è¡¨: {sorted(all_users)}")

    # æ­¥éª¤ 3ï¼šToken é¢„ä¼°å’Œåˆ†æ‰¹å†³ç­–
    # ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if extract_config is None:
        extract_config = EXTRACT_CONFIG

    max_batch_size = extract_config.max_memcells_per_profile_batch
    batch_strategy = extract_config.profile_batch_strategy
    enable_token_estimation = extract_config.enable_token_estimation
    max_estimated_tokens = extract_config.max_estimated_tokens

    # å‚æ•°éªŒè¯
    if max_batch_size <= 0:
        print(f"[ProfileExtract] âš ï¸ æ— æ•ˆçš„æ‰¹æ¬¡å¤§å° {max_batch_size}ï¼Œä½¿ç”¨é»˜è®¤å€¼ 50")
        max_batch_size = 50

    # Token é¢„ä¼°
    if enable_token_estimation:
        estimated_tokens = estimate_total_tokens(memcell_list)
        print(f"[ProfileExtract] ğŸ“ Token é¢„ä¼°: ~{estimated_tokens:,} tokens")

        if estimated_tokens > max_estimated_tokens:
            print(
                f"[ProfileExtract] âš ï¸ é¢„ä¼° Token æ•°è¶…è¿‡é™åˆ¶ ({max_estimated_tokens:,})ï¼Œå¼ºåˆ¶åˆ†æ‰¹å¤„ç†"
            )
            # æ ¹æ® token æ•°åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
            if estimated_tokens > 0:  # é˜²æ­¢é™¤é›¶é”™è¯¯
                adjusted_batch_size = int(
                    max_batch_size * (max_estimated_tokens / estimated_tokens)
                )
                # ç¡®ä¿æ‰¹æ¬¡å¤§å°åœ¨åˆç†èŒƒå›´å†…ï¼ˆæœ€å° 5ï¼Œæœ€å¤§ä¸è¶…è¿‡åŸå§‹å€¼ï¼‰
                adjusted_batch_size = max(5, min(max_batch_size, adjusted_batch_size))
                max_batch_size = adjusted_batch_size
                print(f"[ProfileExtract] ğŸ”§ è°ƒæ•´æ‰¹æ¬¡å¤§å°: {max_batch_size}")

    # æ­¥éª¤ 4ï¼šåˆ†æ‰¹å¤„ç†
    batches = split_memcells_into_batches(
        memcell_list, max_batch_size=max_batch_size, strategy=batch_strategy
    )

    total_batches = len(batches)

    if total_batches == 1:
        print(f"[ProfileExtract] ğŸš€ MemCell æ•°é‡åœ¨é™åˆ¶å†…ï¼Œä½¿ç”¨å•æ‰¹æ¬¡å¤„ç†")
    else:
        print(
            f"[ProfileExtract] ğŸ“¦ å°† {len(memcell_list)} ä¸ª MemCell åˆ†ä¸º {total_batches} æ‰¹å¤„ç†"
        )
        print(f"[ProfileExtract] ğŸ“‹ åˆ†æ‰¹ç­–ç•¥: {batch_strategy}")

    # æ­¥éª¤ 5ï¼šé€æ‰¹æå– Profileï¼ˆä½¿ç”¨å¢é‡æ›´æ–°ï¼‰
    all_profiles_map: Dict[str, Any] = {}  # user_id -> ProfileMemory
    total_llm_time = 0.0

    try:
        for batch_idx, batch_memcells in enumerate(batches, 1):
            batch_start = time.time()

            print(
                f"\n[ProfileExtract] ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx}/{total_batches} ({len(batch_memcells)} ä¸ª MemCell)"
            )

            # Token é¢„ä¼°ï¼ˆä»…å¯¹å½“å‰æ‰¹æ¬¡ï¼‰
            if enable_token_estimation:
                batch_tokens = estimate_total_tokens(batch_memcells)
                print(
                    f"[ProfileExtract]   ğŸ“ æ‰¹æ¬¡ Token é¢„ä¼°: ~{batch_tokens:,} tokens"
                )

            # æ„å»ºæå–è¯·æ±‚ï¼ˆä½¿ç”¨å¢é‡æ›´æ–°ï¼‰
            # old_memory_list åŒ…å«ä¹‹å‰æ‰€æœ‰æ‰¹æ¬¡æå–çš„ Profile
            old_memory_list = list(all_profiles_map.values())

            extract_request = ProfileMemoryExtractRequest(
                memcell_list=batch_memcells,
                user_id_list=[],  # ç©ºåˆ—è¡¨ï¼Œè®© LLM è‡ªåŠ¨è¯†åˆ«å‚ä¸è€…
                group_id=group_id,
                group_name=group_name,
                old_memory_list=old_memory_list,  # å¢é‡æ›´æ–°ï¼šä¼ å…¥ä¹‹å‰æ‰¹æ¬¡çš„ç»“æœ
            )

            # æ‰§è¡Œæå–
            print(f"[ProfileExtract]   ğŸ“¡ è°ƒç”¨ LLM æå– Profile...")
            llm_start = time.time()
            if extract_config.scenario_type == ScenarioType.GROUP_CHAT:
                batch_profile_memories = await profile_extractor.extract_memory(
                    extract_request
                )
            elif extract_config.scenario_type == ScenarioType.ASSISTANT:
                batch_profile_memories = (
                    await profile_extractor.extract_profile_companion(extract_request)
                )
            else:
                raise ValueError(
                    f"Invalid scenario type: {extract_config.scenario_type}"
                )
            llm_elapsed = time.time() - llm_start
            total_llm_time += llm_elapsed

            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            PERF_METRICS.llm_calls += 1
            PERF_METRICS.llm_total_time += llm_elapsed

            print(f"[ProfileExtract]   â±ï¸  LLM è°ƒç”¨è€—æ—¶: {llm_elapsed:.2f}s")

            # éªŒè¯ç»“æœ
            if not batch_profile_memories:
                print(f"[ProfileExtract]   âš ï¸ æ‰¹æ¬¡ {batch_idx} æœªè¿”å› Profile")
                continue

            print(
                f"[ProfileExtract]   âœ… æ‰¹æ¬¡ {batch_idx} è¿”å› {len(batch_profile_memories)} ä¸ª Profile"
            )

            # åˆå¹¶ç»“æœï¼ˆæŒ‰ user_id å»é‡ï¼Œåæ‰¹æ¬¡è¦†ç›–å‰æ‰¹æ¬¡ï¼‰
            for profile in batch_profile_memories:
                user_id = getattr(profile, 'user_id', None)
                if user_id:
                    all_profiles_map[user_id] = profile
                    print(f"[ProfileExtract]     âœ“ æ›´æ–° {user_id} çš„ Profile")

            batch_elapsed = time.time() - batch_start
            print(
                f"[ProfileExtract]   ğŸ“Š æ‰¹æ¬¡ {batch_idx} å®Œæˆï¼Œè€—æ—¶: {batch_elapsed:.2f}s"
            )

        # æ­¥éª¤ 6ï¼šä¿å­˜æ‰€æœ‰ Profile
        print(f"\n[ProfileExtract] ğŸ’¾ ä¿å­˜ Profile åˆ°æ–‡ä»¶...")
        profile_count = 0
        saved_user_ids = []

        for user_id, profile in all_profiles_map.items():
            await save_individual_profile_to_file(
                profile=profile, user_id=user_id, output_dir=output_dir
            )

            profile_count += 1
            saved_user_ids.append(user_id)
            PERF_METRICS.profile_count += 1

            print(f"[ProfileExtract]   âœ“ ä¿å­˜ {user_id} çš„ Profile")

        # æ­¥éª¤ 7ï¼šç”ŸæˆæŠ¥å‘Š
        print(f"\n[ProfileExtract] ğŸ“‹ æå–ç»“æœæ±‡æ€»:")
        print(f"  - å¤„ç†æ‰¹æ¬¡: {total_batches} æ‰¹")
        print(f"  - LLM æ€»è€—æ—¶: {total_llm_time:.2f}s")
        print(f"  - æˆåŠŸæå–: {profile_count}/{len(all_users)} ä¸ª Profile")
        print(f"  - å·²ä¿å­˜ç”¨æˆ·: {sorted(saved_user_ids)}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·æœªè¢«æå–
        missing_users = all_users - set(saved_user_ids)
        if missing_users:
            print(f"  - âš ï¸ æœªæå–åˆ°çš„ç”¨æˆ·: {sorted(missing_users)}")
            print(f"     æç¤º: è¿™äº›ç”¨æˆ·å¯èƒ½åœ¨å¯¹è¯ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯")

        return profile_count

    except Exception as e:
        print(f"[ProfileExtract] âŒ Profile æå–å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return 0


# ============================================================================
# æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ - Extract æ¨¡å¼
# ============================================================================


async def extract_and_dump(
    events: List[Dict[str, Any]],
    extract_config: ExtractModeConfig,
    llm_config: LLMConfig,
    mongo_config: MongoDBConfig,
) -> Dict[str, int]:
    """ä»å¯¹è¯äº‹ä»¶ä¸­æå– MemCell å¹¶ä¿å­˜ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    è¿™æ˜¯ Extract æ¨¡å¼çš„æ ¸å¿ƒå¤„ç†æµç¨‹ï¼š
    1. åˆå§‹åŒ– MongoDB å’Œ Beanie
    2. åˆ›å»º LLM Provider å’Œ MemCell æå–å™¨
    3. æµå¼å¤„ç†å¯¹è¯æ¶ˆæ¯
    4. æ£€æµ‹è¯é¢˜è¾¹ç•Œå¹¶æå– MemCell
    5. è‡ªåŠ¨ç”Ÿæˆæƒ…æ™¯è®°å¿†
    6. æ‰¹é‡ä¿å­˜åˆ° MongoDB å’Œæœ¬åœ° JSONï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰
    7. æäº¤åˆ°åå°èšç±» worker
    8. æ‰¹é‡æå–ä¸ªäºº Profileï¼ˆä»ç¾¤ç»„ MemCellï¼‰
    9. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰

    Args:
        events: åŸå§‹å¯¹è¯äº‹ä»¶åˆ—è¡¨
        extract_config: æå–é…ç½®
        llm_config: LLM é…ç½®
        mongo_config: MongoDB é…ç½®

    Returns:
        åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼ˆsaved_filesã€profile_count ç­‰ï¼‰
    """

    # è®°å½•å¼€å§‹æ—¶é—´
    if extract_config.enable_performance_metrics:
        PERF_METRICS.total_start_time = time.time()

    # åˆå§‹åŒ– MongoDB è¿æ¥
    await ensure_mongo_beanie_ready(mongo_config)

    # åˆ›å»º LLM Provider
    provider = LLMProvider(
        llm_config.provider,
        model=llm_config.model,
        api_key=llm_config.api_key,
        base_url=llm_config.base_url,
        temperature=llm_config.temperature,
        max_tokens=llm_config.max_tokens,
    )

    # åˆ›å»º MemCell æå–å™¨
    extractor = ConvMemCellExtractor(provider)
    # å°è¯•ä»è¾“å‡ºç›®å½•åŠ è½½å†å²èšç±»çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
    try:
        extractor.cluster_worker.load_state_from_dir(str(extract_config.output_dir))
        print(f"[Extract] â™»ï¸ å·²åŠ è½½å†å²èšç±»çŠ¶æ€: {extract_config.output_dir}")
    except Exception:
        pass

    # åˆ›å»ºæ‰¹é‡ MongoDB å†™å…¥å™¨ï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰
    mongo_writer = BatchMongoWriter(
        batch_size=extract_config.mongo_batch_size,
        perf_metrics=(
            PERF_METRICS if extract_config.enable_performance_metrics else None
        ),
    )

    # åˆ›å»º Profile æå–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    profile_extractor = None
    if extract_config.enable_profile_extraction:
        try:
            # ç¾¤èŠ/é™ªä¼´åœºæ™¯å…±ç”¨ Profile æå–å™¨
            profile_extractor = ProfileMemoryExtractor(llm_provider=provider)
            print(f"[Extract] âœ… Profile æå–å™¨å·²åˆ›å»º")
        except Exception as e:
            print(f"[Extract] âš ï¸ Profile æå–å™¨åˆ›å»ºå¤±è´¥: {e}")
            print(f"[Extract] ç»§ç»­æ‰§è¡Œ MemCell æå–ï¼Œè·³è¿‡ Profile æå–")

    history: List[RawData] = []
    saved_files = 0
    profile_count = 0

    # åœ¨çº¿ç”»åƒï¼šæ¯æ¡ MemCell å®Œæˆåè§¦å‘ï¼ˆæ›¿ä»£åŸæ¥çš„æ‰¹é‡æå–ï¼‰
    # ç»´æŠ¤åœ¨çº¿ç”»åƒç¼“å­˜ä¸ç°‡å†… MemCell ç¼“å†²
    # æ³¨æ„ï¼šä¸ºæœ€å°æ”¹åŠ¨ï¼Œä¸å¼•å…¥åˆ¤åˆ«/å»æŠ–ï¼Œåç»­å¯åŠ 
    memcell_batch: List = []  # ä¿ç•™ä½†ä¸ä½¿ç”¨ï¼Œç”¨äºå…¼å®¹é€»è¾‘
    online_profiles_map: Dict[str, Any] = {}
    cluster_to_memcells: Dict[str, List[Any]] = {}
    session_memcells: List[Any] = []

    # Companion åœºæ™¯ä»·å€¼åˆ¤åˆ«å™¨
    companion_disc: Optional[ValueDiscriminatorCompanion] = None
    if (
        extract_config.scenario_type == ScenarioType.ASSISTANT
        and profile_extractor is not None
    ):
        try:
            companion_disc = ValueDiscriminatorCompanion(
                provider, DiscriminatorCompanionConfig(min_confidence=0.6)
            )
        except Exception:
            companion_disc = None

    # åœ¨çº¿ç”»åƒè¾“å‡ºç›®å½•
    profiles_dir_group = extract_config.output_dir / "profiles"
    profiles_dir_group.mkdir(parents=True, exist_ok=True)
    profiles_dir_companion = extract_config.output_dir / "profiles_companion"
    profiles_dir_companion.mkdir(parents=True, exist_ok=True)

    print(f"[Extract] å¼€å§‹å¤„ç† {len(events)} ä¸ªå¯¹è¯äº‹ä»¶...")
    print(f"[Extract] Profile æå–ç­–ç•¥: æ‰€æœ‰ MemCell å®Œæˆåï¼Œä¸ºæ¯ä¸ªå‚ä¸è€…æå–ä¸ªäººç”»åƒ")

    # åˆ›å»ºè¿›åº¦è¿½è¸ªå™¨ï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰
    progress_tracker = None
    if extract_config.enable_progress_bar:
        progress_tracker = ProgressTracker(len(events), "MemCellæå–")

    for idx, entry in enumerate(events):
        # 1. è¿‡æ»¤ä¸æ”¯æŒçš„æ¶ˆæ¯ç±»å‹
        if not _is_supported_msg_type(entry, extract_config.supported_msg_types):
            continue

        # 2. å½’ä¸€åŒ–æ¶ˆæ¯æ ¼å¼
        message_payload = normalize_entry(entry)
        if message_payload is None:
            continue

        message_id = (
            entry.get("message_id")
            or entry.get("id")
            or entry.get("uuid")
            or entry.get("event_id")
        )
        raw_item = RawData(
            content=message_payload,
            data_id=str(message_id or idx),
            data_type=DataTypeEnum.CONVERSATION,
        )

        # 3. ä½¿ç”¨ç¬¬ä¸€æ¡æœ‰æ•ˆæ¶ˆæ¯åˆå§‹åŒ–å†å²è®°å½•
        if not history:
            history.append(raw_item)
            continue

        # 4. æ„å»ºæå–è¯·æ±‚ï¼ˆå•æ¶ˆæ¯æµå¼å¤„ç†ï¼‰
        request = ConversationMemCellExtractRequest(
            history_raw_data_list=list(history),
            new_raw_data_list=[raw_item],
            user_id_list=[],
            group_id=extract_config.group_id,
            smart_mask_flag=True,
        )

        # 5. æ‰§è¡Œ MemCell æå–ï¼ˆå¸¦æ€§èƒ½è¿½è¸ªï¼‰
        try:
            llm_start = time.time()
            memcell, status = await extractor.extract_memcell(
                request,
                use_semantic_extraction=extract_config.enable_semantic_extraction,
            )
            llm_elapsed = time.time() - llm_start

            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            if extract_config.enable_performance_metrics:
                PERF_METRICS.llm_calls += 1
                PERF_METRICS.llm_total_time += llm_elapsed

            should_end = memcell is not None
            should_wait = bool(status.should_wait) if status is not None else False
        except json.JSONDecodeError as e:
            print(f"\n[Extract] âš ï¸ JSON è§£æé”™è¯¯ï¼Œè·³è¿‡å½“å‰æ¶ˆæ¯: {e}")
            history.append(raw_item)
            if len(history) > extract_config.history_window_size:
                history = history[-extract_config.history_window_size :]
            if progress_tracker:
                progress_tracker.update()
            continue
        except Exception as e:
            print(f"\n[Extract] âš ï¸ MemCell æå–å¤±è´¥: {e}")
            history.append(raw_item)
            if len(history) > extract_config.history_window_size:
                history = history[-extract_config.history_window_size :]
            if progress_tracker:
                progress_tracker.update()
            continue

        # 6. æ ¹æ®æå–ç»“æœå¤„ç†
        if should_end:
            # æˆåŠŸæå–åˆ° MemCell
            saved_files += 1

            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            if extract_config.enable_performance_metrics:
                PERF_METRICS.memcell_count += 1

            write_memcell_to_file(memcell, saved_files, extract_config.output_dir)

            # æ‰¹é‡ä¿å­˜åˆ° MongoDBï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            # await mongo_writer.add(memcell)
            await _save_memcell_to_mongodb(memcell)
            # ä¿å­˜èšç±»å¿«ç…§
            await _save_clustering_snapshot(
                extractor, extract_config.output_dir, extract_config.group_id
            )

            # åœ¨çº¿è§¦å‘ç”»åƒï¼ˆæ›¿ä»£åŸæ‰¹é‡æå–ï¼‰
            if extract_config.enable_profile_extraction:
                if profile_extractor is not None:
                    if extract_config.scenario_type == ScenarioType.GROUP_CHAT:
                        # è·å–ç°‡ID
                        assignments = extractor.cluster_worker.get_assignments()
                        gid = extract_config.group_id or "__default__"
                        mapping = (
                            assignments.get(gid, {})
                            if isinstance(assignments, dict)
                            else {}
                        )
                        cluster_id = (
                            mapping.get(str(memcell.event_id))
                            if isinstance(mapping, dict)
                            else None
                        )
                        if not cluster_id:
                            cluster_id = f"cluster_{str(memcell.event_id)[:8]}"
                        # ç´¯ç§¯è¯¥ç°‡çš„ MemCell
                        bucket = cluster_to_memcells.setdefault(cluster_id, [])
                        bucket.append(memcell)

                        # å‘èµ·åœ¨çº¿æå–ï¼ˆä¸å·¥ä½œç”»åƒä¸€è‡´ï¼‰
                        extract_request = ProfileMemoryExtractRequest(
                            memcell_list=bucket,
                            user_id_list=[],
                            group_id=extract_config.group_id,
                            group_name=extract_config.group_name,
                            old_memory_list=(
                                list(online_profiles_map.values())
                                if online_profiles_map
                                else None
                            ),
                        )
                        batch_profile_memories = await profile_extractor.extract_memory(
                            extract_request
                        )
                        if batch_profile_memories:
                            for profile in batch_profile_memories:
                                uid = getattr(profile, 'user_id', None)
                                if not uid:
                                    continue
                                online_profiles_map[uid] = profile
                                await save_individual_profile_to_file(
                                    profile=profile,
                                    user_id=uid,
                                    output_dir=profiles_dir_group,
                                )
                    elif extract_config.scenario_type == ScenarioType.ASSISTANT:
                        # é™ªä¼´åœºæ™¯ï¼šå…ˆåšä»·å€¼åˆ¤åˆ«ï¼ˆæœ€è¿‘ä¸¤æ¡ä¸Šä¸‹æ–‡ï¼‰ï¼Œé«˜ä»·å€¼æ‰è§¦å‘
                        session_memcells.append(memcell)
                        do_extract = True
                        if companion_disc is not None:
                            recent_ctx = (
                                session_memcells[-3:-1]
                                if len(session_memcells) >= 2
                                else []
                            )
                            ok, conf, reason = await companion_disc.is_high_value(
                                memcell, recent_ctx
                            )
                            do_extract = bool(ok)
                        if do_extract:
                            extract_request = ProfileMemoryExtractRequest(
                                memcell_list=list(session_memcells),
                                user_id_list=[],
                                group_id=extract_config.group_id,
                                group_name=extract_config.group_name,
                                old_memory_list=(
                                    list(online_profiles_map.values())
                                    if online_profiles_map
                                    else None
                                ),
                            )
                            batch_profile_memories = (
                                await profile_extractor.extract_profile_companion(
                                    extract_request
                                )
                            )
                            if batch_profile_memories:
                                for profile in batch_profile_memories:
                                    uid = getattr(profile, 'user_id', None)
                                    if not uid:
                                        continue
                                    online_profiles_map[uid] = profile
                                    await save_individual_profile_to_file(
                                        profile=profile,
                                        user_id=uid,
                                        output_dir=profiles_dir_companion,
                                    )

            # é‡ç½®å†å²è®°å½•
            history = [raw_item]

            # æ›´æ–°è¿›åº¦è¿½è¸ª
            if progress_tracker:
                progress_tracker.update()
            continue

        if should_wait:
            # ç­‰å¾…æ›´å¤šæ¶ˆæ¯æ¥åˆ¤æ–­è¯é¢˜è¾¹ç•Œ
            history.append(raw_item)
            if len(history) > extract_config.history_window_size:
                history = history[-extract_config.history_window_size :]
            if progress_tracker:
                progress_tracker.update()
            continue

        # if idx == len(events) - 1:

        # ç»§ç»­å½“å‰è¯é¢˜ï¼Œå¢é•¿å†å²è®°å½•ï¼ˆå¸¦çª—å£é™åˆ¶ï¼‰
        history.append(raw_item)
        if len(history) > extract_config.history_window_size:
            history = history[-extract_config.history_window_size :]
        if progress_tracker:
            progress_tracker.update()

    # 7. åˆ·æ–°å‰©ä½™çš„ MongoDB å†™å…¥ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    await mongo_writer.flush()
    print(f"[Extract] âœ… å†™å…¥å®Œæˆ")
    # await _save_memcell_to_mongodb(memcell_batch)

    # 8. å…è®¸åå° worker å®Œæˆæœ€åçš„å¤„ç†
    await asyncio.sleep(0.5)

    # 8. ä¿å­˜æœ€ç»ˆèšç±»ç»“æœ
    await _save_clustering_snapshot(
        extractor, extract_config.output_dir, extract_config.group_id
    )

    # 9. æ—§çš„æ‰¹é‡ Profile æå–å·²ç”±åœ¨çº¿æ¨¡å¼æ›¿ä»£ï¼›æ­¤å¤„è·³è¿‡
    profile_count = len(online_profiles_map)

    print(f"\n[Extract] âœ… æå–å®Œæˆï¼")
    print(f"  - MemCell: {saved_files} ä¸ª")
    if profile_extractor is not None:
        print(f"  - Profile: {profile_count} ä¸ªï¼ˆä¸ªäººç”»åƒï¼‰")
    print(f"  - è¾“å‡ºç›®å½•: {extract_config.output_dir}")

    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    if extract_config.enable_performance_metrics:
        PERF_METRICS.report()

    return {"saved_files": saved_files, "profile_count": profile_count}


async def extract_profiles_only(
    extract_config: ExtractModeConfig, llm_config: LLMConfig
) -> Dict[str, int]:
    """ä»…æå– Profile æ¨¡å¼ - ä»å·²æœ‰ MemCell ä¸­æå–ä¸ªäºº Profile

    é€‚ç”¨åœºæ™¯ï¼šå·²ç»ç”Ÿæˆäº† MemCell æ–‡ä»¶ï¼Œç°åœ¨éœ€è¦æå– Profile

    Args:
        extract_config: æå–é…ç½®
        llm_config: LLM é…ç½®

    Returns:
        åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    print(f"\n[ExtractProfileOnly] å¼€å§‹ä»å·²æœ‰ MemCell æå–ä¸ªäºº Profile...")
    print(f"  - MemCell æ¥æº: {extract_config.memcell_source}")

    # 1. åŠ è½½å·²æœ‰ MemCell
    if extract_config.memcell_source == "file":
        memcells = load_memcells_from_files(extract_config.memcell_input_dir)
    else:
        print(
            f"[ExtractProfileOnly] âŒ æœªçŸ¥çš„ MemCell æ¥æº: {extract_config.memcell_source}"
        )
        return {"profile_count": 0}

    if not memcells:
        print(f"[ExtractProfileOnly] âŒ æœªåŠ è½½åˆ° MemCellï¼Œé€€å‡º")
        return {"profile_count": 0}

    # 2. åˆ›å»º LLM Provider å’Œ Profile æå–å™¨
    provider = LLMProvider(
        llm_config.provider,
        model=llm_config.model,
        api_key=llm_config.api_key,
        base_url=llm_config.base_url,
        temperature=llm_config.temperature,
        max_tokens=llm_config.max_tokens,
    )

    try:
        profile_extractor = ProfileMemoryExtractor(llm_provider=provider)
        print(f"[ExtractProfileOnly] âœ… Profile æå–å™¨å·²åˆ›å»º")
    except Exception as e:
        print(f"[ExtractProfileOnly] âŒ Profile æå–å™¨åˆ›å»ºå¤±è´¥: {e}")
        return {"profile_count": 0}

    # 3. æå–ä¸ªäºº Profile
    profile_count = await extract_individual_profiles_from_group_memcells(
        memcell_list=memcells,
        profile_extractor=profile_extractor,
        group_id=extract_config.group_id,
        group_name=extract_config.group_name,
        output_dir=extract_config.output_dir,
        extract_config=extract_config,  # ä¼ å…¥é…ç½®ä»¥å¯ç”¨åˆ†æ‰¹ä¼˜åŒ–
    )

    print(f"\n[ExtractProfileOnly] âœ… Profile æå–å®Œæˆ!")
    print(f"  - è¾“å…¥ MemCell: {len(memcells)} ä¸ª")
    print(f"  - æå– Profile: {profile_count} ä¸ªï¼ˆä¸ªäººç”»åƒï¼‰")
    print(f"  - è¾“å‡ºç›®å½•: {extract_config.output_dir}")

    return {"profile_count": profile_count}


# ============================================================================
# ä¸»å…¥å£å‡½æ•°
# ============================================================================


async def run_extract_mode(
    extract_config: ExtractModeConfig,
    llm_config: LLMConfig,
    mongo_config: MongoDBConfig,
) -> None:
    """è¿è¡Œ Extract æ¨¡å¼

    Args:
        extract_config: æå–é…ç½®
        llm_config: LLM é…ç½®
        mongo_config: MongoDB é…ç½®
    """
    # 1. éªŒè¯ LLM API Key
    api_key_present = any(
        [
            llm_config.api_key,
            os.getenv("OPENROUTER_API_KEY"),
            os.getenv("OPENAI_API_KEY"),
        ]
    )
    if not api_key_present:
        print("[Extract] âŒ LLM_API_KEY / OPENROUTER_API_KEY / OPENAI_API_KEY æœªè®¾ç½®")
        print("[Extract] æ— æ³•è¿è¡Œ LLM æå–ï¼Œè¯·é…ç½® API å¯†é’¥")
        return

    # 2. éªŒè¯è¾“å…¥æ–‡ä»¶
    if not extract_config.data_file.exists():
        print(f"[Extract] âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {extract_config.data_file}")
        return

    # 3. å‡†å¤‡è¾“å‡ºç›®å½•
    ensure_output_dir(extract_config.output_dir, clean_previous=True)

    # 4. åŠ è½½å¯¹è¯äº‹ä»¶
    print(f"[Extract] åŠ è½½å¯¹è¯æ•°æ®: {extract_config.data_file}")
    events = load_events(extract_config.data_file)

    # 5. æ‰§è¡Œæå–
    result = await extract_and_dump(events, extract_config, llm_config, mongo_config)
    saved_memcells = result.get("saved_files", 0)
    saved_profiles = result.get("profile_count", 0)

    print(f"\n[Extract] âœ… æå–å®Œæˆï¼")
    print(f"  - ä¿å­˜äº† {saved_memcells} ä¸ª MemCell")
    print(f"  - ä¿å­˜äº† {saved_profiles} ä¸ª Profileï¼ˆä¸ªäººç”»åƒï¼‰")
    print(f"  - è¾“å‡ºç›®å½•: {extract_config.output_dir}")


async def run_extract_profile_only_mode(
    extract_config: ExtractModeConfig, llm_config: LLMConfig
) -> None:
    """è¿è¡Œä»…æå– Profile æ¨¡å¼

    Args:
        extract_config: æå–é…ç½®
        llm_config: LLM é…ç½®
    """
    # éªŒè¯ LLM API Key
    api_key_present = any(
        [
            llm_config.api_key,
            os.getenv("OPENROUTER_API_KEY"),
            os.getenv("OPENAI_API_KEY"),
        ]
    )
    if not api_key_present:
        print("[ExtractProfileOnly] âŒ LLM_API_KEY æœªè®¾ç½®")
        return

    # éªŒè¯è¾“å…¥ç›®å½•ï¼ˆfile æ¨¡å¼ï¼‰
    if (
        extract_config.memcell_source == "file"
        and not extract_config.memcell_input_dir.exists()
    ):
        print(
            f"[ExtractProfileOnly] âŒ MemCell è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {extract_config.memcell_input_dir}"
        )
        return

    # å‡†å¤‡è¾“å‡ºç›®å½•
    extract_config.output_dir.mkdir(parents=True, exist_ok=True)

    # æ‰§è¡Œæå–
    result = await extract_profiles_only(extract_config, llm_config)
    profile_count = result.get("profile_count", 0)

    print(f"\n[ExtractProfileOnly] âœ… å®Œæˆï¼")
    print(f"  - æå–äº† {profile_count} ä¸ª Profileï¼ˆä¸ªäººç”»åƒï¼‰")


def main() -> None:
    """ä¸»å…¥å£å‡½æ•°"""
    print("=" * 80)
    print("è®°å¿†æå–å·¥å…· - MemCell & Profile Extractor")
    print("=" * 80)
    print(f"è¿è¡Œæ¨¡å¼: {CURRENT_RUN_MODE.value}")
    print(f"åœºæ™¯ç±»å‹: {EXTRACT_CONFIG.scenario_type.value}")
    print("=" * 80 + "\n")

    try:
        if CURRENT_RUN_MODE in (RunMode.EXTRACT, RunMode.EXTRACT_ALL):
            # å®Œæ•´æå–æ¨¡å¼
            asyncio.run(run_extract_mode(EXTRACT_CONFIG, LLM_CONFIG, MONGO_CONFIG))

        elif CURRENT_RUN_MODE == RunMode.EXTRACT_MEMCELL_ONLY:
            # ä»…æå– MemCell æ¨¡å¼
            print("[Info] ä»…æå– MemCell æ¨¡å¼ - Profile æå–å·²ç¦ç”¨")
            EXTRACT_CONFIG.enable_profile_extraction = False
            asyncio.run(run_extract_mode(EXTRACT_CONFIG, LLM_CONFIG, MONGO_CONFIG))

        elif CURRENT_RUN_MODE == RunMode.EXTRACT_PROFILE_ONLY:
            # ä»…æå– Profile æ¨¡å¼
            asyncio.run(run_extract_profile_only_mode(EXTRACT_CONFIG, LLM_CONFIG))

        else:
            print(f"[Error] æœªçŸ¥çš„è¿è¡Œæ¨¡å¼: {CURRENT_RUN_MODE}")

    except KeyboardInterrupt:
        print("\n\n[Info] ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
    except Exception as e:
        print(f"\n[Error] ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()

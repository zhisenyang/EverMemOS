"""
LinkDocMemCellExtractor æµ‹è¯•

æµ‹è¯•æ–‡æ¡£è®°å¿†å•å…ƒæå–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æ¡£MemCellç”Ÿæˆ
- å¤šæ•°æ®æºæ”¯æŒï¼ˆNotionã€Google Driveã€Dropboxï¼‰
- æ–‡æ¡£è¿‡æ»¤é€»è¾‘
- é•¿æ–‡æ¡£å¤„ç†

ä½¿ç”¨æ–¹æ³•ï¼š
    python src/bootstrap.py tests/test_linkdoc_memcell_extractor.py
"""

import pytest
import asyncio
import json
import os
from datetime import datetime
from typing import Dict, Any, List

# å¯¼å…¥ä¾èµ–æ³¨å…¥ç›¸å…³æ¨¡å—
from core.di.utils import get_bean_by_type
from core.observation.logger import get_logger

# å¯¼å…¥è¦æµ‹è¯•çš„æ¨¡å—
from memory_layer.memcell_extractor.linkdoc_memcell_extractor import (
    LinkDocMemCellExtractor,
    LinkDocMemCellExtractRequest,
    FilterConfig,
)
from memory_layer.memcell_extractor.base_memcell_extractor import (
    RawData,
    MemCell,
    StatusResult,
)
from memory_layer.llm.llm_provider import LLMProvider
from memory_layer.llm.openai_provider import OpenAIProvider
from memory_layer.types import RawDataType
from infra_layer.adapters.input.mq.mapper.linkdoc_mapper import (
    convert_notion_document_to_raw_data,
    convert_dropbox_document_to_raw_data,
    convert_google_document_to_raw_data,
    convert_memo_document_to_raw_data,
)

# è·å–æ—¥å¿—è®°å½•å™¨
logger = get_logger(__name__)


def get_llm_provider() -> LLMProvider:
    """è·å–LLM Providerï¼Œå…ˆå°è¯•DIå®¹å™¨ï¼Œå¤±è´¥åˆ™ç›´æ¥åˆ›å»º"""
    try:
        # å°è¯•ä»DIå®¹å™¨è·å–
        return get_bean_by_type(LLMProvider)
    except:
        # å¦‚æœDIå®¹å™¨ä¸­æ²¡æœ‰ï¼Œåˆ™ç›´æ¥åˆ›å»º
        logger.info("DIå®¹å™¨ä¸­æœªæ‰¾åˆ°LLMProviderï¼Œç›´æ¥åˆ›å»º...")
        return LLMProvider(
            "openai", model="google/gemini-2.5-flash", temperature=0.3, max_tokens=16384
        )


def get_llm_provider_with_stats() -> OpenAIProvider:
    """è·å–å¸¦ç»Ÿè®¡åŠŸèƒ½çš„LLM Provider"""
    return OpenAIProvider(
        model="google/gemini-2.5-flash",
        temperature=0.3,
        max_tokens=16384,
        enable_stats=True,
    )


class TestLinkDocMemCellExtractor:
    """LinkDocMemCellExtractor æµ‹è¯•ç±»"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•å‰çš„è®¾ç½®"""
        self.test_user_id = "test_user_123"
        self.test_timestamp = int(datetime.now().timestamp())

        # åˆ›å»ºè¿‡æ»¤é…ç½®
        self.filter_config = FilterConfig(
            enable_filtering=False,  # æµ‹è¯•æ—¶é»˜è®¤å…³é—­è¿‡æ»¤
            min_content_length=20,
            max_content_length=500000,
            filter_preview_length=500,
        )

    def create_test_notion_document(
        self,
        title: str = "Test Document",
        content: str = "This is a test document for LinkDoc extraction.",
        is_deleted: bool = False,
    ) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•ç”¨çš„Notionæ–‡æ¡£æ•°æ®"""
        return {
            'id': 'test_notion_123',
            'third_party_user_id': "test_user@notion.com",
            'title': title,
            'body_content': content,
            'is_delete': is_deleted,
            'last_update_timestamp': str(self.test_timestamp),
            'notion_url': 'https://notion.so/test',
            'create_timestamp': str(self.test_timestamp),
            'object_id': 'test_obj_id',
            'parent_id': 'parent_123',
            'object_type': '1',
            'parent_type': '3',
        }

    def create_test_google_document(
        self,
        name: str = "Test Google Doc",
        content: str = "This is a test Google Drive document.",
        is_trashed: bool = False,
    ) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•ç”¨çš„Google Driveæ–‡æ¡£æ•°æ®"""
        return {
            '_id': 'google_test_123',
            'third_party_user_id': 'test_user@gmail.com',
            'name': name,
            'content': content,
            'explicitlyTrashed': is_trashed,
            'modify_timestamp': str(self.test_timestamp),
            'downloadUrl': 'https://drive.google.com/test',
            'type': 'application/vnd.google-apps.document',
            'owners': ['test_user@gmail.com'],
            'file_id': 'google_file_123',
        }

    def create_test_dropbox_document(
        self,
        name: str = "Test Dropbox Doc",
        content: str = "This is a test Dropbox document.",
        is_deleted: bool = False,
    ) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•ç”¨çš„Dropboxæ–‡æ¡£æ•°æ®"""
        return {
            '_id': 'dropbox_test_123',
            'third_party_user_id': 'test_user@dropbox.com',
            'name': name,
            'content': content,
            'deleteFlag': 1 if is_deleted else 0,
            'modify_timestamp': str(self.test_timestamp),
            'downloadUrl': 'https://dropbox.com/test',
            'type': 'application/pdf',
            'file_id': 'dropbox_file_123',
        }

    def create_test_memo_document(
        self,
        title: str = "Test Memo",
        body: str = "This is a test memo document.",
        is_deleted: bool = False,
    ) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•ç”¨çš„Memoæ–‡æ¡£æ•°æ®"""
        return {
            'id': 'memo_test_123',
            'readerids': [self.test_user_id],
            'title': title,
            'body': body,
            'files': [],
            'importsource': None,
            'delete_flag': 1 if is_deleted else 0,
            'updatetime': self.test_timestamp * 1000,  # Memoä½¿ç”¨æ¯«ç§’æ—¶é—´æˆ³
            'create_timestamp': f"{datetime.fromtimestamp(self.test_timestamp).strftime('%Y-%m-%d %H:%M:%S')}.123456",
            'last_update_timestamp': f"{datetime.fromtimestamp(self.test_timestamp).strftime('%Y-%m-%d %H:%M:%S')}.123456",
            'creatorid': self.test_user_id,
            'bodyhtml': None,
            'labels': [],
            'shareids': [],
        }

    def create_long_document_content(self, repeat_count: int = 5) -> str:
        """åˆ›å»ºé•¿æ–‡æ¡£å†…å®¹ç”¨äºæµ‹è¯•åˆ†å—å¤„ç†"""
        base_content = """
# æµ‹è¯•æ–‡æ¡£æ ‡é¢˜

è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•é•¿æ–‡æ¡£å¤„ç†çš„ç¤ºä¾‹æ–‡æ¡£ã€‚æ–‡æ¡£åŒ…å«å¤šä¸ªæ®µè½å’Œç« èŠ‚ã€‚

## ç¬¬ä¸€ç« ï¼šé¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ—¨åœ¨å¼€å‘ä¸€ä¸ªé«˜æ•ˆçš„æ–‡æ¡£å¤„ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†å„ç§æ ¼å¼çš„æ–‡æ¡£ï¼ŒåŒ…æ‹¬ä½†ä¸é™äº Notionã€Google Drive å’Œ Dropbox ä¸­çš„æ–‡æ¡£ã€‚

### 1.1 é¡¹ç›®ç›®æ ‡

- å®ç°é«˜æ•ˆçš„æ–‡æ¡£å†…å®¹æå–
- æ”¯æŒå¤šç§æ–‡æ¡£æº
- æä¾›æ™ºèƒ½çš„å†…å®¹åˆ†æå’Œæ‘˜è¦
- ç¡®ä¿ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œç¨³å®šæ€§

### 1.2 æŠ€æœ¯æ ˆ

æˆ‘ä»¬é‡‡ç”¨ç°ä»£åŒ–çš„æŠ€æœ¯æ ˆæ¥ç¡®ä¿ç³»ç»Ÿçš„æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§ï¼š
- Python 3.8+
- FastAPI æ¡†æ¶
- PostgreSQL æ•°æ®åº“
- Redis ç¼“å­˜
- Docker å®¹å™¨åŒ–éƒ¨ç½²

## ç¬¬äºŒç« ï¼šç³»ç»Ÿæ¶æ„

ç³»ç»Ÿé‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

1. æ–‡æ¡£æå–æœåŠ¡
2. å†…å®¹åˆ†ææœåŠ¡
3. å­˜å‚¨æœåŠ¡
4. API ç½‘å…³
5. ç”¨æˆ·ç®¡ç†æœåŠ¡

æ¯ä¸ªæœåŠ¡éƒ½å…·æœ‰ç‹¬ç«‹çš„æ•°æ®åº“å’Œç¼“å­˜ï¼Œé€šè¿‡æ¶ˆæ¯é˜Ÿåˆ—è¿›è¡Œå¼‚æ­¥é€šä¿¡ã€‚
"""
        return base_content * repeat_count

    @pytest.mark.asyncio
    async def test_basic_notion_extraction(self):
        """æµ‹è¯•åŸºç¡€Notionæ–‡æ¡£æå–"""
        print("\nğŸ§ª æµ‹è¯•åŸºç¡€Notionæ–‡æ¡£æå–")

        # è·å–LLM Provider
        llm_provider = get_llm_provider()
        extractor = LinkDocMemCellExtractor(
            llm_provider, filter_config=self.filter_config
        )

        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        test_doc = self.create_test_notion_document(
            title="é¡¹ç›®éœ€æ±‚æ–‡æ¡£",
            content="è¿™æ˜¯ä¸€ä¸ªè¯¦ç»†çš„é¡¹ç›®éœ€æ±‚æ–‡æ¡£ï¼ŒåŒ…å«äº†ç”¨æˆ·æ•…äº‹ã€åŠŸèƒ½éœ€æ±‚ã€éåŠŸèƒ½éœ€æ±‚ç­‰å†…å®¹ã€‚æ–‡æ¡£æ—¨åœ¨ä¸ºå¼€å‘å›¢é˜Ÿæä¾›æ¸…æ™°çš„é¡¹ç›®æŒ‡å¯¼ã€‚",
        )

        # è½¬æ¢ä¸ºRawData
        raw_data = await convert_notion_document_to_raw_data(test_doc)

        print(f"ğŸ“‹ æµ‹è¯•æ–‡æ¡£ä¿¡æ¯:")
        print(f"   - æ ‡é¢˜: {test_doc['title']}")
        print(f"   - å†…å®¹é•¿åº¦: {len(test_doc['body_content'])} å­—ç¬¦")
        print(f"   - ç”¨æˆ·ID: {test_doc['third_party_user_id']}")

        # åˆ›å»ºè¯·æ±‚
        request = LinkDocMemCellExtractRequest(
            history_raw_data_list=[],
            new_raw_data_list=[raw_data],
            user_id_list=[self.test_user_id],
        )
        raw_data.content['user_id_list'] = request.user_id_list
        # æ‰§è¡Œæå–
        result = await extractor.extract_memcell(request)

        # éªŒè¯ç»“æœ
        assert result is not None, "æå–ç»“æœä¸åº”è¯¥ä¸ºNone"
        memcell, status_result = result

        print(f"âœ… æå–å®Œæˆ:")
        print(f"   - MemCell: {memcell is not None}")
        print(f"   - should_wait: {status_result.should_wait}")

        if memcell:
            print(f"\nğŸ“„ MemCellè¯¦ç»†ä¿¡æ¯:")
            print(f"   - event_id: {memcell.event_id}")
            print(f"   - user_id_list: {memcell.user_id_list}")
            print(f"   - file_name: {memcell.file_name}")
            print(f"   - file_type: {memcell.file_type}")
            print(f"   - source_type: {memcell.source_type}")
            print(f"   - type: {memcell.type}")
            print(f"   - timestamp: {memcell.timestamp}")
            print(f"   - subject: {memcell.subject}")
            print(f"   - summary: {memcell.summary}")
            print(f"   - keywords: {memcell.keywords}")
            print(f"   - clipsæ•°é‡: {len(memcell.clips) if memcell.clips else 0}")

            # éªŒè¯åŸºæœ¬å­—æ®µ
            assert memcell.event_id is not None
            assert len(memcell.user_id_list) > 0
            assert memcell.file_name == test_doc['title']
            assert memcell.source_type == 'notion'
            assert memcell.type == RawDataType.LINKDOC
            assert memcell.summary is not None

        else:
            print("âš ï¸ æ²¡æœ‰ç”ŸæˆMemCell")

    @pytest.mark.asyncio
    async def test_multiple_data_sources(self):
        """æµ‹è¯•å¤šæ•°æ®æºæ”¯æŒ"""
        print("\nğŸ§ª æµ‹è¯•å¤šæ•°æ®æºæ”¯æŒ")

        # è·å–LLM Provider
        llm_provider = get_llm_provider()
        extractor = LinkDocMemCellExtractor(
            llm_provider, filter_config=self.filter_config
        )

        # å‡†å¤‡ä¸åŒæ•°æ®æºçš„æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "name": "Notionæ–‡æ¡£",
                "doc": self.create_test_notion_document(
                    title="æŠ€æœ¯è®¾è®¡æ–‡æ¡£",
                    content="è¿™æ˜¯ä¸€ä»½è¯¦ç»†çš„æŠ€æœ¯è®¾è®¡æ–‡æ¡£ï¼ŒåŒ…å«ç³»ç»Ÿæ¶æ„ã€æ•°æ®åº“è®¾è®¡ã€APIè®¾è®¡ç­‰å†…å®¹ã€‚",
                ),
                "mapper": convert_notion_document_to_raw_data,
                "expected_source": "notion",
            },
            {
                "name": "Google Driveæ–‡æ¡£",
                "doc": self.create_test_google_document(
                    name="ç”¨æˆ·æ‰‹å†Œ",
                    content="è¿™æ˜¯ä¸€ä»½ç”¨æˆ·æ“ä½œæ‰‹å†Œï¼Œè¯¦ç»†ä»‹ç»äº†äº§å“çš„å„é¡¹åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•ã€‚",
                ),
                "mapper": convert_google_document_to_raw_data,
                "expected_source": "google",
            },
            {
                "name": "Dropboxæ–‡æ¡£",
                "doc": self.create_test_dropbox_document(
                    name="é¡¹ç›®æ€»ç»“æŠ¥å‘Š",
                    content="è¿™æ˜¯é¡¹ç›®å®Œæˆåçš„æ€»ç»“æŠ¥å‘Šï¼ŒåŒ…å«é¡¹ç›®æˆæœã€ç»éªŒæ•™è®­å’Œæ”¹è¿›å»ºè®®ã€‚",
                ),
                "mapper": convert_dropbox_document_to_raw_data,
                "expected_source": "dropbox",
            },
            {
                "name": "Memoæ–‡æ¡£",
                "doc": self.create_test_memo_document(
                    title="å·¥ä½œç¬”è®°",
                    body="è¿™æ˜¯ä¸€ä»½å·¥ä½œç¬”è®°ï¼Œè®°å½•äº†ä»Šå¤©çš„å·¥ä½œè¿›å±•å’Œé‡è¦å†³ç­–ã€‚",
                ),
                "mapper": convert_memo_document_to_raw_data,
                "expected_source": "memo",
            },
        ]

        for i, test_case in enumerate(test_cases):
            print(f"\nğŸ“‹ æµ‹è¯•ç”¨ä¾‹ {i+1}: {test_case['name']}")

            # è½¬æ¢ä¸ºRawData
            raw_data = await test_case["mapper"](test_case["doc"])

            # åˆ›å»ºè¯·æ±‚
            user_id = self.test_user_id
            request = LinkDocMemCellExtractRequest(
                history_raw_data_list=[],
                new_raw_data_list=[raw_data],
                user_id_list=[user_id],
            )

            # æ‰§è¡Œæå–
            result = await extractor.extract_memcell(request)

            if result and result[0]:
                memcell, status_result = result
                print(f"âœ… {test_case['name']} æå–æˆåŠŸ:")
                print(f"   - æ•°æ®æº: {memcell.source_type}")
                print(f"   - æ–‡ä»¶å: {memcell.file_name}")
                print(f"   - æ‘˜è¦: {memcell.summary}")

                # éªŒè¯æ•°æ®æºç±»å‹
                assert memcell.source_type == test_case["expected_source"]
                assert memcell.type == RawDataType.LINKDOC

            else:
                print(f"âš ï¸ {test_case['name']} æœªèƒ½æå–MemCell")

    @pytest.mark.asyncio
    async def test_document_filtering(self):
        """æµ‹è¯•æ–‡æ¡£è¿‡æ»¤åŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•æ–‡æ¡£è¿‡æ»¤åŠŸèƒ½")

        # è·å–LLM Provider
        llm_provider = get_llm_provider()

        # åˆ›å»ºå¯ç”¨è¿‡æ»¤çš„é…ç½®
        filter_config = FilterConfig(
            enable_filtering=True,
            min_content_length=30,
            max_content_length=500000,
            exclude_keywords=["æ¸¸æˆ", "å¨±ä¹", "è´­ç‰©"],
        )

        extractor = LinkDocMemCellExtractor(llm_provider, filter_config=filter_config)

        # æµ‹è¯•ç”¨ä¾‹ï¼šåº”è¯¥è¢«è¿‡æ»¤çš„æ–‡æ¡£
        filter_test_cases = [
            {
                "name": "å†…å®¹è¿‡çŸ­æ–‡æ¡£",
                "doc": self.create_test_notion_document(title="çŸ­ç¬”è®°", content="Hi"),
                "should_be_filtered": True,
                "reason": "å†…å®¹å¤ªçŸ­",
            },
            {
                "name": "åŒ…å«æ’é™¤å…³é”®è¯æ–‡æ¡£",
                "doc": self.create_test_notion_document(
                    title="æˆ‘çš„æ¸¸æˆæ”¶è—",
                    content="è¿™æ˜¯æˆ‘æœ€å–œæ¬¢çš„æ¸¸æˆåˆ—è¡¨ï¼ŒåŒ…å«å„ç§ç±»å‹çš„æ¸¸æˆæ¨èã€‚",
                ),
                "should_be_filtered": True,
                "reason": "åŒ…å«æ’é™¤å…³é”®è¯",
            },
            {
                "name": "å·²åˆ é™¤æ–‡æ¡£",
                "doc": self.create_test_notion_document(
                    title="é‡è¦æ–‡æ¡£",
                    content="è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å·¥ä½œæ–‡æ¡£ï¼ŒåŒ…å«é¡¹ç›®ç›¸å…³ä¿¡æ¯ã€‚",
                    is_deleted=True,
                ),
                "should_be_filtered": True,
                "reason": "æ–‡æ¡£å·²åˆ é™¤",
            },
            {
                "name": "åŒ…å«å…¬å¸éƒ¨é—¨åä½†å†…å®¹æ— æ„ä¹‰æ–‡æ¡£",
                "doc": self.create_test_notion_document(
                    title="tanka éšæœºæ–‡æ¡£",
                    content="tanka å•Šå•Šå•Šå•Š éšä¾¿å†™å†™ 123456 xyz abc tanka å“ˆå“ˆå“ˆ æ— èŠçš„å†…å®¹ blah blah tanka æµ‹è¯•æµ‹è¯• éšæœºå­—ç¬¦ä¸² qwerty asdf æ²¡æœ‰ä»»ä½•æ„ä¹‰çš„æ–‡å­—å †ç Œ",
                ),
                "should_be_filtered": True,
                "reason": "è™½ç„¶åŒ…å«å…¬å¸éƒ¨é—¨åï¼Œä½†å†…å®¹æ— æ„ä¹‰",
            },
            {
                "name": "æ­£å¸¸å·¥ä½œæ–‡æ¡£",
                "doc": self.create_test_notion_document(
                    title="é¡¹ç›®ä¼šè®®çºªè¦",
                    content="ä»Šå¤©çš„é¡¹ç›®ä¼šè®®è®¨è®ºäº†æŠ€æœ¯æ–¹æ¡ˆã€æ—¶é—´å®‰æ’å’Œèµ„æºåˆ†é…ç­‰é‡è¦è®®é¢˜ã€‚å›¢é˜Ÿå†³å®šé‡‡ç”¨å¾®æœåŠ¡æ¶æ„ã€‚",
                ),
                "should_be_filtered": False,
                "reason": "æ­£å¸¸æ–‡æ¡£",
            },
        ]

        for i, test_case in enumerate(filter_test_cases):
            print(f"\nğŸ“‹ è¿‡æ»¤æµ‹è¯• {i+1}: {test_case['name']}")

            # è½¬æ¢ä¸ºRawData
            raw_data = await convert_notion_document_to_raw_data(test_case["doc"])

            # æµ‹è¯•é¢„å¤„ç†è¿‡æ»¤
            should_process, reason = await extractor.pre_process(raw_data)

            print(f"   - é¢„æœŸ: {'è¿‡æ»¤' if test_case['should_be_filtered'] else 'é€šè¿‡'}")
            print(f"   - å®é™…: {'è¿‡æ»¤' if not should_process else 'é€šè¿‡'}")
            print(f"   - åŸå› : {reason}")

            if test_case["should_be_filtered"]:
                assert (
                    not should_process
                ), f"æ–‡æ¡£åº”è¯¥è¢«è¿‡æ»¤ä½†å´é€šè¿‡äº†: {test_case['name']}"
                print(f"âœ… æ­£ç¡®è¿‡æ»¤: {test_case['reason']}")
            else:
                # æ³¨æ„ï¼šæ­£å¸¸æ–‡æ¡£å¯èƒ½åœ¨LLMè¿‡æ»¤é˜¶æ®µè¢«è¿‡æ»¤ï¼Œè¿™é‡ŒåªéªŒè¯è§„åˆ™è¿‡æ»¤
                print(f"âœ… é€šè¿‡è§„åˆ™è¿‡æ»¤é˜¶æ®µ")

    @pytest.mark.asyncio
    async def test_long_document_processing(self):
        """æµ‹è¯•é•¿æ–‡æ¡£å¤„ç†"""
        print("\nğŸ§ª æµ‹è¯•é•¿æ–‡æ¡£å¤„ç†")

        # è·å–LLM Provider
        llm_provider = get_llm_provider()

        # åˆ›å»ºæ”¯æŒé•¿æ–‡æ¡£çš„extractorï¼ˆè¾ƒå°çš„chunk sizeç”¨äºæµ‹è¯•ï¼‰
        extractor = LinkDocMemCellExtractor(
            llm_provider,
            max_chars_per_chunk=1000,  # è¾ƒå°çš„chunkç”¨äºæµ‹è¯•
            filter_config=self.filter_config,
        )

        # åˆ›å»ºé•¿æ–‡æ¡£
        long_content = self.create_long_document_content(repeat_count=3)
        long_doc = self.create_test_notion_document(
            title="è¯¦ç»†æŠ€æœ¯æ–‡æ¡£", content=long_content
        )

        print(f"ğŸ“‹ é•¿æ–‡æ¡£ä¿¡æ¯:")
        print(f"   - æ ‡é¢˜: {long_doc['title']}")
        print(f"   - å†…å®¹é•¿åº¦: {len(long_content)} å­—ç¬¦")
        print(f"   - é¢„æœŸä¼šè¢«åˆ†å—å¤„ç†")

        # è½¬æ¢ä¸ºRawData
        raw_data = await convert_notion_document_to_raw_data(long_doc)

        # åˆ›å»ºè¯·æ±‚
        request = LinkDocMemCellExtractRequest(
            history_raw_data_list=[],
            new_raw_data_list=[raw_data],
            user_id_list=[self.test_user_id],
        )

        # æ‰§è¡Œæå–
        result = await extractor.extract_memcell(request)

        # éªŒè¯ç»“æœ
        if result and result[0]:
            memcell, status_result = result
            print(f"âœ… é•¿æ–‡æ¡£å¤„ç†æˆåŠŸ:")
            print(f"   - æ–‡ä»¶å: {memcell.file_name}")
            print(f"   - æ‘˜è¦é•¿åº¦: {len(memcell.summary)} å­—ç¬¦")
            print(f"   - clipsæ•°é‡: {len(memcell.clips) if memcell.clips else 0}")
            print(f"   - å…³é”®è¯: {memcell.keywords[:5] if memcell.keywords else []}")

            # éªŒè¯é•¿æ–‡æ¡£å¤„ç†
            assert memcell.clips is not None
            assert len(memcell.clips) > 1, "é•¿æ–‡æ¡£åº”è¯¥è¢«åˆ†æˆå¤šä¸ªclips"
            assert memcell.summary is not None
            assert len(memcell.summary) > 50, "é•¿æ–‡æ¡£åº”è¯¥æœ‰è¾ƒè¯¦ç»†çš„æ‘˜è¦"

            print(f"âœ… é•¿æ–‡æ¡£æˆåŠŸåˆ†æˆ {len(memcell.clips)} ä¸ªclips")

        else:
            print("âš ï¸ é•¿æ–‡æ¡£å¤„ç†å¤±è´¥")

    @pytest.mark.asyncio
    async def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        print("\nğŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†")

        # è·å–LLM Provider
        llm_provider = get_llm_provider()
        extractor = LinkDocMemCellExtractor(
            llm_provider, filter_config=self.filter_config
        )

        # æµ‹è¯•ç©ºè¯·æ±‚
        print("\nğŸ“‹ æµ‹è¯•ç©ºè¯·æ±‚:")
        empty_request = LinkDocMemCellExtractRequest(
            history_raw_data_list=[], new_raw_data_list=[], user_id_list=[]
        )

        result = await extractor.extract_memcell(empty_request)
        assert result is not None
        memcell, status_result = result
        assert memcell is None
        print("âœ… ç©ºè¯·æ±‚æ­£ç¡®è¿”å›None")

        # æµ‹è¯•æ— æ•ˆæ•°æ®
        print("\nğŸ“‹ æµ‹è¯•æ— æ•ˆæ•°æ®:")
        invalid_doc = {
            'id': 'invalid_doc',
            'title': '',  # ç©ºæ ‡é¢˜
            'body_content': '',  # ç©ºå†…å®¹
            'tanka_user_id': self.test_user_id,
        }

        try:
            raw_data = await convert_notion_document_to_raw_data(invalid_doc)
            request = LinkDocMemCellExtractRequest(
                history_raw_data_list=[],
                new_raw_data_list=[raw_data],
                user_id_list=[self.test_user_id],
            )

            result = await extractor.extract_memcell(request)
            print("âœ… æ— æ•ˆæ•°æ®å¤„ç†å®Œæˆï¼ˆå¯èƒ½è¢«è¿‡æ»¤æˆ–ç”Ÿæˆé»˜è®¤MemCellï¼‰")

        except Exception as e:
            print(f"âœ… æ— æ•ˆæ•°æ®æ­£ç¡®æŠ›å‡ºå¼‚å¸¸: {type(e).__name__}")

    @pytest.mark.asyncio
    async def test_memo_extraction(self):
        """æµ‹è¯•Memoæ–‡æ¡£æå–"""
        print("\nğŸ§ª æµ‹è¯•Memoæ–‡æ¡£æå–")

        # è·å–LLM Provider
        llm_provider = get_llm_provider()
        extractor = LinkDocMemCellExtractor(
            llm_provider, filter_config=self.filter_config
        )

        # æµ‹è¯•ä¸åŒç±»å‹çš„memoæ–‡æ¡£
        memo_test_cases = [
            {
                "name": "å·¥ä½œç¬”è®°",
                "doc": self.create_test_memo_document(
                    title="é¡¹ç›®ä¼šè®®çºªè¦",
                    body="ä»Šæ—¥ä¼šè®®è®¨è®ºäº†æ–°åŠŸèƒ½å¼€å‘è®¡åˆ’ï¼š\n1. ç”¨æˆ·ç•Œé¢ä¼˜åŒ–\n2. æ€§èƒ½æå‡æ–¹æ¡ˆ\n3. å®‰å…¨æ€§æ”¹è¿›\nå†³å®šä¸‹å‘¨å¼€å§‹å®æ–½ç¬¬ä¸€é˜¶æ®µã€‚",
                ),
            },
            {
                "name": "èŠå¤©è®°å½•",
                "doc": self.create_test_memo_document(
                    title="Chat History with å›¢é˜Ÿæˆå‘˜ on 2025-01-15",
                    body="å›¢é˜Ÿæˆå‘˜: å…³äºæ–°é¡¹ç›®çš„æŠ€æœ¯é€‰å‹ï¼Œæˆ‘å»ºè®®ä½¿ç”¨å¾®æœåŠ¡æ¶æ„ã€‚\næˆ‘: åŒæ„ï¼Œè¿™æ ·å¯ä»¥æé«˜ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œç»´æŠ¤æ€§ã€‚",
                ),
            },
            {
                "name": "é•¿å†…å®¹memo",
                "doc": self.create_test_memo_document(
                    title="æŠ€æœ¯è°ƒç ”æŠ¥å‘Š",
                    body=self.create_long_document_content(
                        repeat_count=2
                    ),  # ä½¿ç”¨é•¿å†…å®¹
                ),
            },
        ]

        for i, test_case in enumerate(memo_test_cases):
            print(f"\nğŸ“‹ Memoæµ‹è¯• {i+1}: {test_case['name']}")

            # è½¬æ¢ä¸ºRawData
            raw_data = await convert_memo_document_to_raw_data(test_case["doc"])

            print(f"   - æ ‡é¢˜: {test_case['doc']['title']}")
            print(f"   - å†…å®¹é•¿åº¦: {len(test_case['doc']['body'])} å­—ç¬¦")
            print(f"   - åˆ›å»ºè€…ID: {test_case['doc']['creatorid']}")

            # åˆ›å»ºè¯·æ±‚
            request = LinkDocMemCellExtractRequest(
                history_raw_data_list=[],
                new_raw_data_list=[raw_data],
                user_id_list=[self.test_user_id],
            )

            # æ‰§è¡Œæå–
            result = await extractor.extract_memcell(request)

            # éªŒè¯ç»“æœ
            if result and result[0]:
                memcell, status_result = result
                print(f"âœ… Memoæå–æˆåŠŸ:")
                print(f"   - æ•°æ®æº: {memcell.source_type}")
                print(f"   - æ–‡ä»¶å: {memcell.file_name}")
                print(f"   - æ‘˜è¦: {memcell.summary}")

                # éªŒè¯memoç‰¹å®šå­—æ®µ
                assert memcell.source_type == "memo"
                assert memcell.type == RawDataType.LINKDOC
                assert memcell.file_name == test_case["doc"]["title"]

                # éªŒè¯å‚ä¸è€…ä¿¡æ¯ï¼ˆåŒ…å«åˆ›å»ºè€…ã€è¯»è€…ã€åˆ†äº«è€…ï¼‰
                expected_participants = test_case["doc"].get(
                    "readerids", []
                ) + test_case["doc"].get("shareids", [])
                print(f"   - å‚ä¸è€…æ•°é‡: {len(expected_participants)}")

            else:
                print(f"âš ï¸ Memo {test_case['name']} æœªèƒ½æå–MemCell")

    @pytest.mark.asyncio
    async def test_extractor_configuration(self):
        """æµ‹è¯•æå–å™¨é…ç½®"""
        print("\nğŸ§ª æµ‹è¯•æå–å™¨é…ç½®")

        # è·å–LLM Provider
        llm_provider = get_llm_provider()

        # æµ‹è¯•ä¸åŒé…ç½®
        configs = [
            {"name": "é»˜è®¤é…ç½®", "config": FilterConfig()},
            {
                "name": "ä¸¥æ ¼è¿‡æ»¤é…ç½®",
                "config": FilterConfig(
                    enable_filtering=True,
                    min_content_length=100,
                    exclude_keywords=["æµ‹è¯•", "demo"],
                ),
            },
            {
                "name": "å°chunké…ç½®",
                "config": FilterConfig(enable_filtering=False),
                "max_chars": 1000,
            },
        ]

        for config_test in configs:
            print(f"\nğŸ“‹ æµ‹è¯• {config_test['name']}:")

            # åˆ›å»ºextractor
            kwargs = {'filter_config': config_test['config']}
            if 'max_chars' in config_test:
                kwargs['max_chars_per_chunk'] = config_test['max_chars']

            extractor = LinkDocMemCellExtractor(llm_provider, **kwargs)

            # éªŒè¯é…ç½®
            assert extractor.llm_provider is not None
            assert extractor.filter_config is not None

            if 'max_chars' in config_test:
                assert extractor.max_chars_per_chunk == config_test['max_chars']

            print(f"âœ… {config_test['name']} åˆ›å»ºæˆåŠŸ")

    @pytest.mark.asyncio
    async def test_token_statistics(self):
        """æµ‹è¯•tokenç»Ÿè®¡åŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•tokenç»Ÿè®¡åŠŸèƒ½")

        # ä½¿ç”¨å¸¦ç»Ÿè®¡åŠŸèƒ½çš„LLM Provider
        llm_provider = get_llm_provider_with_stats()
        extractor = LinkDocMemCellExtractor(
            llm_provider, filter_config=self.filter_config
        )

        # æµ‹è¯•æ–‡æ¡£åˆ—è¡¨
        test_docs = [
            self.create_test_notion_document(
                title="æŠ€æœ¯æ–‡æ¡£1",
                content="è¿™æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£ï¼ŒåŒ…å«ç³»ç»Ÿæ¶æ„è®¾è®¡ã€æ•°æ®åº“è®¾è®¡å’ŒAPIè®¾è®¡ç­‰å†…å®¹ã€‚",
            ),
            self.create_test_google_document(
                name="ç”¨æˆ·æ‰‹å†Œ1",
                content="è¿™æ˜¯ç”¨æˆ·æ“ä½œæ‰‹å†Œï¼Œè¯¦ç»†ä»‹ç»äº†äº§å“çš„å„é¡¹åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•ã€‚",
            ),
            self.create_test_dropbox_document(
                name="é¡¹ç›®æŠ¥å‘Š1",
                content="è¿™æ˜¯é¡¹ç›®å®Œæˆåçš„æ€»ç»“æŠ¥å‘Šï¼ŒåŒ…å«é¡¹ç›®æˆæœã€ç»éªŒæ•™è®­å’Œæ”¹è¿›å»ºè®®ã€‚",
            ),
        ]

        # ç»Ÿè®¡å˜é‡
        total_tokens = 0
        total_calls = 0
        file_stats = []

        for i, doc in enumerate(test_docs):
            print(
                f"\nğŸ“„ å¤„ç†æ–‡æ¡£ {i+1}: {doc.get('title', doc.get('name', 'Unknown'))}"
            )

            # è½¬æ¢ä¸ºRawData
            if 'title' in doc:  # Notionæ–‡æ¡£
                raw_data = await convert_notion_document_to_raw_data(doc)
            elif 'name' in doc and 'explicitlyTrashed' in doc:  # Googleæ–‡æ¡£
                raw_data = await convert_google_document_to_raw_data(doc)
            else:  # Dropboxæ–‡æ¡£
                raw_data = await convert_dropbox_document_to_raw_data(doc)

            # åˆ›å»ºè¯·æ±‚
            user_id = self.test_user_id
            request = LinkDocMemCellExtractRequest(
                history_raw_data_list=[],
                new_raw_data_list=[raw_data],
                user_id_list=[user_id],
            )

            # æ‰§è¡Œæå–
            result = await extractor.extract_memcell(request)

            # è·å–tokenç»Ÿè®¡
            current_stats = llm_provider.get_current_call_stats()
            if current_stats:
                tokens = current_stats.get('total_tokens', 0)
                total_tokens += tokens
                total_calls += 1

                file_stat = {
                    'file_name': doc.get('title', doc.get('name', f'Document_{i+1}')),
                    'tokens': tokens,
                    'prompt_tokens': current_stats.get('prompt_tokens', 0),
                    'completion_tokens': current_stats.get('completion_tokens', 0),
                }
                file_stats.append(file_stat)

                print(
                    f"   âœ… æˆåŠŸ: {tokens} tokens (Prompt: {current_stats.get('prompt_tokens', 0)}, Completion: {current_stats.get('completion_tokens', 0)})"
                )
            else:
                print(f"   âš ï¸ æ— tokenç»Ÿè®¡ä¿¡æ¯")

        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        print(f"\nğŸ“Š === TOKENç»Ÿè®¡ç»“æœ ===")
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶æ•°: {len(file_stats)}")
        print(f"ğŸ”„ APIè°ƒç”¨æ¬¡æ•°: {total_calls}")
        print(f"ğŸ“ æ€»Tokenæ•°: {total_tokens:,}")
        print(
            f"ğŸ“ˆ å¹³å‡æ¯æ–‡ä»¶Tokenæ•°: {total_tokens / len(file_stats) if file_stats else 0:.1f}"
        )
        print(
            f"ğŸ“ˆ å¹³å‡æ¯æ¬¡è°ƒç”¨Tokenæ•°: {total_tokens / total_calls if total_calls > 0 else 0:.1f}"
        )

        print(f"\nğŸ“‹ === æ–‡ä»¶è¯¦æƒ… ===")
        for i, stat in enumerate(file_stats, 1):
            print(f"{i}. {stat['file_name']}: {stat['tokens']} tokens")

        # éªŒè¯ç»Ÿè®¡ç»“æœ
        assert total_calls > 0, "åº”è¯¥æœ‰APIè°ƒç”¨"
        assert total_tokens > 0, "åº”è¯¥æœ‰tokenä½¿ç”¨"
        assert len(file_stats) == len(test_docs), "åº”è¯¥æœ‰å¯¹åº”æ•°é‡çš„æ–‡ä»¶ç»Ÿè®¡"


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹è¿è¡ŒLinkDocMemCellExtractoræµ‹è¯•")
    print("=" * 60)

    test_instance = TestLinkDocMemCellExtractor()

    try:
        # è¿è¡Œæµ‹è¯•æ–¹æ³•
        test_instance.setup_method()
        await test_instance.test_basic_notion_extraction()

        test_instance.setup_method()
        await test_instance.test_multiple_data_sources()

        test_instance.setup_method()
        await test_instance.test_document_filtering()

        test_instance.setup_method()
        await test_instance.test_long_document_processing()

        test_instance.setup_method()
        await test_instance.test_error_handling()

        test_instance.setup_method()
        await test_instance.test_memo_extraction()

        test_instance.setup_method()
        await test_instance.test_extractor_configuration()

        test_instance.setup_method()
        await test_instance.test_token_statistics()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        raise


if __name__ == "__main__":
    # å½“ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ—¶æ‰§è¡Œ
    # æ³¨æ„ï¼šé€šè¿‡ bootstrap.py è¿è¡Œæ—¶ï¼Œç¯å¢ƒå·²ç»åˆå§‹åŒ–å®Œæˆ
    asyncio.run(run_all_tests())

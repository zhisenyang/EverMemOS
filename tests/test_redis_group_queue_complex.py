"""
Redisæ¶ˆæ¯åˆ†ç»„é˜Ÿåˆ—ç®¡ç†å™¨é«˜çº§æµ‹è¯•

æµ‹è¯•è¦†ç›–ï¼š
1. å¤šæ¶ˆè´¹è€…å¹¶å‘æŠ•é€’å’Œæ¶ˆè´¹æµ‹è¯•
2. å¤§æ•°æ®é‡å†…å­˜å ç”¨æµ‹è¯•
"""

import asyncio
import sys
import os
import time
import random
import string
import json
import traceback
from typing import Set, Dict, Any

# å°è¯•å¯¼å…¥ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨mock
try:
    # å…ˆå°è¯• Mock tanka_ai_toolkit ä¾èµ–
    from unittest.mock import MagicMock

    # Mock tanka_ai_toolkit æ¨¡å—
    sys.modules['tanka_ai_toolkit'] = MagicMock()
    sys.modules['tanka_ai_toolkit.utils'] = MagicMock()
    sys.modules['tanka_ai_toolkit.utils.log_tools'] = MagicMock()
    sys.modules['tanka_ai_toolkit.utils.log_tools.tanka_log'] = MagicMock()

    from core.queue.redis_group_queue.redis_group_queue_item import SimpleQueueItem
    from core.di.utils import get_bean

    IMPORTS_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥æ ¸å¿ƒæ¨¡å—ï¼ˆä½¿ç”¨Mockä¾èµ–ï¼‰")
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥å¤±è´¥: {e}")
    print("å°†ä½¿ç”¨Mockå¯¹è±¡è¿›è¡Œæµ‹è¯•...")
    IMPORTS_AVAILABLE = False

    # åˆ›å»ºMockç±»
    class MockSimpleQueueItem:
        def __init__(self, data, item_type):
            self.data = data
            self.item_type = item_type
            self.id = f"msg_{random.randint(1000, 9999)}"

        def to_json_str(self):
            return json.dumps(
                {"data": self.data, "item_type": self.item_type, "id": self.id}
            )

    SimpleQueueItem = MockSimpleQueueItem


def generate_random_data(size_kb: int = 1) -> Dict[str, Any]:
    """ç”ŸæˆæŒ‡å®šå¤§å°çš„éšæœºæ•°æ®"""
    # è®¡ç®—éœ€è¦çš„å­—ç¬¦æ•°ï¼ˆå¤§çº¦ï¼‰
    target_size = size_kb * 1024

    # ç”Ÿæˆéšæœºå­—ç¬¦ä¸²ä½œä¸ºä¸»è¦å†…å®¹
    content_size = target_size - 200  # é¢„ç•™ä¸€äº›ç©ºé—´ç»™å…¶ä»–å­—æ®µ
    random_content = ''.join(
        random.choices(string.ascii_letters + string.digits, k=content_size)
    )

    return {
        "id": f"data_{random.randint(100000, 999999)}",
        "content": random_content,
        "timestamp": time.time(),
        "metadata": {
            "type": "test_data",
            "size_kb": size_kb,
            "generated_at": time.time(),
        },
    }


def generate_random_group_key() -> str:
    """ç”Ÿæˆéšæœºåˆ†ç»„é”®"""
    return f"group_{random.randint(1, 1000)}"


async def producer_worker(
    manager,
    producer_id: int,
    target_count: int,
    delivered_ids: Set[str],
    delay_range: tuple = (0.01, 0.1),
):
    """
    ç”Ÿäº§è€…å·¥ä½œåç¨‹

    Args:
        manager: é˜Ÿåˆ—ç®¡ç†å™¨å®ä¾‹
        producer_id: ç”Ÿäº§è€…ID
        target_count: ç›®æ ‡æŠ•é€’æ•°é‡
        delivered_ids: å·²æŠ•é€’æ¶ˆæ¯IDé›†åˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        delay_range: å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
    """
    delivered_count = 0

    print(f"ğŸš€ ç”Ÿäº§è€… {producer_id} å¼€å§‹å·¥ä½œï¼Œç›®æ ‡æŠ•é€’ {target_count} æ¡æ¶ˆæ¯")

    while delivered_count < target_count:
        try:
            # åˆ›å»ºæ¶ˆæ¯
            message_id = f"producer_{producer_id}_msg_{delivered_count + 1}_{int(time.time() * 1000000)}"
            message = SimpleQueueItem(
                data={
                    "producer_id": producer_id,
                    "message_id": message_id,
                    "sequence": delivered_count + 1,
                    "content": f"Message from producer {producer_id}, sequence {delivered_count + 1}",
                    "timestamp": time.time(),
                },
                item_type="test_message",
            )

            # éšæœºé€‰æ‹©åˆ†ç»„
            group_key = generate_random_group_key()

            # å°è¯•æŠ•é€’
            success = await manager.deliver_message(group_key, message)

            if success:
                delivered_ids.add(message_id)
                delivered_count += 1

                if delivered_count % 50 == 0:
                    print(
                        f"ğŸ“¤ ç”Ÿäº§è€… {producer_id} å·²æŠ•é€’ {delivered_count}/{target_count} æ¡æ¶ˆæ¯"
                    )
            else:
                # æŠ•é€’å¤±è´¥ï¼Œå¯èƒ½æ˜¯è¾¾åˆ°äº†ä¸Šé™ï¼Œç¨ç­‰ä¸€ä¸‹å†è¯•
                await asyncio.sleep(random.uniform(0.1, 0.5))

            # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿæ–­æ–­ç»­ç»­çš„æŠ•é€’
            await asyncio.sleep(random.uniform(*delay_range))

        except Exception as e:  # pylint: disable=broad-except
            print(f"âŒ ç”Ÿäº§è€… {producer_id} æŠ•é€’æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            await asyncio.sleep(0.1)

    print(f"âœ… ç”Ÿäº§è€… {producer_id} å®Œæˆå·¥ä½œï¼Œå®é™…æŠ•é€’ {delivered_count} æ¡æ¶ˆæ¯")


async def consumer_worker(
    manager,
    consumer_id: int,
    consumed_ids: Set[str],
    consumer_consumed_ids: Dict[str, Set[str]],  # æ¯ä¸ªæ¶ˆè´¹è€…çš„æ¶ˆè´¹IDé›†åˆ
    stop_event: asyncio.Event,
    owner_id: str,
    rebalance_probability: float = 0.05,  # æ¯æ¬¡å¾ªç¯5%çš„æ¦‚ç‡è§¦å‘rebalance
):
    """
    æ¶ˆè´¹è€…å·¥ä½œåç¨‹

    Args:
        manager: é˜Ÿåˆ—ç®¡ç†å™¨å®ä¾‹
        consumer_id: æ¶ˆè´¹è€…ID
        consumed_ids: å…¨å±€å·²æ¶ˆè´¹æ¶ˆæ¯IDé›†åˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        consumer_consumed_ids: æ¯ä¸ªæ¶ˆè´¹è€…çš„æ¶ˆè´¹IDé›†åˆ
        stop_event: åœæ­¢äº‹ä»¶
        owner_id: æ¶ˆè´¹è€…çš„owner_id
        rebalance_probability: æ¯æ¬¡å¾ªç¯è§¦å‘rebalanceçš„æ¦‚ç‡
    """
    consumed_count = 0
    rebalance_count = 0

    # å…ˆåŠ å…¥æ¶ˆè´¹è€…
    await manager.join_consumer(owner_id)
    print(f"ğŸ”„ æ¶ˆè´¹è€… {consumer_id} (owner_id: {owner_id}) å·²åŠ å…¥å¹¶å¼€å§‹å·¥ä½œ")

    # åˆå§‹åŒ–è¯¥æ¶ˆè´¹è€…çš„æ¶ˆè´¹IDé›†åˆ
    if owner_id not in consumer_consumed_ids:
        consumer_consumed_ids[owner_id] = set()

    while not stop_event.is_set():
        try:
            # éšæœºè§¦å‘rebalance
            if random.random() < rebalance_probability:
                try:
                    result = await manager.rebalance_partitions()
                    rebalance_count += 1
                    print(
                        f"ğŸ”„ æ¶ˆè´¹è€… {consumer_id} è§¦å‘ç¬¬ {rebalance_count} æ¬¡ rebalanceï¼Œç»“æœ: {result}"
                    )
                except Exception as rebalance_error:  # pylint: disable=broad-except
                    print(f"âš ï¸ æ¶ˆè´¹è€… {consumer_id} rebalanceå¤±è´¥: {rebalance_error}")

            # è·å–æ¶ˆæ¯ï¼ˆä½¿ç”¨æŒ‡å®šçš„owner_idï¼‰
            messages = await manager.get_messages(score_threshold=0, owner_id=owner_id)

            if messages:
                for message in messages:
                    # ä»æ¶ˆæ¯æ•°æ®ä¸­æå–message_id
                    if hasattr(message, 'data') and isinstance(message.data, dict):
                        message_id = message.data.get("message_id")
                        if message_id:
                            consumed_ids.add(message_id)
                            consumer_consumed_ids[owner_id].add(message_id)
                            consumed_count += 1

                if consumed_count % 50 == 0:
                    print(f"ğŸ“¥ æ¶ˆè´¹è€… {consumer_id} å·²æ¶ˆè´¹ {consumed_count} æ¡æ¶ˆæ¯")
            else:
                # æ²¡æœ‰æ¶ˆæ¯ï¼ŒçŸ­æš‚ç­‰å¾…
                await asyncio.sleep(0.1)

        except Exception as e:  # pylint: disable=broad-except
            print(f"âŒ æ¶ˆè´¹è€… {consumer_id} æ¶ˆè´¹æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            await asyncio.sleep(0.1)

    print(
        f"âœ… æ¶ˆè´¹è€… {consumer_id} (owner_id: {owner_id}) åœæ­¢å·¥ä½œï¼Œæ€»å…±æ¶ˆè´¹ {consumed_count} æ¡æ¶ˆæ¯ï¼Œè§¦å‘ {rebalance_count} æ¬¡ rebalance"
    )


async def test_concurrent_producers_consumers():
    """
    æµ‹è¯•1ï¼šå¤šä¸ªæ¶ˆè´¹è€…æ–­æ–­ç»­ç»­åœ°æŠ•é€’ï¼Œç›´åˆ°1000ä¸ªä¸Šé™ï¼Œä¸€è¾¹å¤šä¸ªæ¶ˆè´¹è€…æŒç»­æ¶ˆè´¹ï¼Œ
    æœ€ç»ˆæ¶ˆè´¹è€…çš„æŠ•é€’id set=å¤šä¸ªæ¶ˆè´¹è€…çš„æ¶ˆè´¹ id set
    """
    print("\n" + "=" * 80)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ï¼šå¤šæ¶ˆè´¹è€…å¹¶å‘æŠ•é€’å’Œæ¶ˆè´¹")
    print("=" * 80)

    if not IMPORTS_AVAILABLE:
        print("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šä¾èµ–æ¨¡å—ä¸å¯ç”¨")
        return

    try:
        # è·å–ç®¡ç†å™¨å·¥å‚
        manager_factory = get_bean("redis_group_queue_manager_factory")

        # åˆ›å»ºæµ‹è¯•ç”¨çš„ç®¡ç†å™¨ï¼ˆé™åˆ¶1000æ¡æ¶ˆæ¯ï¼‰
        test_manager = await manager_factory.get_manager_with_config(
            key_prefix="concurrent_test_manager",
            max_total_messages=1000,
            auto_start=True,
        )

        # æ¸…ç†æµ‹è¯•æ•°æ®
        await test_manager.force_cleanup_and_reset()

        # å…±äº«æ•°æ®ç»“æ„ï¼ˆä½¿ç”¨setå­˜å‚¨æ¶ˆæ¯IDï¼‰
        delivered_ids: Set[str] = set()
        consumed_ids: Set[str] = set()
        consumer_consumed_ids: Dict[str, Set[str]] = {}  # æ¯ä¸ªæ¶ˆè´¹è€…çš„æ¶ˆè´¹IDé›†åˆ

        # é…ç½®å‚æ•°
        num_producers = 5  # 5ä¸ªç”Ÿäº§è€…
        num_consumers = 3  # 3ä¸ªæ¶ˆè´¹è€…
        messages_per_producer = 200  # æ¯ä¸ªç”Ÿäº§è€…æŠ•é€’200æ¡ï¼ˆæ€»å…±1000æ¡ï¼‰

        print("ğŸ“‹ æµ‹è¯•é…ç½®:")
        print(f"   - ç”Ÿäº§è€…æ•°é‡: {num_producers}")
        print(f"   - æ¶ˆè´¹è€…æ•°é‡: {num_consumers}")
        print(f"   - æ¯ä¸ªç”Ÿäº§è€…ç›®æ ‡æŠ•é€’: {messages_per_producer}")
        print("   - æœ€å¤§æ€»æ¶ˆæ¯æ•°: 1000")

        # åˆ›å»ºåœæ­¢äº‹ä»¶
        stop_event = asyncio.Event()

        # å¯åŠ¨æ¶ˆè´¹è€…ï¼ˆæ¯ä¸ªæ¶ˆè´¹è€…ä½¿ç”¨ä¸åŒçš„owner_idï¼Œå¹¶è®¾ç½®ä¸åŒçš„rebalanceæ¦‚ç‡ï¼‰
        consumer_tasks = []
        rebalance_probabilities = [0.03, 0.05, 0.07]  # ä¸åŒæ¶ˆè´¹è€…ä¸åŒçš„rebalanceæ¦‚ç‡
        for i in range(num_consumers):
            owner_id = f"consumer_{i + 1}_{int(time.time() * 1000)}"  # å”¯ä¸€çš„owner_id
            rebalance_prob = rebalance_probabilities[i % len(rebalance_probabilities)]
            task = asyncio.create_task(
                consumer_worker(
                    test_manager,
                    i + 1,
                    consumed_ids,
                    consumer_consumed_ids,
                    stop_event,
                    owner_id,
                    rebalance_probability=rebalance_prob,
                )
            )
            consumer_tasks.append(task)

        # ç­‰å¾…æ¶ˆè´¹è€…å¯åŠ¨å’ŒåŠ å…¥
        await asyncio.sleep(2)

        # å¯åŠ¨ç”Ÿäº§è€…
        producer_tasks = []
        for i in range(num_producers):
            task = asyncio.create_task(
                producer_worker(
                    test_manager,
                    i + 1,
                    messages_per_producer,
                    delivered_ids,
                    delay_range=(0.01, 0.05),  # è¾ƒå¿«çš„æŠ•é€’é€Ÿåº¦
                )
            )
            producer_tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ç”Ÿäº§è€…å®Œæˆ
        print("â³ ç­‰å¾…æ‰€æœ‰ç”Ÿäº§è€…å®Œæˆ...")
        await asyncio.gather(*producer_tasks)

        print(f"ğŸ“Š ç”Ÿäº§é˜¶æ®µå®Œæˆï¼Œå·²æŠ•é€’æ¶ˆæ¯æ•°: {len(delivered_ids)}")

        # ç­‰å¾…æ¶ˆè´¹è€…æ¶ˆè´¹å®Œæ‰€æœ‰æ¶ˆæ¯
        print("â³ ç­‰å¾…æ¶ˆè´¹è€…æ¶ˆè´¹å®Œæ‰€æœ‰æ¶ˆæ¯...")
        max_wait_time = 200  # æœ€å¤šç­‰å¾…200ç§’
        wait_start = time.time()

        while (
            len(consumed_ids) < len(delivered_ids)
            and (time.time() - wait_start) < max_wait_time
        ):
            await asyncio.sleep(1)
            print(f"ğŸ“ˆ æ¶ˆè´¹è¿›åº¦: {len(consumed_ids)}/{len(delivered_ids)}")
            if len(consumed_ids) >= len(delivered_ids):
                break

        # åœæ­¢æ¶ˆè´¹è€…
        stop_event.set()
        await asyncio.gather(*consumer_tasks, return_exceptions=True)

        # éªŒè¯ç»“æœ
        print("\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
        print(f"   - æŠ•é€’æ¶ˆæ¯æ•°: {len(delivered_ids)}")
        print(f"   - æ¶ˆè´¹æ¶ˆæ¯æ•°: {len(consumed_ids)}")
        print(f"   - æŠ•é€’IDé›†åˆå¤§å°: {len(delivered_ids)}")
        print(f"   - æ¶ˆè´¹IDé›†åˆå¤§å°: {len(consumed_ids)}")

        # æ£€æŸ¥IDé›†åˆæ˜¯å¦ç›¸ç­‰
        missing_in_consumed = delivered_ids - consumed_ids
        extra_in_consumed = consumed_ids - delivered_ids

        print(f"   - æŠ•é€’ä½†æœªæ¶ˆè´¹çš„æ¶ˆæ¯: {len(missing_in_consumed)}")
        print(f"   - æ¶ˆè´¹ä½†æœªæŠ•é€’çš„æ¶ˆæ¯: {len(extra_in_consumed)}")

        if missing_in_consumed:
            print(f"   - ç¼ºå¤±çš„æ¶ˆæ¯IDç¤ºä¾‹: {list(missing_in_consumed)[:5]}")

        if extra_in_consumed:
            print(f"   - å¤šä½™çš„æ¶ˆæ¯IDç¤ºä¾‹: {list(extra_in_consumed)[:5]}")

        # éªŒè¯æ¯ä¸ªæ¶ˆè´¹è€…çš„æ¶ˆè´¹æƒ…å†µ
        print("\nğŸ“Š å„æ¶ˆè´¹è€…æ¶ˆè´¹ç»Ÿè®¡:")
        total_consumer_messages = 0
        all_consumer_ids = set()

        for owner_id, ids in consumer_consumed_ids.items():
            print(f"   - {owner_id}: {len(ids)} æ¡æ¶ˆæ¯")
            total_consumer_messages += len(ids)
            all_consumer_ids.update(ids)

        # éªŒè¯æ¶ˆè´¹è€…ä¹‹é—´æ²¡æœ‰é‡å¤å¤„ç†æ¶ˆæ¯
        overlap_count = total_consumer_messages - len(all_consumer_ids)
        print(f"   - æ¶ˆè´¹è€…é—´é‡å¤å¤„ç†çš„æ¶ˆæ¯æ•°: {overlap_count}")

        # éªŒè¯åˆ†åŒºé—´æ¶ˆæ¯IDä¸é‡åˆ
        partition_overlap = len(consumed_ids) - len(all_consumer_ids)
        print(f"   - åˆ†åŒºé—´é‡å¤çš„æ¶ˆæ¯æ•°: {partition_overlap}")

        # æ–­è¨€éªŒè¯
        assert len(delivered_ids) > 0, "åº”è¯¥æœ‰æŠ•é€’çš„æ¶ˆæ¯"
        assert len(consumed_ids) > 0, "åº”è¯¥æœ‰æ¶ˆè´¹çš„æ¶ˆæ¯"
        assert (
            delivered_ids == consumed_ids
        ), f"æŠ•é€’IDé›†åˆåº”è¯¥ç­‰äºæ¶ˆè´¹IDé›†åˆï¼ŒæŠ•é€’={len(delivered_ids)}, æ¶ˆè´¹={len(consumed_ids)}"
        assert (
            overlap_count == 0
        ), f"æ¶ˆè´¹è€…ä¹‹é—´ä¸åº”è¯¥é‡å¤å¤„ç†æ¶ˆæ¯ï¼Œé‡å¤æ•°: {overlap_count}"
        assert (
            partition_overlap == 0
        ), f"åˆ†åŒºé—´ä¸åº”è¯¥æœ‰é‡å¤æ¶ˆæ¯ï¼Œé‡å¤æ•°: {partition_overlap}"

        print("âœ… æµ‹è¯•é€šè¿‡ï¼šæŠ•é€’IDé›†åˆ = æ¶ˆè´¹IDé›†åˆï¼Œä¸”å„æ¶ˆè´¹è€…/åˆ†åŒºé—´æ— é‡å¤")

        # æ¸…ç†
        await test_manager.shutdown()
        await test_manager.force_cleanup_and_reset()

    except Exception as e:  # pylint: disable=broad-except
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        raise


async def test_large_data_memory_usage():
    """
    æµ‹è¯•2ï¼šéšæœºç”Ÿæˆ20ä¸‡ä¸ª1KBå¤§å°çš„éšæœºåˆ†ç»„æ•°æ®ï¼Œå…¨éƒ¨æŠ•é€’åˆ°é˜Ÿåˆ—ä¸­ï¼Œç„¶åè·å–Rediså†…å­˜å ç”¨é‡
    """
    print("\n" + "=" * 80)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ï¼šå¤§æ•°æ®é‡å†…å­˜å ç”¨")
    print("=" * 80)

    if not IMPORTS_AVAILABLE:
        print("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šä¾èµ–æ¨¡å—ä¸å¯ç”¨")
        return

    try:
        # è·å–Rediså®¢æˆ·ç«¯ç”¨äºå†…å­˜ç»Ÿè®¡
        redis_provider = get_bean("redis_provider")
        redis_client = await redis_provider.get_named_client(
            "default", decode_responses=True
        )

        # è·å–ç®¡ç†å™¨å·¥å‚
        manager_factory = get_bean("redis_group_queue_manager_factory")

        # åˆ›å»ºæµ‹è¯•ç”¨çš„ç®¡ç†å™¨ï¼ˆå…è®¸å¤§é‡æ¶ˆæ¯ï¼‰
        test_manager = await manager_factory.get_manager_with_config(
            key_prefix="memory_test_manager",
            max_total_messages=250000,  # å…è®¸25ä¸‡æ¡æ¶ˆæ¯
            auto_start=True,
        )

        # æ¸…ç†æµ‹è¯•æ•°æ®
        await test_manager.force_cleanup_and_reset()

        # è·å–åˆå§‹å†…å­˜ä½¿ç”¨æƒ…å†µ
        initial_info = await redis_client.info("memory")
        initial_memory = initial_info.get("used_memory", 0)
        initial_memory_human = initial_info.get("used_memory_human", "0B")

        print(f"ğŸ“Š åˆå§‹Rediså†…å­˜ä½¿ç”¨: {initial_memory_human} ({initial_memory} bytes)")

        # é…ç½®å‚æ•°
        total_messages = 200000  # 20ä¸‡æ¡æ¶ˆæ¯
        message_size_kb = 1  # æ¯æ¡æ¶ˆæ¯1KB
        batch_size = 1000  # æ‰¹é‡æŠ•é€’å¤§å°

        print("ğŸ“‹ æµ‹è¯•é…ç½®:")
        print(f"   - æ€»æ¶ˆæ¯æ•°: {total_messages:,}")
        print(f"   - æ¯æ¡æ¶ˆæ¯å¤§å°: {message_size_kb}KB")
        print(f"   - æ‰¹é‡æŠ•é€’å¤§å°: {batch_size}")
        print(f"   - é¢„è®¡æ•°æ®é‡: {total_messages * message_size_kb / 1024:.1f}MB")

        # ç”Ÿæˆå¹¶æŠ•é€’æ¶ˆæ¯
        delivered_count = 0
        failed_count = 0
        start_time = time.time()

        print("ğŸš€ å¼€å§‹ç”Ÿæˆå’ŒæŠ•é€’æ¶ˆæ¯...")

        for batch_start in range(0, total_messages, batch_size):
            batch_end = min(batch_start + batch_size, total_messages)
            batch_tasks = []

            # åˆ›å»ºæ‰¹é‡æŠ•é€’ä»»åŠ¡
            for i in range(batch_start, batch_end):
                # ç”Ÿæˆéšæœºæ•°æ®
                random_data = generate_random_data(message_size_kb)
                message_id = f"large_msg_{i:06d}_{int(time.time() * 1000000)}"

                # éšæœºåˆ†ç»„
                group_key = generate_random_group_key()
                message = SimpleQueueItem(
                    data={
                        "message_id": message_id,
                        "sequence": i,
                        "group_key": group_key,
                        "payload": random_data,
                        "timestamp": time.time(),
                    },
                    item_type="large_test_message",
                )

                # åˆ›å»ºæŠ•é€’ä»»åŠ¡
                task = test_manager.deliver_message(group_key, message)
                batch_tasks.append(task)

            # å¹¶å‘æ‰§è¡Œæ‰¹é‡æŠ•é€’
            results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # ç»Ÿè®¡ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    failed_count += 1
                elif result:
                    delivered_count += 1
                else:
                    failed_count += 1

            # è¿›åº¦æŠ¥å‘Š
            if (batch_end) % 10000 == 0:
                elapsed = time.time() - start_time
                rate = delivered_count / elapsed if elapsed > 0 else 0
                print(
                    f"ğŸ“ˆ è¿›åº¦: {batch_end:,}/{total_messages:,} "
                    f"(æˆåŠŸ: {delivered_count:,}, å¤±è´¥: {failed_count:,}, "
                    f"é€Ÿç‡: {rate:.0f} msg/s)"
                )

        total_time = time.time() - start_time

        print("\nğŸ“Š æŠ•é€’å®Œæˆç»Ÿè®¡:")
        print(f"   - æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"   - æˆåŠŸæŠ•é€’: {delivered_count:,}")
        print(f"   - å¤±è´¥æŠ•é€’: {failed_count:,}")
        print(f"   - å¹³å‡é€Ÿç‡: {delivered_count / total_time:.0f} msg/s")

        # è·å–æŠ•é€’åçš„å†…å­˜ä½¿ç”¨æƒ…å†µ
        final_info = await redis_client.info("memory")
        final_memory = final_info.get("used_memory", 0)
        final_memory_human = final_info.get("used_memory_human", "0B")

        # è®¡ç®—å†…å­˜å¢é•¿
        memory_increase = final_memory - initial_memory
        memory_increase_mb = memory_increase / (1024 * 1024)

        print("\nğŸ’¾ å†…å­˜ä½¿ç”¨ç»Ÿè®¡:")
        print(f"   - åˆå§‹å†…å­˜: {initial_memory_human} ({initial_memory:,} bytes)")
        print(f"   - æœ€ç»ˆå†…å­˜: {final_memory_human} ({final_memory:,} bytes)")
        print(f"   - å†…å­˜å¢é•¿: {memory_increase_mb:.2f}MB ({memory_increase:,} bytes)")

        if delivered_count > 0:
            avg_memory_per_msg = memory_increase / delivered_count
            print(f"   - å¹³å‡æ¯æ¡æ¶ˆæ¯å†…å­˜å¼€é”€: {avg_memory_per_msg:.2f} bytes")
            print(
                f"   - å†…å­˜æ•ˆç‡: {(message_size_kb * 1024) / avg_memory_per_msg * 100:.1f}%"
            )

        # è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
        stats = await test_manager.get_stats(
            include_all_partitions=True, include_partition_details=True
        )
        total_queue_size = stats.get("actual_messages_in_queues", 0)

        print("\nğŸ“‹ é˜Ÿåˆ—ç»Ÿè®¡:")
        print(f"   - é˜Ÿåˆ—ä¸­æ¶ˆæ¯æ€»æ•°: {total_queue_size:,}")
        print(f"   - åˆ†åŒºæ•°é‡: {stats.get('total_queues', 0)}")
        print(f"   - éç©ºåˆ†åŒºæ•°é‡: {stats.get('non_empty_partitions', 0)}")
        print(f"   - æœ€å¤§åˆ†åŒºå¤§å°: {stats.get('max_partition_size', 0)}")
        print(f"   - æœ€å°åˆ†åŒºå¤§å°: {stats.get('min_partition_size', 0)}")

        # éªŒè¯ç»“æœ
        assert delivered_count > 0, "åº”è¯¥æœ‰æˆåŠŸæŠ•é€’çš„æ¶ˆæ¯"
        assert memory_increase > 0, "å†…å­˜ä½¿ç”¨åº”è¯¥æœ‰å¢é•¿"

        print("âœ… å¤§æ•°æ®é‡å†…å­˜å ç”¨æµ‹è¯•å®Œæˆ")

        # æ–°å¢ï¼šå•ä¸ªæ¶ˆè´¹è€…æµ‹è¯•ï¼ŒéªŒè¯åˆ†åŒºé”®ä¸é‡å¤
        print("\n" + "=" * 60)
        print("ğŸ§ª å¼€å§‹å•ä¸ªæ¶ˆè´¹è€…åˆ†åŒºé”®éªŒè¯æµ‹è¯•")
        print("=" * 60)

        # åˆ›å»ºå•ä¸ªæ¶ˆè´¹è€…
        consumer_owner_id = f"single_consumer_{int(time.time() * 1000)}"
        await test_manager.join_consumer(consumer_owner_id)
        print(f"ğŸ”„ æ¶ˆè´¹è€… {consumer_owner_id} å·²åŠ å…¥")

        # è¿›è¡Œ10æ¬¡pollæµ‹è¯•
        poll_count = 10
        target_messages_per_poll = 50
        all_partition_keys = []  # è®°å½•æ‰€æœ‰æ‰¹æ¬¡çš„åˆ†åŒºé”®
        batch_partition_stats = []  # æ¯æ‰¹æ¬¡çš„åˆ†åŒºç»Ÿè®¡

        print(f"ğŸ“‹ æ¶ˆè´¹è€…æµ‹è¯•é…ç½®:")
        print(f"   - Pollæ¬¡æ•°: {poll_count}")
        print(f"   - æ¯æ¬¡ç›®æ ‡æ¶ˆæ¯æ•°: {target_messages_per_poll}")

        for poll_idx in range(poll_count):
            print(f"\nğŸ” ç¬¬ {poll_idx + 1}/{poll_count} æ¬¡ Poll:")

            # è·å–æ¶ˆæ¯
            messages = await test_manager.get_messages(
                score_threshold=0, owner_id=consumer_owner_id
            )

            actual_count = len(messages)
            print(f"   - å®é™…è·å–æ¶ˆæ¯æ•°: {actual_count}")
            assert (
                actual_count == 50
            ), f"åº”è¯¥è·å–åˆ°50æ¡æ¶ˆæ¯ï¼Œå®é™…è·å–åˆ°{actual_count}æ¡æ¶ˆæ¯"

            all_group_keys = set()
            for message in messages:
                if hasattr(message, 'data') and isinstance(message.data, dict):
                    group_key = message.data["group_key"]
                    all_group_keys.add(group_key)

            print(f"   - æ‰¹æ¬¡å†…å”¯ä¸€åˆ†åŒºé”®æ•°: {len(all_group_keys)}")
            assert (
                len(all_group_keys) == 50
            ), f"åº”è¯¥è·å–åˆ°50ä¸ªå”¯ä¸€åˆ†åŒºé”®ï¼Œå®é™…è·å–åˆ°{len(all_group_keys)}ä¸ªå”¯ä¸€åˆ†åŒºé”®"

            hash_group_keys = set()
            for group_key in all_group_keys:
                hash_group_keys.add(
                    test_manager._hash_group_key_to_partition(group_key)
                )

            print(f"   - æ‰¹æ¬¡å†…å”¯ä¸€åˆ†åŒºé”®æ•°: {len(hash_group_keys)}")
            assert (
                len(hash_group_keys) == 50
            ), f"åº”è¯¥è·å–åˆ°50ä¸ªå”¯ä¸€åˆ†åŒºé”®ï¼Œå®é™…è·å–åˆ°{len(hash_group_keys)}ä¸ªå”¯ä¸€åˆ†åŒºé”®"

        # æ¸…ç†æµ‹è¯•æ•°æ®
        print("ğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®...")
        await test_manager.force_cleanup_and_reset()

        # è·å–æ¸…ç†åçš„å†…å­˜ä½¿ç”¨æƒ…å†µ
        cleanup_info = await redis_client.info("memory")
        cleanup_memory = cleanup_info.get("used_memory", 0)
        cleanup_memory_human = cleanup_info.get("used_memory_human", "0B")

        print(f"   - æ¸…ç†åå†…å­˜: {cleanup_memory_human} ({cleanup_memory:,} bytes)")
        print(f"   - é‡Šæ”¾å†…å­˜: {(final_memory - cleanup_memory) / (1024 * 1024):.2f}MB")

    except Exception as e:  # pylint: disable=broad-except
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        raise


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹è¿è¡ŒRedisåˆ†ç»„é˜Ÿåˆ—é«˜çº§æµ‹è¯•...")

    if not IMPORTS_AVAILABLE:
        print("âŒ æ— æ³•è¿è¡Œæµ‹è¯•ï¼šæ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥")
        return

    # æ£€æŸ¥Redisè¿æ¥
    try:
        redis_provider = get_bean("redis_provider")
        redis_client = await redis_provider.get_named_client(
            "default", decode_responses=True
        )
        await redis_client.ping()
        print("âœ… Redisè¿æ¥æ­£å¸¸")
    except Exception as e:  # pylint: disable=broad-except
        print(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿RedisæœåŠ¡æ­£åœ¨è¿è¡Œå¹¶ä¸”é…ç½®æ­£ç¡®")
        return

    # å®šä¹‰æµ‹è¯•å‡½æ•°
    tests = [test_concurrent_producers_consumers, test_large_data_memory_usage]

    passed = 0
    failed = 0

    # å¯åŠ¨æ—¶æ¸…ç†æ•°æ®åº“
    try:
        await redis_client.flushdb()
        print("ğŸ§¹ å¯åŠ¨æ—¶Redisæ•°æ®åº“å·²æ¸…ç†")
    except Exception as e:  # pylint: disable=broad-except
        print(f"âš ï¸ å¯åŠ¨æ—¶æ¸…ç†Redisæ•°æ®åº“å¤±è´¥: {e}")
        return

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    for test_func in tests:
        test_name = test_func.__name__
        print(f"\n{'='*60}")
        print(f"ğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")
        print(f"{'='*60}")

        try:
            start_time = time.time()
            await test_func()
            elapsed = time.time() - start_time
            print(f"âœ… æµ‹è¯•é€šè¿‡: {test_name} (è€—æ—¶: {elapsed:.2f}ç§’)")
            passed += 1
        except Exception as e:  # pylint: disable=broad-except
            elapsed = time.time() - start_time
            print(f"âŒ æµ‹è¯•å¤±è´¥: {test_name} (è€—æ—¶: {elapsed:.2f}ç§’)")
            print(f"é”™è¯¯: {e}")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            failed += 1

        # æµ‹è¯•é—´æ¸…ç†
        try:
            await redis_client.flushdb()
            print("ğŸ§¹ æµ‹è¯•é—´Redisæ•°æ®åº“å·²æ¸…ç†")
        except Exception as e:  # pylint: disable=broad-except
            print(f"âš ï¸ æµ‹è¯•é—´æ¸…ç†Redisæ•°æ®åº“å¤±è´¥: {e}")

    # è¾“å‡ºæ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print(f"{'='*60}")
    print(f"âœ… é€šè¿‡: {passed}")
    print(f"âŒ å¤±è´¥: {failed}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {passed / (passed + failed) * 100:.1f}%")

    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
    else:
        print(f"âš ï¸ æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥")


if __name__ == "__main__":
    asyncio.run(run_all_tests())

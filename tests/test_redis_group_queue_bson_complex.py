"""
Redisæ¶ˆæ¯åˆ†ç»„é˜Ÿåˆ—ç®¡ç†å™¨BSONåºåˆ—åŒ–é«˜çº§æµ‹è¯•

åŸºäºtest_redis_group_queue_complex.pyï¼Œæµ‹è¯•BSONåºåˆ—åŒ–æ¨¡å¼ä¸‹çš„ï¼š
1. å¤šæ¶ˆè´¹è€…å¹¶å‘æŠ•é€’å’Œæ¶ˆè´¹æµ‹è¯•
2. å¤§æ•°æ®é‡å†…å­˜å ç”¨æµ‹è¯•
"""

import asyncio
import sys
import os
import time
import random
import string
import json
import traceback
import base64
from typing import Set, Dict, Any

# Mockä¾èµ–æ¨¡å—
from unittest.mock import MagicMock

sys.modules['tanka_ai_toolkit'] = MagicMock()
sys.modules['tanka_ai_toolkit.utils'] = MagicMock()
sys.modules['tanka_ai_toolkit.utils.log_tools'] = MagicMock()
sys.modules['tanka_ai_toolkit.utils.log_tools.tanka_log'] = MagicMock()

try:
    from core.queue.redis_group_queue.redis_group_queue_item import (
        SimpleQueueItem,
        SerializationMode,
    )
    from core.queue.redis_group_queue.redis_msg_group_queue_manager_factory import (
        RedisGroupQueueManagerFactory,
    )
    from core.di.utils import get_bean_by_type
    from component.redis_provider import RedisProvider

    IMPORTS_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥æ ¸å¿ƒæ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    IMPORTS_AVAILABLE = False
    sys.exit(1)


def generate_random_data(size_kb: int = 1) -> Dict[str, Any]:
    """ç”ŸæˆæŒ‡å®šå¤§å°çš„éšæœºæ•°æ®"""
    # è®¡ç®—éœ€è¦çš„å­—ç¬¦æ•°ï¼ˆå¤§çº¦ï¼‰
    target_size = size_kb * 1024

    # ç”Ÿæˆéšæœºå­—ç¬¦ä¸²ä½œä¸ºä¸»è¦å†…å®¹
    content_size = target_size - 200  # é¢„ç•™ä¸€äº›ç©ºé—´ç»™å…¶ä»–å­—æ®µ
    random_content = ''.join(
        random.choices(string.ascii_letters + string.digits, k=content_size)
    )

    return {
        "id": f"data_{random.randint(100000, 999999)}",
        "content": random_content,
        "timestamp": time.time(),
        "metadata": {
            "type": "test_data",
            "size_kb": size_kb,
            "generated_at": time.time(),
        },
    }


def generate_random_group_key() -> str:
    """ç”Ÿæˆéšæœºåˆ†ç»„é”®"""
    return f"group_{random.randint(1, 1000)}"


def generate_binary_data(size_bytes: int = 1024) -> bytes:
    """ç”ŸæˆæŒ‡å®šå¤§å°çš„éšæœºäºŒè¿›åˆ¶æ•°æ®"""
    return bytes([random.randint(0, 255) for _ in range(size_bytes)])


async def bson_producer_worker(
    manager,
    producer_id: int,
    target_count: int,
    delivered_ids: Set[str],
    delay_range: tuple = (0.01, 0.1),
):
    """
    BSONç”Ÿäº§è€…å·¥ä½œåç¨‹

    Args:
        manager: é˜Ÿåˆ—ç®¡ç†å™¨å®ä¾‹ï¼ˆBSONæ¨¡å¼ï¼‰
        producer_id: ç”Ÿäº§è€…ID
        target_count: ç›®æ ‡æŠ•é€’æ•°é‡
        delivered_ids: å·²æŠ•é€’æ¶ˆæ¯IDé›†åˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        delay_range: å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
    """
    delivered_count = 0

    print(f"ğŸš€ BSONç”Ÿäº§è€… {producer_id} å¼€å§‹å·¥ä½œï¼Œç›®æ ‡æŠ•é€’ {target_count} æ¡æ¶ˆæ¯")

    while delivered_count < target_count:
        try:
            # åˆ›å»ºæ¶ˆæ¯ï¼ŒåŒ…å«äºŒè¿›åˆ¶æ•°æ®
            message_id = f"bson_producer_{producer_id}_msg_{delivered_count + 1}_{int(time.time() * 1000000)}"

            # ç”Ÿæˆä¸€äº›äºŒè¿›åˆ¶æ•°æ®å¹¶ç¼–ç ä¸ºbase64
            binary_data = generate_binary_data(256)  # 256å­—èŠ‚çš„äºŒè¿›åˆ¶æ•°æ®
            encoded_binary = base64.b64encode(binary_data).decode('utf-8')

            message = SimpleQueueItem(
                data={
                    "producer_id": producer_id,
                    "message_id": message_id,
                    "sequence": delivered_count + 1,
                    "content": f"BSON Message from producer {producer_id}, sequence {delivered_count + 1}",
                    "timestamp": time.time(),
                    "binary_payload": encoded_binary,
                    "metadata": {
                        "serialization": "bson",
                        "binary_size": len(binary_data),
                        "unicode_test": f"ä¸­æ–‡æµ‹è¯•{producer_id}ğŸš€",
                    },
                },
                item_type="bson_test_message",
            )

            # éšæœºé€‰æ‹©åˆ†ç»„
            group_key = generate_random_group_key()

            # å°è¯•æŠ•é€’
            success = await manager.deliver_message(group_key, message)

            if success:
                delivered_ids.add(message_id)
                delivered_count += 1

                if delivered_count % 50 == 0:
                    print(
                        f"ğŸ“¤ BSONç”Ÿäº§è€… {producer_id} å·²æŠ•é€’ {delivered_count}/{target_count} æ¡æ¶ˆæ¯"
                    )
            else:
                # æŠ•é€’å¤±è´¥ï¼Œå¯èƒ½æ˜¯è¾¾åˆ°äº†ä¸Šé™ï¼Œç¨ç­‰ä¸€ä¸‹å†è¯•
                await asyncio.sleep(random.uniform(0.1, 0.5))

            # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿæ–­æ–­ç»­ç»­çš„æŠ•é€’
            await asyncio.sleep(random.uniform(*delay_range))

        except Exception as e:
            print(f"âŒ BSONç”Ÿäº§è€… {producer_id} æŠ•é€’æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            await asyncio.sleep(0.1)

    print(f"âœ… BSONç”Ÿäº§è€… {producer_id} å®Œæˆå·¥ä½œï¼Œå®é™…æŠ•é€’ {delivered_count} æ¡æ¶ˆæ¯")


async def bson_consumer_worker(
    manager,
    consumer_id: int,
    consumed_ids: Set[str],
    consumer_consumed_ids: Dict[str, Set[str]],
    stop_event: asyncio.Event,
    owner_id: str,
    rebalance_probability: float = 0.05,
):
    """
    BSONæ¶ˆè´¹è€…å·¥ä½œåç¨‹

    Args:
        manager: é˜Ÿåˆ—ç®¡ç†å™¨å®ä¾‹ï¼ˆBSONæ¨¡å¼ï¼‰
        consumer_id: æ¶ˆè´¹è€…ID
        consumed_ids: å…¨å±€å·²æ¶ˆè´¹æ¶ˆæ¯IDé›†åˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        consumer_consumed_ids: æ¯ä¸ªæ¶ˆè´¹è€…çš„æ¶ˆè´¹IDé›†åˆ
        stop_event: åœæ­¢äº‹ä»¶
        owner_id: æ¶ˆè´¹è€…çš„owner_id
        rebalance_probability: æ¯æ¬¡å¾ªç¯è§¦å‘rebalanceçš„æ¦‚ç‡
    """
    consumed_count = 0
    rebalance_count = 0
    binary_verification_count = 0

    # å…ˆåŠ å…¥æ¶ˆè´¹è€…
    await manager.join_consumer(owner_id)
    print(f"ğŸ”„ BSONæ¶ˆè´¹è€… {consumer_id} (owner_id: {owner_id}) å·²åŠ å…¥å¹¶å¼€å§‹å·¥ä½œ")

    # åˆå§‹åŒ–è¯¥æ¶ˆè´¹è€…çš„æ¶ˆè´¹IDé›†åˆ
    if owner_id not in consumer_consumed_ids:
        consumer_consumed_ids[owner_id] = set()

    while not stop_event.is_set():
        try:
            # éšæœºè§¦å‘rebalance
            if random.random() < rebalance_probability:
                try:
                    result = await manager.rebalance_partitions()
                    rebalance_count += 1
                    print(
                        f"ğŸ”„ BSONæ¶ˆè´¹è€… {consumer_id} è§¦å‘ç¬¬ {rebalance_count} æ¬¡ rebalanceï¼Œç»“æœ: {result}"
                    )
                except Exception as rebalance_error:
                    print(
                        f"âš ï¸ BSONæ¶ˆè´¹è€… {consumer_id} rebalanceå¤±è´¥: {rebalance_error}"
                    )

            # è·å–æ¶ˆæ¯ï¼ˆä½¿ç”¨æŒ‡å®šçš„owner_idï¼‰
            messages = await manager.get_messages(score_threshold=0, owner_id=owner_id)

            if messages:
                for message in messages:
                    # ä»æ¶ˆæ¯æ•°æ®ä¸­æå–message_id
                    if hasattr(message, 'data') and isinstance(message.data, dict):
                        message_id = message.data.get("message_id")
                        if message_id:
                            consumed_ids.add(message_id)
                            consumer_consumed_ids[owner_id].add(message_id)
                            consumed_count += 1

                            # éªŒè¯äºŒè¿›åˆ¶æ•°æ®å®Œæ•´æ€§ï¼ˆéšæœºæŠ½æ ·ï¼‰
                            if random.random() < 0.1:  # 10%çš„æ¦‚ç‡éªŒè¯
                                try:
                                    binary_payload = message.data.get("binary_payload")
                                    if binary_payload:
                                        decoded_binary = base64.b64decode(
                                            binary_payload
                                        )
                                        expected_size = message.data.get(
                                            "metadata", {}
                                        ).get("binary_size", 0)
                                        if len(decoded_binary) == expected_size:
                                            binary_verification_count += 1
                                except Exception as verify_error:
                                    print(
                                        f"âš ï¸ BSONæ¶ˆè´¹è€… {consumer_id} äºŒè¿›åˆ¶æ•°æ®éªŒè¯å¤±è´¥: {verify_error}"
                                    )

                if consumed_count % 50 == 0:
                    print(
                        f"ğŸ“¥ BSONæ¶ˆè´¹è€… {consumer_id} å·²æ¶ˆè´¹ {consumed_count} æ¡æ¶ˆæ¯ "
                        f"(äºŒè¿›åˆ¶éªŒè¯: {binary_verification_count})"
                    )
            else:
                # æ²¡æœ‰æ¶ˆæ¯ï¼ŒçŸ­æš‚ç­‰å¾…
                await asyncio.sleep(0.1)

        except Exception as e:
            print(f"âŒ BSONæ¶ˆè´¹è€… {consumer_id} æ¶ˆè´¹æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            await asyncio.sleep(0.1)

    print(
        f"âœ… BSONæ¶ˆè´¹è€… {consumer_id} (owner_id: {owner_id}) åœæ­¢å·¥ä½œï¼Œ"
        f"æ€»å…±æ¶ˆè´¹ {consumed_count} æ¡æ¶ˆæ¯ï¼Œè§¦å‘ {rebalance_count} æ¬¡ rebalanceï¼Œ"
        f"äºŒè¿›åˆ¶éªŒè¯ {binary_verification_count} æ¬¡"
    )


async def test_bson_concurrent_producers_consumers():
    """
    æµ‹è¯•1ï¼šBSONæ¨¡å¼ä¸‹å¤šä¸ªæ¶ˆè´¹è€…å¹¶å‘æŠ•é€’å’Œæ¶ˆè´¹æµ‹è¯•
    """
    print("\n" + "=" * 80)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ï¼šBSONæ¨¡å¼å¤šæ¶ˆè´¹è€…å¹¶å‘æŠ•é€’å’Œæ¶ˆè´¹")
    print("=" * 80)

    try:
        # è·å–ç®¡ç†å™¨å·¥å‚å’ŒRedisæä¾›è€…
        manager_factory = get_bean_by_type(RedisGroupQueueManagerFactory)
        redis_provider = get_bean_by_type(RedisProvider)

        # æ¸…ç†Redisæ•°æ®åº“
        redis_client = await redis_provider.get_named_client(
            "default", decode_responses=True
        )
        await redis_client.flushdb()
        print("ğŸ§¹ Redisæ•°æ®åº“å·²æ¸…ç†")

        # åˆ›å»ºBSONæ¨¡å¼çš„æµ‹è¯•ç®¡ç†å™¨ï¼ˆé™åˆ¶1000æ¡æ¶ˆæ¯ï¼‰
        test_manager = await manager_factory.get_manager_with_config(
            key_prefix="bson_concurrent_test",
            serialization_mode=SerializationMode.BSON,
            max_total_messages=1000,
            auto_start=True,
        )

        # æ¸…ç†æµ‹è¯•æ•°æ®
        await test_manager.force_cleanup_and_reset()

        # å…±äº«æ•°æ®ç»“æ„ï¼ˆä½¿ç”¨setå­˜å‚¨æ¶ˆæ¯IDï¼‰
        delivered_ids: Set[str] = set()
        consumed_ids: Set[str] = set()
        consumer_consumed_ids: Dict[str, Set[str]] = {}

        # é…ç½®å‚æ•°
        num_producers = 5  # 5ä¸ªç”Ÿäº§è€…
        num_consumers = 3  # 3ä¸ªæ¶ˆè´¹è€…
        messages_per_producer = 200  # æ¯ä¸ªç”Ÿäº§è€…æŠ•é€’200æ¡ï¼ˆæ€»å…±1000æ¡ï¼‰

        print("ğŸ“‹ BSONæµ‹è¯•é…ç½®:")
        print(f"   - åºåˆ—åŒ–æ¨¡å¼: BSON")
        print(f"   - ç”Ÿäº§è€…æ•°é‡: {num_producers}")
        print(f"   - æ¶ˆè´¹è€…æ•°é‡: {num_consumers}")
        print(f"   - æ¯ä¸ªç”Ÿäº§è€…ç›®æ ‡æŠ•é€’: {messages_per_producer}")
        print("   - æœ€å¤§æ€»æ¶ˆæ¯æ•°: 1000")
        print("   - åŒ…å«äºŒè¿›åˆ¶æ•°æ®éªŒè¯")

        # åˆ›å»ºåœæ­¢äº‹ä»¶
        stop_event = asyncio.Event()

        # å¯åŠ¨æ¶ˆè´¹è€…ï¼ˆæ¯ä¸ªæ¶ˆè´¹è€…ä½¿ç”¨ä¸åŒçš„owner_idï¼‰
        consumer_tasks = []
        rebalance_probabilities = [0.03, 0.05, 0.07]
        for i in range(num_consumers):
            owner_id = f"bson_consumer_{i + 1}_{int(time.time() * 1000)}"
            rebalance_prob = rebalance_probabilities[i % len(rebalance_probabilities)]
            task = asyncio.create_task(
                bson_consumer_worker(
                    test_manager,
                    i + 1,
                    consumed_ids,
                    consumer_consumed_ids,
                    stop_event,
                    owner_id,
                    rebalance_probability=rebalance_prob,
                )
            )
            consumer_tasks.append(task)

        # ç­‰å¾…æ¶ˆè´¹è€…å¯åŠ¨å’ŒåŠ å…¥
        await asyncio.sleep(2)

        # å¯åŠ¨ç”Ÿäº§è€…
        producer_tasks = []
        for i in range(num_producers):
            task = asyncio.create_task(
                bson_producer_worker(
                    test_manager,
                    i + 1,
                    messages_per_producer,
                    delivered_ids,
                    delay_range=(0.01, 0.05),
                )
            )
            producer_tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ç”Ÿäº§è€…å®Œæˆ
        print("â³ ç­‰å¾…æ‰€æœ‰BSONç”Ÿäº§è€…å®Œæˆ...")
        await asyncio.gather(*producer_tasks)

        print(f"ğŸ“Š BSONç”Ÿäº§é˜¶æ®µå®Œæˆï¼Œå·²æŠ•é€’æ¶ˆæ¯æ•°: {len(delivered_ids)}")

        # ç­‰å¾…æ¶ˆè´¹è€…æ¶ˆè´¹å®Œæ‰€æœ‰æ¶ˆæ¯
        print("â³ ç­‰å¾…BSONæ¶ˆè´¹è€…æ¶ˆè´¹å®Œæ‰€æœ‰æ¶ˆæ¯...")
        max_wait_time = 200  # æœ€å¤šç­‰å¾…200ç§’
        wait_start = time.time()

        while (
            len(consumed_ids) < len(delivered_ids)
            and (time.time() - wait_start) < max_wait_time
        ):
            await asyncio.sleep(1)
            print(f"ğŸ“ˆ BSONæ¶ˆè´¹è¿›åº¦: {len(consumed_ids)}/{len(delivered_ids)}")
            if len(consumed_ids) >= len(delivered_ids):
                break

        # åœæ­¢æ¶ˆè´¹è€…
        stop_event.set()
        await asyncio.gather(*consumer_tasks, return_exceptions=True)

        # éªŒè¯ç»“æœ
        print("\nğŸ“Š BSONæµ‹è¯•ç»“æœç»Ÿè®¡:")
        print(f"   - æŠ•é€’æ¶ˆæ¯æ•°: {len(delivered_ids)}")
        print(f"   - æ¶ˆè´¹æ¶ˆæ¯æ•°: {len(consumed_ids)}")
        print(f"   - æŠ•é€’IDé›†åˆå¤§å°: {len(delivered_ids)}")
        print(f"   - æ¶ˆè´¹IDé›†åˆå¤§å°: {len(consumed_ids)}")

        # æ£€æŸ¥IDé›†åˆæ˜¯å¦ç›¸ç­‰
        missing_in_consumed = delivered_ids - consumed_ids
        extra_in_consumed = consumed_ids - delivered_ids

        print(f"   - æŠ•é€’ä½†æœªæ¶ˆè´¹çš„æ¶ˆæ¯: {len(missing_in_consumed)}")
        print(f"   - æ¶ˆè´¹ä½†æœªæŠ•é€’çš„æ¶ˆæ¯: {len(extra_in_consumed)}")

        if missing_in_consumed:
            print(f"   - ç¼ºå¤±çš„æ¶ˆæ¯IDç¤ºä¾‹: {list(missing_in_consumed)[:5]}")

        if extra_in_consumed:
            print(f"   - å¤šä½™çš„æ¶ˆæ¯IDç¤ºä¾‹: {list(extra_in_consumed)[:5]}")

        # éªŒè¯æ¯ä¸ªæ¶ˆè´¹è€…çš„æ¶ˆè´¹æƒ…å†µ
        print("\nğŸ“Š å„BSONæ¶ˆè´¹è€…æ¶ˆè´¹ç»Ÿè®¡:")
        total_consumer_messages = 0
        all_consumer_ids = set()

        for owner_id, ids in consumer_consumed_ids.items():
            print(f"   - {owner_id}: {len(ids)} æ¡æ¶ˆæ¯")
            total_consumer_messages += len(ids)
            all_consumer_ids.update(ids)

        # éªŒè¯æ¶ˆè´¹è€…ä¹‹é—´æ²¡æœ‰é‡å¤å¤„ç†æ¶ˆæ¯
        overlap_count = total_consumer_messages - len(all_consumer_ids)
        print(f"   - æ¶ˆè´¹è€…é—´é‡å¤å¤„ç†çš„æ¶ˆæ¯æ•°: {overlap_count}")

        # éªŒè¯åˆ†åŒºé—´æ¶ˆæ¯IDä¸é‡åˆ
        partition_overlap = len(consumed_ids) - len(all_consumer_ids)
        print(f"   - åˆ†åŒºé—´é‡å¤çš„æ¶ˆæ¯æ•°: {partition_overlap}")

        # æ–­è¨€éªŒè¯
        assert len(delivered_ids) > 0, "åº”è¯¥æœ‰æŠ•é€’çš„æ¶ˆæ¯"
        assert len(consumed_ids) > 0, "åº”è¯¥æœ‰æ¶ˆè´¹çš„æ¶ˆæ¯"
        assert (
            delivered_ids == consumed_ids
        ), f"æŠ•é€’IDé›†åˆåº”è¯¥ç­‰äºæ¶ˆè´¹IDé›†åˆï¼ŒæŠ•é€’={len(delivered_ids)}, æ¶ˆè´¹={len(consumed_ids)}"
        assert (
            overlap_count == 0
        ), f"æ¶ˆè´¹è€…ä¹‹é—´ä¸åº”è¯¥é‡å¤å¤„ç†æ¶ˆæ¯ï¼Œé‡å¤æ•°: {overlap_count}"
        assert (
            partition_overlap == 0
        ), f"åˆ†åŒºé—´ä¸åº”è¯¥æœ‰é‡å¤æ¶ˆæ¯ï¼Œé‡å¤æ•°: {partition_overlap}"

        print("âœ… BSONæµ‹è¯•é€šè¿‡ï¼šæŠ•é€’IDé›†åˆ = æ¶ˆè´¹IDé›†åˆï¼Œä¸”å„æ¶ˆè´¹è€…/åˆ†åŒºé—´æ— é‡å¤")

        # æ¸…ç†
        await test_manager.shutdown()
        await test_manager.force_cleanup_and_reset()
        await manager_factory.stop_all_managers()
        await redis_client.flushdb()
        await redis_client.close()

    except Exception as e:
        print(f"âŒ BSONå¹¶å‘æµ‹è¯•å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        raise


async def test_bson_large_data_memory_usage():
    """
    æµ‹è¯•2ï¼šBSONæ¨¡å¼ä¸‹å¤§æ•°æ®é‡å†…å­˜å ç”¨æµ‹è¯•
    """
    print("\n" + "=" * 80)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ï¼šBSONæ¨¡å¼å¤§æ•°æ®é‡å†…å­˜å ç”¨")
    print("=" * 80)

    try:
        # è·å–ç®¡ç†å™¨å·¥å‚å’ŒRedisæä¾›è€…
        manager_factory = get_bean_by_type(RedisGroupQueueManagerFactory)
        redis_provider = get_bean_by_type(RedisProvider)

        # è·å–Rediså®¢æˆ·ç«¯ç”¨äºå†…å­˜ç»Ÿè®¡
        redis_client = await redis_provider.get_named_client(
            "default", decode_responses=True
        )
        await redis_client.flushdb()

        # åˆ›å»ºBSONæ¨¡å¼çš„æµ‹è¯•ç®¡ç†å™¨ï¼ˆå…è®¸å¤§é‡æ¶ˆæ¯ï¼‰
        test_manager = await manager_factory.get_manager_with_config(
            key_prefix="bson_memory_test",
            serialization_mode=SerializationMode.BSON,
            max_total_messages=250000,  # å…è®¸25ä¸‡æ¡æ¶ˆæ¯
            auto_start=True,
        )

        # æ¸…ç†æµ‹è¯•æ•°æ®
        await test_manager.force_cleanup_and_reset()

        # è·å–åˆå§‹å†…å­˜ä½¿ç”¨æƒ…å†µ
        initial_info = await redis_client.info("memory")
        initial_memory = initial_info.get("used_memory", 0)
        initial_memory_human = initial_info.get("used_memory_human", "0B")

        print(f"ğŸ“Š åˆå§‹Rediså†…å­˜ä½¿ç”¨: {initial_memory_human} ({initial_memory} bytes)")

        # é…ç½®å‚æ•°ï¼ˆç›¸æ¯”JSONç‰ˆæœ¬ï¼Œå‡å°‘æ•°é‡å› ä¸ºBSONæ•°æ®æ›´å¤§ï¼‰
        total_messages = 100000  # 10ä¸‡æ¡æ¶ˆæ¯ï¼ˆBSONæ•°æ®æ›´å¤§ï¼‰
        message_size_kb = 1  # æ¯æ¡æ¶ˆæ¯1KB
        batch_size = 1000  # æ‰¹é‡æŠ•é€’å¤§å°
        binary_data_size = 512  # æ¯æ¡æ¶ˆæ¯åŒ…å«512å­—èŠ‚äºŒè¿›åˆ¶æ•°æ®

        print("ğŸ“‹ BSONæµ‹è¯•é…ç½®:")
        print(f"   - åºåˆ—åŒ–æ¨¡å¼: BSON")
        print(f"   - æ€»æ¶ˆæ¯æ•°: {total_messages:,}")
        print(f"   - æ¯æ¡æ¶ˆæ¯å¤§å°: {message_size_kb}KB")
        print(f"   - äºŒè¿›åˆ¶æ•°æ®å¤§å°: {binary_data_size} bytes")
        print(f"   - æ‰¹é‡æŠ•é€’å¤§å°: {batch_size}")
        print(f"   - é¢„è®¡æ•°æ®é‡: {total_messages * message_size_kb / 1024:.1f}MB")

        # ç”Ÿæˆå¹¶æŠ•é€’æ¶ˆæ¯
        delivered_count = 0
        failed_count = 0
        start_time = time.time()

        print("ğŸš€ å¼€å§‹ç”Ÿæˆå’ŒæŠ•é€’BSONæ¶ˆæ¯...")

        for batch_start in range(0, total_messages, batch_size):
            batch_end = min(batch_start + batch_size, total_messages)
            batch_tasks = []

            # åˆ›å»ºæ‰¹é‡æŠ•é€’ä»»åŠ¡
            for i in range(batch_start, batch_end):
                # ç”Ÿæˆéšæœºæ•°æ®
                random_data = generate_random_data(message_size_kb)
                message_id = f"bson_large_msg_{i:06d}_{int(time.time() * 1000000)}"

                # ç”ŸæˆäºŒè¿›åˆ¶æ•°æ®
                binary_data = generate_binary_data(binary_data_size)
                encoded_binary = base64.b64encode(binary_data).decode('utf-8')

                # éšæœºåˆ†ç»„
                group_key = generate_random_group_key()
                message = SimpleQueueItem(
                    data={
                        "message_id": message_id,
                        "sequence": i,
                        "group_key": group_key,
                        "payload": random_data,
                        "binary_data": encoded_binary,
                        "timestamp": time.time(),
                        "metadata": {
                            "serialization": "bson",
                            "binary_size": len(binary_data),
                            "unicode_test": f"BSONå¤§æ•°æ®æµ‹è¯•{i}ğŸ”¥",
                            "nested_data": {
                                "level1": {"level2": {"value": f"nested_{i}"}},
                                "array": [1, 2, 3, f"item_{i}"],
                                "boolean": i % 2 == 0,
                                "null_field": None,
                            },
                        },
                    },
                    item_type="bson_large_test_message",
                )

                # åˆ›å»ºæŠ•é€’ä»»åŠ¡
                task = test_manager.deliver_message(group_key, message)
                batch_tasks.append(task)

            # å¹¶å‘æ‰§è¡Œæ‰¹é‡æŠ•é€’
            results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # ç»Ÿè®¡ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    failed_count += 1
                elif result:
                    delivered_count += 1
                else:
                    failed_count += 1

            # è¿›åº¦æŠ¥å‘Š
            if (batch_end) % 10000 == 0:
                elapsed = time.time() - start_time
                rate = delivered_count / elapsed if elapsed > 0 else 0
                print(
                    f"ğŸ“ˆ BSONè¿›åº¦: {batch_end:,}/{total_messages:,} "
                    f"(æˆåŠŸ: {delivered_count:,}, å¤±è´¥: {failed_count:,}, "
                    f"é€Ÿç‡: {rate:.0f} msg/s)"
                )

        total_time = time.time() - start_time

        print("\nğŸ“Š BSONæŠ•é€’å®Œæˆç»Ÿè®¡:")
        print(f"   - æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"   - æˆåŠŸæŠ•é€’: {delivered_count:,}")
        print(f"   - å¤±è´¥æŠ•é€’: {failed_count:,}")
        print(f"   - å¹³å‡é€Ÿç‡: {delivered_count / total_time:.0f} msg/s")

        # è·å–æŠ•é€’åçš„å†…å­˜ä½¿ç”¨æƒ…å†µ
        final_info = await redis_client.info("memory")
        final_memory = final_info.get("used_memory", 0)
        final_memory_human = final_info.get("used_memory_human", "0B")

        # è®¡ç®—å†…å­˜å¢é•¿
        memory_increase = final_memory - initial_memory
        memory_increase_mb = memory_increase / (1024 * 1024)

        print("\nğŸ’¾ BSONå†…å­˜ä½¿ç”¨ç»Ÿè®¡:")
        print(f"   - åˆå§‹å†…å­˜: {initial_memory_human} ({initial_memory:,} bytes)")
        print(f"   - æœ€ç»ˆå†…å­˜: {final_memory_human} ({final_memory:,} bytes)")
        print(f"   - å†…å­˜å¢é•¿: {memory_increase_mb:.2f}MB ({memory_increase:,} bytes)")

        if delivered_count > 0:
            avg_memory_per_msg = memory_increase / delivered_count
            print(f"   - å¹³å‡æ¯æ¡æ¶ˆæ¯å†…å­˜å¼€é”€: {avg_memory_per_msg:.2f} bytes")
            print(
                f"   - å†…å­˜æ•ˆç‡: {(message_size_kb * 1024) / avg_memory_per_msg * 100:.1f}%"
            )

        # è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
        stats = await test_manager.get_stats(
            include_all_partitions=True, include_partition_details=True
        )
        total_queue_size = stats.get("actual_messages_in_queues", 0)

        print("\nğŸ“‹ BSONé˜Ÿåˆ—ç»Ÿè®¡:")
        print(f"   - é˜Ÿåˆ—ä¸­æ¶ˆæ¯æ€»æ•°: {total_queue_size:,}")
        print(f"   - åˆ†åŒºæ•°é‡: {stats.get('total_queues', 0)}")
        print(f"   - éç©ºåˆ†åŒºæ•°é‡: {stats.get('non_empty_partitions', 0)}")
        print(f"   - æœ€å¤§åˆ†åŒºå¤§å°: {stats.get('max_partition_size', 0)}")
        print(f"   - æœ€å°åˆ†åŒºå¤§å°: {stats.get('min_partition_size', 0)}")

        # éªŒè¯ç»“æœ
        assert delivered_count > 0, "åº”è¯¥æœ‰æˆåŠŸæŠ•é€’çš„æ¶ˆæ¯"
        assert memory_increase > 0, "å†…å­˜ä½¿ç”¨åº”è¯¥æœ‰å¢é•¿"

        print("âœ… BSONå¤§æ•°æ®é‡å†…å­˜å ç”¨æµ‹è¯•å®Œæˆ")

        # BSONç‰¹æœ‰ï¼šå•ä¸ªæ¶ˆè´¹è€…æµ‹è¯•ï¼ŒéªŒè¯äºŒè¿›åˆ¶æ•°æ®å®Œæ•´æ€§
        print("\n" + "=" * 60)
        print("ğŸ§ª å¼€å§‹BSONå•ä¸ªæ¶ˆè´¹è€…äºŒè¿›åˆ¶æ•°æ®éªŒè¯æµ‹è¯•")
        print("=" * 60)

        # åˆ›å»ºå•ä¸ªæ¶ˆè´¹è€…
        consumer_owner_id = f"bson_single_consumer_{int(time.time() * 1000)}"
        await test_manager.join_consumer(consumer_owner_id)
        print(f"ğŸ”„ BSONæ¶ˆè´¹è€… {consumer_owner_id} å·²åŠ å…¥")

        # è¿›è¡Œ10æ¬¡pollæµ‹è¯•ï¼Œæ¯æ¬¡éªŒè¯äºŒè¿›åˆ¶æ•°æ®
        poll_count = 10
        target_messages_per_poll = 50
        binary_verification_success = 0
        binary_verification_total = 0

        print(f"ğŸ“‹ BSONæ¶ˆè´¹è€…æµ‹è¯•é…ç½®:")
        print(f"   - Pollæ¬¡æ•°: {poll_count}")
        print(f"   - æ¯æ¬¡ç›®æ ‡æ¶ˆæ¯æ•°: {target_messages_per_poll}")
        print(f"   - åŒ…å«äºŒè¿›åˆ¶æ•°æ®å®Œæ•´æ€§éªŒè¯")

        for poll_idx in range(poll_count):
            print(f"\nğŸ” ç¬¬ {poll_idx + 1}/{poll_count} æ¬¡ BSON Poll:")

            # è·å–æ¶ˆæ¯
            messages = await test_manager.get_messages(
                score_threshold=0, owner_id=consumer_owner_id
            )

            actual_count = len(messages)
            print(f"   - å®é™…è·å–æ¶ˆæ¯æ•°: {actual_count}")
            assert (
                actual_count == 50
            ), f"åº”è¯¥è·å–åˆ°50æ¡æ¶ˆæ¯ï¼Œå®é™…è·å–åˆ°{actual_count}æ¡æ¶ˆæ¯"

            # éªŒè¯æ¶ˆæ¯å’ŒäºŒè¿›åˆ¶æ•°æ®
            all_group_keys = set()
            for message in messages:
                if hasattr(message, 'data') and isinstance(message.data, dict):
                    group_key = message.data["group_key"]
                    all_group_keys.add(group_key)

                    # éªŒè¯äºŒè¿›åˆ¶æ•°æ®å®Œæ•´æ€§
                    try:
                        binary_data_encoded = message.data.get("binary_data")
                        if binary_data_encoded:
                            decoded_binary = base64.b64decode(binary_data_encoded)
                            expected_size = message.data.get("metadata", {}).get(
                                "binary_size", 0
                            )
                            binary_verification_total += 1
                            if len(decoded_binary) == expected_size:
                                binary_verification_success += 1
                    except Exception as verify_error:
                        print(f"âš ï¸ äºŒè¿›åˆ¶æ•°æ®éªŒè¯å¤±è´¥: {verify_error}")

            print(f"   - æ‰¹æ¬¡å†…å”¯ä¸€åˆ†åŒºé”®æ•°: {len(all_group_keys)}")
            assert (
                len(all_group_keys) == 50
            ), f"åº”è¯¥è·å–åˆ°50ä¸ªå”¯ä¸€åˆ†åŒºé”®ï¼Œå®é™…è·å–åˆ°{len(all_group_keys)}ä¸ªå”¯ä¸€åˆ†åŒºé”®"

            # éªŒè¯åˆ†åŒºhash
            hash_group_keys = set()
            for group_key in all_group_keys:
                hash_group_keys.add(
                    test_manager._hash_group_key_to_partition(group_key)
                )

            print(f"   - æ‰¹æ¬¡å†…å”¯ä¸€hashåˆ†åŒºæ•°: {len(hash_group_keys)}")
            assert (
                len(hash_group_keys) == 50
            ), f"åº”è¯¥è·å–åˆ°50ä¸ªå”¯ä¸€hashåˆ†åŒºï¼Œå®é™…è·å–åˆ°{len(hash_group_keys)}ä¸ªå”¯ä¸€hashåˆ†åŒº"

        # äºŒè¿›åˆ¶æ•°æ®éªŒè¯ç»“æœ
        print(f"\nğŸ“Š BSONäºŒè¿›åˆ¶æ•°æ®éªŒè¯ç»“æœ:")
        print(f"   - éªŒè¯æ€»æ•°: {binary_verification_total}")
        print(f"   - éªŒè¯æˆåŠŸ: {binary_verification_success}")
        print(
            f"   - æˆåŠŸç‡: {binary_verification_success / binary_verification_total * 100:.1f}%"
        )

        assert (
            binary_verification_success == binary_verification_total
        ), "æ‰€æœ‰äºŒè¿›åˆ¶æ•°æ®éªŒè¯éƒ½åº”è¯¥æˆåŠŸ"

        # æ¸…ç†æµ‹è¯•æ•°æ®
        print("ğŸ§¹ æ¸…ç†BSONæµ‹è¯•æ•°æ®...")
        await test_manager.force_cleanup_and_reset()

        # è·å–æ¸…ç†åçš„å†…å­˜ä½¿ç”¨æƒ…å†µ
        cleanup_info = await redis_client.info("memory")
        cleanup_memory = cleanup_info.get("used_memory", 0)
        cleanup_memory_human = cleanup_info.get("used_memory_human", "0B")

        print(f"   - æ¸…ç†åå†…å­˜: {cleanup_memory_human} ({cleanup_memory:,} bytes)")
        print(f"   - é‡Šæ”¾å†…å­˜: {(final_memory - cleanup_memory) / (1024 * 1024):.2f}MB")

        # æ¸…ç†
        await test_manager.shutdown()
        await manager_factory.stop_all_managers()
        await redis_client.flushdb()
        await redis_client.close()

    except Exception as e:
        print(f"âŒ BSONå¤§æ•°æ®é‡æµ‹è¯•å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        raise


async def run_bson_complex_tests():
    """è¿è¡Œæ‰€æœ‰BSONå¤æ‚æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹è¿è¡ŒRedisåˆ†ç»„é˜Ÿåˆ—BSONåºåˆ—åŒ–é«˜çº§æµ‹è¯•...")

    # æ£€æŸ¥Redisè¿æ¥
    try:
        redis_provider = get_bean_by_type(RedisProvider)
        redis_client = await redis_provider.get_named_client(
            "default", decode_responses=True
        )
        await redis_client.ping()
        print("âœ… Redisè¿æ¥æ­£å¸¸")
        await redis_client.close()
    except Exception as e:
        print(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿RedisæœåŠ¡æ­£åœ¨è¿è¡Œå¹¶ä¸”é…ç½®æ­£ç¡®")
        return

    # å®šä¹‰æµ‹è¯•å‡½æ•°
    tests = [
        test_bson_concurrent_producers_consumers,
        test_bson_large_data_memory_usage,
    ]

    passed = 0
    failed = 0

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    for test_func in tests:
        test_name = test_func.__name__
        print(f"\n{'='*60}")
        print(f"ğŸ§ª è¿è¡ŒBSONæµ‹è¯•: {test_name}")
        print(f"{'='*60}")

        try:
            start_time = time.time()
            await test_func()
            elapsed = time.time() - start_time
            print(f"âœ… BSONæµ‹è¯•é€šè¿‡: {test_name} (è€—æ—¶: {elapsed:.2f}ç§’)")
            passed += 1
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"âŒ BSONæµ‹è¯•å¤±è´¥: {test_name} (è€—æ—¶: {elapsed:.2f}ç§’)")
            print(f"é”™è¯¯: {e}")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            failed += 1
            break  # é‡åˆ°å¤±è´¥å°±åœæ­¢

    # è¾“å‡ºæ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ“Š BSONæµ‹è¯•æ€»ç»“")
    print(f"{'='*60}")
    print(f"âœ… é€šè¿‡: {passed}")
    print(f"âŒ å¤±è´¥: {failed}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {passed / (passed + failed) * 100:.1f}%")

    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰BSONæµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
    else:
        print(f"âš ï¸ æœ‰ {failed} ä¸ªBSONæµ‹è¯•å¤±è´¥")


if __name__ == "__main__":
    asyncio.run(run_bson_complex_tests())

"""æ£€ç´¢å·¥å…·å‡½æ•°

æä¾›å¤šç§æ£€ç´¢ç­–ç•¥çš„å®ç°ï¼š
- Embedding å‘é‡æ£€ç´¢
- BM25 å…³é”®è¯æ£€ç´¢
- RRF èåˆæ£€ç´¢
- Agentic æ£€ç´¢ï¼ˆLLM å¼•å¯¼çš„å¤šè½®æ£€ç´¢ï¼‰
"""

import re
import time
import jieba
import numpy as np
import logging
import asyncio
from typing import List, Tuple, Dict, Any, Optional
from core.nlp.stopwords_utils import filter_stopwords as filter_chinese_stopwords
from .vectorize_service import get_vectorize_service

logger = logging.getLogger(__name__)


def build_bm25_index(candidates):
    """æ„å»º BM25 ç´¢å¼•ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
    try:
        import nltk
        from nltk.corpus import stopwords
        from nltk.stem import PorterStemmer
        from nltk.tokenize import word_tokenize
        from rank_bm25 import BM25Okapi
    except ImportError as e:
        return None, None, None, None
    
    # ç¡®ä¿ NLTK æ•°æ®å·²ä¸‹è½½
    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        nltk.download("punkt", quiet=True)
    
    try:
        nltk.data.find("tokenizers/punkt_tab")
    except LookupError:
        nltk.download("punkt_tab", quiet=True)
    
    try:
        nltk.data.find("corpora/stopwords")
    except LookupError:
        nltk.download("stopwords", quiet=True)
    
    stemmer = PorterStemmer()
    stop_words = set(stopwords.words("english"))
    
    # æå–æ–‡æœ¬å¹¶åˆ†è¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
    tokenized_docs = []
    for mem in candidates:
        text = getattr(mem, "episode", None) or getattr(mem, "summary", "") or ""
        has_chinese = bool(re.search(r'[\u4e00-\u9fff]', text))
        
        if has_chinese:
            tokens = list(jieba.cut(text))
            processed_tokens = filter_chinese_stopwords(tokens)
        else:
            tokens = word_tokenize(text.lower())
            processed_tokens = [
                stemmer.stem(token)
                for token in tokens
                if token.isalpha() and len(token) >= 2 and token not in stop_words
            ]
        
        tokenized_docs.append(processed_tokens)
    
    bm25 = BM25Okapi(tokenized_docs)
    return bm25, tokenized_docs, stemmer, stop_words


async def search_with_bm25(
    query: str,
    bm25,
    candidates,
    stemmer,
    stop_words,
    top_k: int = 50
) -> List[Tuple]:
    """BM25 æ£€ç´¢ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
    if bm25 is None:
        return []
    
    try:
        from nltk.tokenize import word_tokenize
    except ImportError:
        return []
    
    # åˆ†è¯æŸ¥è¯¢ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
    has_chinese = bool(re.search(r'[\u4e00-\u9fff]', query))
    
    if has_chinese:
        tokens = list(jieba.cut(query))
        tokenized_query = filter_chinese_stopwords(tokens)
    else:
        tokens = word_tokenize(query.lower())
        tokenized_query = [
            stemmer.stem(token)
            for token in tokens
            if token.isalpha() and len(token) >= 2 and token not in stop_words
        ]
    
    if not tokenized_query:
        return []
    
    # è®¡ç®— BM25 åˆ†æ•°
    scores = bm25.get_scores(tokenized_query)
    
    # æ’åºå¹¶è¿”å› Top-K
    results = sorted(
        zip(candidates, scores),
        key=lambda x: x[1],
        reverse=True
    )[:top_k]
    
    return results


def reciprocal_rank_fusion(
    results1: List[Tuple],
    results2: List[Tuple],
    k: int = 60
) -> List[Tuple]:
    """RRF èåˆä¸¤ä¸ªæ£€ç´¢ç»“æœ"""
    doc_rrf_scores = {}
    doc_map = {}
    
    # å¤„ç†ç¬¬ä¸€ä¸ªç»“æœé›†
    for rank, (doc, score) in enumerate(results1, start=1):
        doc_id = id(doc)
        if doc_id not in doc_map:
            doc_map[doc_id] = doc
        doc_rrf_scores[doc_id] = doc_rrf_scores.get(doc_id, 0.0) + 1.0 / (k + rank)
    
    # å¤„ç†ç¬¬äºŒä¸ªç»“æœé›†
    for rank, (doc, score) in enumerate(results2, start=1):
        doc_id = id(doc)
        if doc_id not in doc_map:
            doc_map[doc_id] = doc
        doc_rrf_scores[doc_id] = doc_rrf_scores.get(doc_id, 0.0) + 1.0 / (k + rank)
    
    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
    fused_results = [
        (doc_map[doc_id], rrf_score)
        for doc_id, rrf_score in doc_rrf_scores.items()
    ]
    fused_results.sort(key=lambda x: x[1], reverse=True)
    
    return fused_results


async def lightweight_retrieval(
    query: str,
    candidates,
    emb_top_n: int = 50,
    bm25_top_n: int = 50,
    final_top_n: int = 20
) -> Tuple:
    """è½»é‡çº§æ£€ç´¢ï¼ˆEmbedding + BM25 + RRF èåˆï¼‰"""
    start_time = time.time()
    
    metadata = {
        "retrieval_mode": "lightweight",
        "emb_count": 0,
        "bm25_count": 0,
        "final_count": 0,
        "total_latency_ms": 0.0,
    }
    
    if not candidates:
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    
    # æ„å»º BM25 ç´¢å¼•
    bm25, tokenized_docs, stemmer, stop_words = build_bm25_index(candidates)
    
    # Embedding æ£€ç´¢
    emb_results = []
    try:
        vectorize_service = get_vectorize_service()
        query_vec = await vectorize_service.get_embedding(query)
        query_norm = np.linalg.norm(query_vec)
        
        if query_norm > 0:
            scores = []
            for mem in candidates:
                try:
                    doc_vec = np.array(mem.extend.get("embedding", []))
                    if len(doc_vec) > 0:
                        doc_norm = np.linalg.norm(doc_vec)
                        if doc_norm > 0:
                            sim = np.dot(query_vec, doc_vec) / (query_norm * doc_norm)
                            scores.append((mem, float(sim)))
                except:
                    continue
            
            emb_results = sorted(scores, key=lambda x: x[1], reverse=True)[:emb_top_n]
    except Exception as e:
        pass
    
    metadata["emb_count"] = len(emb_results)
    
    # BM25 æ£€ç´¢
    bm25_results = []
    if bm25 is not None:
        bm25_results = await search_with_bm25(
            query, bm25, candidates, stemmer, stop_words, top_k=bm25_top_n
        )
    
    metadata["bm25_count"] = len(bm25_results)
    
    # RRF èåˆ
    if not emb_results and not bm25_results:
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    elif not emb_results:
        final_results = bm25_results[:final_top_n]
    elif not bm25_results:
        final_results = emb_results[:final_top_n]
    else:
        fused_results = reciprocal_rank_fusion(emb_results, bm25_results, k=60)
        final_results = fused_results[:final_top_n]
    
    metadata["final_count"] = len(final_results)
    metadata["total_latency_ms"] = (time.time() - start_time) * 1000
    
    return final_results, metadata


def multi_rrf_fusion(
    results_list: List[List[Tuple]],
    k: int = 60
) -> List[Tuple]:
    """
    ä½¿ç”¨ RRF èåˆå¤šä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æœï¼ˆå¤šæŸ¥è¯¢èåˆï¼‰
    
    ä¸åŒè·¯ RRF ç±»ä¼¼ï¼Œä½†æ”¯æŒèåˆä»»æ„æ•°é‡çš„æ£€ç´¢ç»“æœã€‚
    æ¯ä¸ªç»“æœé›†è´¡çŒ®çš„åˆ†æ•°ï¼š1 / (k + rank)
    
    åŸç†ï¼š
    - åœ¨å¤šä¸ªæŸ¥è¯¢ä¸­éƒ½æ’åé å‰çš„æ–‡æ¡£ â†’ åˆ†æ•°ç´¯ç§¯é«˜ â†’ æœ€ç»ˆæ’åé å‰
    - è¿™æ˜¯ä¸€ç§"æŠ•ç¥¨æœºåˆ¶"ï¼šå¤šä¸ªæŸ¥è¯¢éƒ½è®¤ä¸ºç›¸å…³çš„æ–‡æ¡£æ›´å¯èƒ½çœŸæ­£ç›¸å…³
    
    Args:
        results_list: å¤šä¸ªæ£€ç´¢ç»“æœåˆ—è¡¨ [
            [(doc1, score), (doc2, score), ...],  # Query 1 ç»“æœ
            [(doc3, score), (doc1, score), ...],  # Query 2 ç»“æœ
            [(doc4, score), (doc2, score), ...],  # Query 3 ç»“æœ
        ]
        k: RRF å¸¸æ•°ï¼ˆé»˜è®¤ 60ï¼‰
    
    Returns:
        èåˆåçš„ç»“æœ [(doc, rrf_score), ...]ï¼ŒæŒ‰ RRF åˆ†æ•°é™åºæ’åˆ—
    
    Example:
        Query 1 ç»“æœ: [(doc_A, 0.9), (doc_B, 0.8), (doc_C, 0.7)]
        Query 2 ç»“æœ: [(doc_B, 0.88), (doc_D, 0.82), (doc_A, 0.75)]
        Query 3 ç»“æœ: [(doc_A, 0.92), (doc_E, 0.85), (doc_B, 0.80)]
        
        RRF åˆ†æ•°è®¡ç®—ï¼š
        doc_A: 1/(60+1) + 1/(60+3) + 1/(60+1) = 0.0323  â† åœ¨ Q1,Q2,Q3 éƒ½å‡ºç°
        doc_B: 1/(60+2) + 1/(60+1) + 1/(60+3) = 0.0323  â† åœ¨ Q1,Q2,Q3 éƒ½å‡ºç°
        doc_C: 1/(60+3) + 0        + 0        = 0.0159  â† åªåœ¨ Q1 å‡ºç°
        doc_D: 0        + 1/(60+2) + 0        = 0.0161  â† åªåœ¨ Q2 å‡ºç°
        doc_E: 0        + 0        + 1/(60+2) = 0.0161  â† åªåœ¨ Q3 å‡ºç°
        
        èåˆç»“æœ: doc_A å’Œ doc_B æ’åæœ€é«˜ï¼ˆè¢«å¤šä¸ªæŸ¥è¯¢è®¤å¯ï¼‰
    """
    if not results_list:
        return []
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªç»“æœé›†ï¼Œç›´æ¥è¿”å›
    if len(results_list) == 1:
        return results_list[0]
    
    # ä½¿ç”¨æ–‡æ¡£çš„å†…å­˜åœ°å€ä½œä¸ºå”¯ä¸€æ ‡è¯†
    doc_rrf_scores = {}  # {doc_id: rrf_score}
    doc_map = {}         # {doc_id: doc}
    
    # éå†æ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æœ
    for query_results in results_list:
        for rank, (doc, score) in enumerate(query_results, start=1):
            doc_id = id(doc)
            if doc_id not in doc_map:
                doc_map[doc_id] = doc
            # ç´¯åŠ  RRF åˆ†æ•°
            doc_rrf_scores[doc_id] = doc_rrf_scores.get(doc_id, 0.0) + 1.0 / (k + rank)
    
    # æŒ‰ RRF åˆ†æ•°æ’åº
    sorted_docs = sorted(doc_rrf_scores.items(), key=lambda x: x[1], reverse=True)
    
    # è½¬æ¢å› (doc, score) æ ¼å¼
    fused_results = [(doc_map[doc_id], rrf_score) for doc_id, rrf_score in sorted_docs]
    
    return fused_results


async def multi_query_retrieval(
    queries: List[str],
    candidates,
    emb_top_n: int = 50,
    bm25_top_n: int = 50,
    final_top_n: int = 40,
    rrf_k: int = 60
) -> Tuple[List[Tuple], Dict[str, Any]]:
    """
    å¤šæŸ¥è¯¢å¹¶è¡Œæ£€ç´¢ + RRF èåˆ
    
    å¯¹æ¯ä¸ªæŸ¥è¯¢æ‰§è¡Œæ··åˆæ£€ç´¢ï¼ˆEmbedding + BM25ï¼‰ï¼Œç„¶åç”¨ RRF èåˆæ‰€æœ‰ç»“æœã€‚
    è¿™ç§ç­–ç•¥å¯ä»¥æ•è·ä¸åŒè§’åº¦çš„ç›¸å…³ä¿¡æ¯ï¼Œæå‡å¬å›ç‡ã€‚
    
    æµç¨‹ï¼š
    1. å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢çš„æ··åˆæ£€ç´¢
    2. ä½¿ç”¨å¤šæŸ¥è¯¢ RRF èåˆç»“æœ
    3. è¿”å› Top-N æ–‡æ¡£
    
    Args:
        queries: æŸ¥è¯¢åˆ—è¡¨ï¼ˆ2-3 ä¸ªï¼‰
        candidates: å€™é€‰è®°å¿†åˆ—è¡¨
        emb_top_n: æ¯ä¸ªæŸ¥è¯¢çš„ Embedding å€™é€‰æ•°
        bm25_top_n: æ¯ä¸ªæŸ¥è¯¢çš„ BM25 å€™é€‰æ•°
        final_top_n: èåˆåè¿”å›çš„æ–‡æ¡£æ•°
        rrf_k: RRF å‚æ•°
    
    Returns:
        (results, metadata)
        - results: èåˆåçš„ Top-N ç»“æœ
        - metadata: åŒ…å«æ€§èƒ½æŒ‡æ ‡å’Œç»Ÿè®¡ä¿¡æ¯
    
    Example:
        >>> queries = [
        ...     "ç”¨æˆ·æœ€å–œæ¬¢çš„èœç³»æ˜¯ä»€ä¹ˆï¼Ÿ",
        ...     "ç”¨æˆ·å–œæ¬¢ä»€ä¹ˆå£å‘³ï¼Ÿ",
        ...     "ç”¨æˆ·æœ‰ä»€ä¹ˆé¥®é£Ÿä¹ æƒ¯ï¼Ÿ"
        ... ]
        >>> results, metadata = await multi_query_retrieval(queries, candidates)
        >>> print(len(results))  # 40
        >>> print(metadata["num_queries"])  # 3
    """
    start_time = time.time()
    
    metadata = {
        "retrieval_mode": "multi_query",
        "num_queries": len(queries),
        "per_query_results": [],
        "total_docs_before_fusion": 0,
        "final_count": 0,
        "total_latency_ms": 0.0,
    }
    
    if not queries or not candidates:
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    
    logger.info(f"Executing {len(queries)} queries in parallel...")
    
    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢çš„æ··åˆæ£€ç´¢
    tasks = [
        lightweight_retrieval(q, candidates, emb_top_n, bm25_top_n, final_top_n)
        for q in queries
    ]
    
    multi_query_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # æ”¶é›†æœ‰æ•ˆç»“æœ
    valid_results = []
    for i, result in enumerate(multi_query_results, 1):
        if isinstance(result, Exception):
            logger.error(f"Query {i} failed: {result}")
            continue
        
        results, query_metadata = result
        if results:
            valid_results.append(results)
            metadata["per_query_results"].append({
                "query_index": i,
                "count": len(results),
                "latency_ms": query_metadata.get("total_latency_ms", 0),
            })
            logger.debug(f"Query {i}: Retrieved {len(results)} documents")
    
    if not valid_results:
        logger.warning("All queries failed")
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    
    # ç»Ÿè®¡èåˆå‰çš„æ€»æ–‡æ¡£æ•°
    metadata["total_docs_before_fusion"] = sum(len(r) for r in valid_results)
    
    # ä½¿ç”¨å¤šæŸ¥è¯¢ RRF èåˆ
    logger.info(f"Fusing {len(valid_results)} query results...")
    fused_results = multi_rrf_fusion(valid_results, k=rrf_k)
    
    # æˆªå– Top-N
    final_results = fused_results[:final_top_n]
    
    metadata["final_count"] = len(final_results)
    metadata["total_latency_ms"] = (time.time() - start_time) * 1000
    
    logger.info(f"Multi-query retrieval: {metadata['total_docs_before_fusion']} â†’ {len(final_results)} docs")
    
    return final_results, metadata


async def rerank_candidates(
    query: str,
    candidates: List[Tuple],
    top_n: int,
    rerank_service
) -> List[Tuple]:
    """
    å¯¹å€™é€‰ç»“æœè¿›è¡Œ Rerank
    
    ä½¿ç”¨ Rerank æœåŠ¡å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åºï¼Œæå‡ç²¾åº¦ã€‚
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        candidates: å€™é€‰ç»“æœ [(doc, score), ...]
        top_n: è¿”å›çš„ Top-N æ•°é‡
        rerank_service: Rerank æœåŠ¡å®ä¾‹
    
    Returns:
        é‡æ’åºåçš„ Top-N ç»“æœ [(doc, new_score), ...]
    
    Note:
        - å¦‚æœ Rerank å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ’åº
        - ä½¿ç”¨æ‰¹é‡å¤„ç†é¿å… API é™æµ
    """
    if not candidates:
        return []
    
    try:
        logger.debug(f"Reranking {len(candidates)} candidates for query: {query[:50]}...")
        
        # ğŸ”¥ è½¬æ¢æ ¼å¼ï¼šå°† [(doc, score)] è½¬ä¸º rerank æœåŠ¡æœŸæœ›çš„æ ¼å¼
        # rerank_service._rerank_all_hits æœŸæœ› List[Dict[str, Any]]
        candidates_for_rerank = []
        for idx, (doc, score) in enumerate(candidates):
            # æ„å»º hit å­—å…¸ï¼ŒåŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ç”¨äº rerank
            hit = {
                "index": idx,
                "score": score,
            }
            
            # å¦‚æœ doc æ˜¯ dictï¼Œç›´æ¥åˆå¹¶
            if isinstance(doc, dict):
                hit.update(doc)
            else:
                # å¦‚æœ doc æ˜¯å¯¹è±¡ï¼Œæå–å…³é”®å­—æ®µ
                hit["episode"] = getattr(doc, "episode", "")
                hit["summary"] = getattr(doc, "summary", "")
                hit["subject"] = getattr(doc, "subject", "")
                
                # å°è¯•æå– event_logï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if hasattr(doc, "event_log"):
                    event_log = doc.event_log
                    if isinstance(event_log, dict):
                        hit["event_log"] = event_log
                    elif event_log:
                        # å¦‚æœæ˜¯å¯¹è±¡ï¼Œè½¬ä¸ºå­—å…¸
                        hit["event_log"] = {
                            "atomic_fact": getattr(event_log, "atomic_fact", []),
                            "time": getattr(event_log, "time", ""),
                        }
            
            candidates_for_rerank.append(hit)
        
        # è°ƒç”¨ rerank æœåŠ¡
        reranked_hits = await rerank_service._rerank_all_hits(
            query,
            candidates_for_rerank,
            top_k=top_n
        )
        
        # è½¬æ¢æ ¼å¼ï¼šä» rerank è¿”å›çš„æ ¼å¼è½¬ä¸º (doc, score) æ ¼å¼
        if reranked_hits:
            # reranked_hits æ ¼å¼: [{"index": ..., "relevance_score": ...}, ...]
            # candidates æ ¼å¼: [(doc, score), ...]
            
            reranked_results = []
            for hit in reranked_hits[:top_n]:
                # æå–ç´¢å¼•
                if isinstance(hit, dict):
                    idx = hit.get("index", hit.get("global_index", 0))
                    new_score = hit.get("relevance_score", 0.0)
                else:
                    # å¦‚æœè¿”å›çš„æ˜¯ tupleï¼Œè¯´æ˜æ ¼å¼æœ‰é—®é¢˜ï¼Œè·³è¿‡
                    logger.warning(f"Unexpected rerank result type: {type(hit)}")
                    continue
                
                if 0 <= idx < len(candidates):
                    doc = candidates[idx][0]
                    reranked_results.append((doc, new_score))
            
            logger.debug(f"Rerank complete: {len(reranked_results)} results")
            return reranked_results if reranked_results else candidates[:top_n]
        else:
            logger.warning("Rerank returned empty results, using original")
            return candidates[:top_n]
    
    except Exception as e:
        logger.error(f"Rerank failed: {e}, using original ranking", exc_info=True)
        return candidates[:top_n]


async def agentic_retrieval(
    query: str,
    candidates,
    llm_provider,
    config: Optional[Any] = None,
) -> Tuple[List[Tuple], Dict[str, Any]]:
    """
    Agentic å¤šè½®æ£€ç´¢ï¼ˆLLM å¼•å¯¼ï¼‰
    
    ä½¿ç”¨ LLM åˆ¤æ–­æ£€ç´¢å……åˆ†æ€§ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œå¤šè½®æ£€ç´¢ã€‚
    
    æµç¨‹ï¼š
    1. Round 1: æ··åˆæ£€ç´¢ â†’ Top 20
    2. Rerank â†’ Top 5 â†’ LLM åˆ¤æ–­å……åˆ†æ€§
    3. å¦‚æœå……åˆ†ï¼šè¿”å›åŸå§‹ Top 20
    4. å¦‚æœä¸å……åˆ†ï¼š
       - LLM ç”Ÿæˆå¤šä¸ªæ”¹è¿›æŸ¥è¯¢ï¼ˆ2-3 ä¸ªï¼‰
       - Round 2: å¹¶è¡Œæ£€ç´¢æ‰€æœ‰æŸ¥è¯¢
       - ä½¿ç”¨ RRF èåˆ â†’ å»é‡åˆå¹¶åˆ° 40 ä¸ª
       - Rerank â†’ è¿”å›æœ€ç»ˆ Top 20
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        candidates: å€™é€‰è®°å¿†åˆ—è¡¨
        llm_provider: LLM Provider (Memory Layer)
        config: Agentic é…ç½®ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        (final_results, metadata)
        - final_results: æœ€ç»ˆæ£€ç´¢ç»“æœ [(doc, score), ...]
        - metadata: åŒ…å«è¯¦ç»†çš„æ£€ç´¢è¿‡ç¨‹ä¿¡æ¯
    
    Example:
        >>> from agentic_layer.agentic_utils import AgenticConfig
        >>> config = AgenticConfig(use_reranker=True)
        >>> results, metadata = await agentic_retrieval(
        ...     query="ç”¨æˆ·å–œæ¬¢åƒä»€ä¹ˆï¼Ÿ",
        ...     candidates=memcells,
        ...     llm_provider=llm,
        ...     config=config
        ... )
        >>> print(metadata["is_sufficient"])  # False
        >>> print(metadata["refined_queries"])  # ["ç”¨æˆ·æœ€å–œæ¬¢çš„èœç³»ï¼Ÿ", ...]
    """
    # å¯¼å…¥é…ç½®å’Œå·¥å…·
    from .agentic_utils import (
        AgenticConfig,
        check_sufficiency,
        generate_multi_queries
    )
    from .rerank_service import get_rerank_service
    
    # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–æä¾›çš„é…ç½®
    if config is None:
        config = AgenticConfig()
    
    start_time = time.time()
    
    metadata = {
        "retrieval_mode": "agentic",
        "is_multi_round": False,
        "round1_count": 0,
        "round1_reranked_count": 0,
        "is_sufficient": None,
        "reasoning": None,
        "missing_info": None,
        "refined_queries": None,
        "round2_count": 0,
        "final_count": 0,
        "total_latency_ms": 0.0,
    }
    
    logger.info(f"{'='*60}")
    logger.info(f"Agentic Retrieval: {query[:60]}...")
    logger.info(f"{'='*60}")
    
    # ========== Round 1: æ··åˆæ£€ç´¢ Top 20 ==========
    logger.info("Round 1: Hybrid search for Top 20...")
    
    try:
        round1_results, round1_metadata = await lightweight_retrieval(
            query=query,
            candidates=candidates,
            emb_top_n=config.round1_emb_top_n,
            bm25_top_n=config.round1_bm25_top_n,
            final_top_n=config.round1_top_n
        )
        
        metadata["round1_count"] = len(round1_results)
        metadata["round1_latency_ms"] = round1_metadata.get("total_latency_ms", 0)
        
        logger.info(f"Round 1: Retrieved {len(round1_results)} documents")
        
        if not round1_results:
            logger.warning("Round 1 returned no results")
            metadata["total_latency_ms"] = (time.time() - start_time) * 1000
            return [], metadata
    
    except Exception as e:
        logger.error(f"Round 1 failed: {e}")
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    
    # ========== Rerank Top 20 â†’ Top 5 ç”¨äº Sufficiency Check ==========
    if config.use_reranker:
        logger.info("Reranking Top 20 to get Top 5 for sufficiency check...")
        
        try:
            rerank_service = get_rerank_service()
            reranked_top5 = await rerank_candidates(
                query=query,
                candidates=round1_results,
                top_n=config.round1_rerank_top_n,
                rerank_service=rerank_service
            )
            
            metadata["round1_reranked_count"] = len(reranked_top5)
            logger.info(f"Rerank: Got Top {len(reranked_top5)} for sufficiency check")
        
        except Exception as e:
            logger.error(f"Rerank failed: {e}, using original Top 5")
            reranked_top5 = round1_results[:config.round1_rerank_top_n]
            metadata["round1_reranked_count"] = len(reranked_top5)
    else:
        # ä¸ä½¿ç”¨ rerankerï¼Œç›´æ¥å–å‰ 5 ä¸ª
        reranked_top5 = round1_results[:config.round1_rerank_top_n]
        metadata["round1_reranked_count"] = len(reranked_top5)
        logger.info("No Rerank: Using original Top 5 for sufficiency check")
    
    if not reranked_top5:
        logger.warning("No results for sufficiency check, returning Round 1 results")
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return round1_results, metadata
    
    # ========== LLM Sufficiency Check ==========
    logger.info("LLM: Checking sufficiency on Top 5...")
    
    try:
        is_sufficient, reasoning, missing_info = await check_sufficiency(
            query=query,
            results=reranked_top5,
            llm_provider=llm_provider,
            max_docs=config.round1_rerank_top_n
        )
        
        metadata["is_sufficient"] = is_sufficient
        metadata["reasoning"] = reasoning
        metadata["missing_info"] = missing_info
        
        logger.info(f"LLM Result: {'âœ… Sufficient' if is_sufficient else 'âŒ Insufficient'}")
        logger.info(f"LLM Reasoning: {reasoning}")
        
    except Exception as e:
        logger.error(f"Sufficiency check failed: {e}, assuming sufficient")
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return round1_results, metadata
    
    # ========== å¦‚æœå……åˆ†ï¼šè¿”å›åŸå§‹ Round 1 çš„ Top 20 ==========
    if is_sufficient:
        logger.info("Decision: Sufficient! Using Round 1 Top 20 results")
        
        final_results = round1_results
        metadata["final_count"] = len(final_results)
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        
        logger.info(f"Complete: Latency {metadata['total_latency_ms']:.0f}ms")
        return final_results, metadata
    
    # ========== å¦‚æœä¸å……åˆ†ï¼šè¿›å…¥ Round 2 ==========
    metadata["is_multi_round"] = True
    logger.info("Decision: Insufficient, entering Round 2")
    if missing_info:
        logger.info(f"Missing: {', '.join(missing_info)}")
    
    # ========== LLM ç”Ÿæˆå¤šä¸ªæ”¹è¿›æŸ¥è¯¢ ==========
    if config.enable_multi_query:
        logger.info("LLM: Generating multiple refined queries...")
        
        try:
            refined_queries, query_strategy = await generate_multi_queries(
                original_query=query,
                results=reranked_top5,
                missing_info=missing_info,
                llm_provider=llm_provider,
                max_docs=config.round1_rerank_top_n,
                num_queries=config.num_queries
            )
            
            metadata["refined_queries"] = refined_queries
            metadata["query_strategy"] = query_strategy
            metadata["num_queries"] = len(refined_queries)
            
            logger.info(f"Generated {len(refined_queries)} queries")
            for i, q in enumerate(refined_queries, 1):
                logger.debug(f"  Query {i}: {q[:80]}...")
        
        except Exception as e:
            logger.error(f"Query generation failed: {e}, using original query")
            refined_queries = [query]
            metadata["refined_queries"] = refined_queries
            metadata["num_queries"] = 1
    else:
        # å•æŸ¥è¯¢æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        refined_queries = [query]
        metadata["refined_queries"] = refined_queries
        metadata["num_queries"] = 1
    
    # ========== Round 2: å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŸ¥è¯¢æ£€ç´¢ ==========
    logger.info(f"Round 2: Executing {len(refined_queries)} queries in parallel...")
    
    try:
        round2_results, round2_metadata = await multi_query_retrieval(
            queries=refined_queries,
            candidates=candidates,
            emb_top_n=config.round1_emb_top_n,
            bm25_top_n=config.round1_bm25_top_n,
            final_top_n=config.round2_per_query_top_n,
            rrf_k=60
        )
        
        metadata["round2_count"] = len(round2_results)
        metadata["round2_latency_ms"] = round2_metadata.get("total_latency_ms", 0)
        metadata["multi_query_total_docs"] = round2_metadata.get("total_docs_before_fusion", 0)
        
        logger.info(f"Round 2: Retrieved {len(round2_results)} unique documents")
    
    except Exception as e:
        logger.error(f"Round 2 failed: {e}, using Round 1 results")
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return round1_results, metadata
    
    # ========== åˆå¹¶ï¼šç¡®ä¿æ€»å…± 40 ä¸ªæ–‡æ¡£ ==========
    logger.info("Merge: Combining Round 1 and Round 2...")
    
    # å»é‡ï¼šä½¿ç”¨æ–‡æ¡£ ID å»é‡
    round1_ids = {id(doc) for doc, _ in round1_results}
    round2_unique = [(doc, score) for doc, score in round2_results if id(doc) not in round1_ids]
    
    # åˆå¹¶ï¼šRound1 Top20 + Round2 å»é‡åçš„æ–‡æ¡£ï¼ˆç¡®ä¿æ€»æ•°<=40ï¼‰
    combined_results = round1_results.copy()
    needed_from_round2 = config.combined_total - len(combined_results)
    combined_results.extend(round2_unique[:needed_from_round2])
    
    logger.info(f"Merge: Round1={len(round1_results)}, Round2_unique={len(round2_unique[:needed_from_round2])}, Total={len(combined_results)}")
    
    # ========== Rerank åˆå¹¶åçš„æ–‡æ¡£ ==========
    if config.use_reranker and len(combined_results) > 0:
        logger.info(f"Rerank: Reranking {len(combined_results)} documents...")
        
        try:
            rerank_service = get_rerank_service()
            final_results = await rerank_candidates(
                query=query,  # ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¿›è¡Œ rerank
                candidates=combined_results,
                top_n=config.final_top_n,
                rerank_service=rerank_service
            )
            
            logger.info(f"Rerank: Final Top {len(final_results)} selected")
        
        except Exception as e:
            logger.error(f"Final rerank failed: {e}, using top {config.final_top_n}")
            final_results = combined_results[:config.final_top_n]
    else:
        # ä¸ä½¿ç”¨ Rerankerï¼Œç›´æ¥è¿”å› Top N
        final_results = combined_results[:config.final_top_n]
        logger.info(f"No Rerank: Returning Top {len(final_results)}")
    
    metadata["final_count"] = len(final_results)
    metadata["total_latency_ms"] = (time.time() - start_time) * 1000
    
    logger.info(f"Complete: Final {len(final_results)} docs | Latency {metadata['total_latency_ms']:.0f}ms")
    logger.info(f"{'='*60}\n")
    
    return final_results, metadata


from dataclasses import dataclass
import random
import time
import json
import traceback
from api_specs.dtos.memory_command import MemorizeRequest, MemorizeOfflineRequest
from memory_layer.memory_manager import MemoryManager
from api_specs.memory_types import (
    MemoryType,
    MemCell,
    Memory,
    RawDataType,
    SemanticMemoryItem,
)
from memory_layer.memory_extractor.event_log_extractor import EventLog
from memory_layer.memory_extractor.profile_memory_extractor import ProfileMemory
from core.di import get_bean_by_type
from component.redis_provider import RedisProvider
from infra_layer.adapters.out.persistence.repository.episodic_memory_raw_repository import (
    EpisodicMemoryRawRepository,
)
from infra_layer.adapters.out.persistence.repository.semantic_memory_record_raw_repository import (
    SemanticMemoryRecordRawRepository,
)
from infra_layer.adapters.out.persistence.repository.event_log_record_raw_repository import (
    EventLogRecordRawRepository,
)
from infra_layer.adapters.out.persistence.repository.conversation_status_raw_repository import (
    ConversationStatusRawRepository,
)
from infra_layer.adapters.out.persistence.repository.conversation_meta_raw_repository import (
    ConversationMetaRawRepository,
)
from infra_layer.adapters.out.persistence.repository.memcell_raw_repository import (
    MemCellRawRepository,
)
from infra_layer.adapters.out.persistence.repository.core_memory_raw_repository import (
    CoreMemoryRawRepository,
)
from infra_layer.adapters.out.persistence.repository.group_user_profile_memory_raw_repository import (
    GroupUserProfileMemoryRawRepository,
)
from infra_layer.adapters.out.persistence.repository.group_profile_raw_repository import (
    GroupProfileRawRepository,
)
from biz_layer.conversation_data_repo import ConversationDataRepository
from api_specs.memory_types import RawDataType
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
import uuid
from datetime import datetime, timedelta
import os
import asyncio
from collections import defaultdict
from common_utils.datetime_utils import (
    get_now_with_timezone,
    to_iso_format,
    from_iso_format,
)
from memory_layer.memcell_extractor.base_memcell_extractor import StatusResult
import traceback

from core.lock.redis_distributed_lock import distributed_lock
from core.observation.logger import get_logger
from infra_layer.adapters.out.search.elasticsearch.converter.episodic_memory_converter import (
    EpisodicMemoryConverter,
)
from infra_layer.adapters.out.search.milvus.converter.episodic_memory_milvus_converter import (
    EpisodicMemoryMilvusConverter,
)
from infra_layer.adapters.out.search.elasticsearch.converter.semantic_memory_converter import (
    SemanticMemoryConverter,
)
from infra_layer.adapters.out.search.milvus.converter.semantic_memory_milvus_converter import (
    SemanticMemoryMilvusConverter,
)
from infra_layer.adapters.out.search.elasticsearch.converter.event_log_converter import (
    EventLogConverter,
)
from infra_layer.adapters.out.search.milvus.converter.event_log_milvus_converter import (
    EventLogMilvusConverter,
)
from infra_layer.adapters.out.search.repository.episodic_memory_milvus_repository import (
    EpisodicMemoryMilvusRepository,
)
from infra_layer.adapters.out.search.repository.episodic_memory_es_repository import (
    EpisodicMemoryEsRepository,
)
from infra_layer.adapters.out.search.repository.semantic_memory_milvus_repository import (
    SemanticMemoryMilvusRepository,
)
from infra_layer.adapters.out.search.repository.event_log_milvus_repository import (
    EventLogMilvusRepository,
)
from biz_layer.mem_sync import MemorySyncService

logger = get_logger(__name__)


@dataclass
class MemoryDocPayload:
    memory_type: MemoryType
    doc: Any


def _clone_semantic_memory_item(raw_item: Any) -> Optional[SemanticMemoryItem]:
    """å°†ä»»æ„ç»“æ„çš„è¯­ä¹‰è®°å¿†æ¡ç›®è½¬æ¢ä¸º SemanticMemoryItem å®ä¾‹"""
    if raw_item is None:
        return None

    if isinstance(raw_item, SemanticMemoryItem):
        return SemanticMemoryItem(
            content=raw_item.content,
            evidence=getattr(raw_item, "evidence", None),
            start_time=getattr(raw_item, "start_time", None),
            end_time=getattr(raw_item, "end_time", None),
            duration_days=getattr(raw_item, "duration_days", None),
            source_episode_id=getattr(raw_item, "source_episode_id", None),
            embedding=getattr(raw_item, "embedding", None),
        )

    if isinstance(raw_item, dict):
        return SemanticMemoryItem(
            content=raw_item.get("content", ""),
            evidence=raw_item.get("evidence"),
            start_time=raw_item.get("start_time"),
            end_time=raw_item.get("end_time"),
            duration_days=raw_item.get("duration_days"),
            source_episode_id=raw_item.get("source_episode_id"),
            embedding=raw_item.get("embedding"),
        )

    return None


def _clone_event_log(raw_event_log: Any) -> Optional[EventLog]:
    """å°†ä»»æ„ç»“æ„çš„äº‹ä»¶æ—¥å¿—è½¬æ¢ä¸º EventLog å®ä¾‹"""
    if raw_event_log is None:
        return None

    if isinstance(raw_event_log, EventLog):
        return EventLog(
            time=getattr(raw_event_log, "time", ""),
            atomic_fact=list(getattr(raw_event_log, "atomic_fact", []) or []),
            fact_embeddings=getattr(raw_event_log, "fact_embeddings", None),
        )

    if isinstance(raw_event_log, dict):
        return EventLog.from_dict(raw_event_log)

    return None


async def _trigger_clustering(
    group_id: str, memcell: MemCell, scene: Optional[str] = None
) -> None:
    """å¼‚æ­¥è§¦å‘ MemCell èšç±»ï¼ˆåå°ä»»åŠ¡ï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰

    Args:
        group_id: ç¾¤ç»„ID
        memcell: åˆšä¿å­˜çš„ MemCell
        scene: å¯¹è¯åœºæ™¯ï¼ˆç”¨äºå†³å®š Profile æå–ç­–ç•¥ï¼‰
            - None/"work"/"company" ç­‰ï¼šä½¿ç”¨ group_chat åœºæ™¯
            - "assistant"/"companion" ç­‰ï¼šä½¿ç”¨ assistant åœºæ™¯
    """
    logger.info(
        f"[èšç±»] å¼€å§‹è§¦å‘èšç±»: group_id={group_id}, event_id={memcell.event_id}, scene={scene}"
    )

    try:
        from memory_layer.cluster_manager import (
            ClusterManager,
            ClusterManagerConfig,
            MongoClusterStorage,
        )
        from memory_layer.profile_manager import (
            ProfileManager,
            ProfileManagerConfig,
            MongoProfileStorage,
        )
        from memory_layer.llm.llm_provider import LLMProvider
        from core.di import get_bean_by_type
        import os

        logger.info(f"[èšç±»] æ­£åœ¨è·å– MongoClusterStorage...")
        # è·å– MongoDB å­˜å‚¨
        mongo_storage = get_bean_by_type(MongoClusterStorage)
        logger.info(f"[èšç±»] MongoClusterStorage è·å–æˆåŠŸ: {type(mongo_storage)}")

        # åˆ›å»º ClusterManagerï¼ˆä½¿ç”¨ MongoDB å­˜å‚¨ï¼‰
        config = ClusterManagerConfig(
            similarity_threshold=0.65,
            max_time_gap_days=7,
            enable_persistence=False,  # MongoDB ä¸éœ€è¦æ–‡ä»¶æŒä¹…åŒ–
        )
        cluster_manager = ClusterManager(config=config, storage=mongo_storage)
        logger.info(f"[èšç±»] ClusterManager åˆ›å»ºæˆåŠŸ")

        # åˆ›å»º ProfileManager å¹¶è¿æ¥åˆ° ClusterManager
        # è·å– MongoDB Profile å­˜å‚¨
        profile_storage = get_bean_by_type(MongoProfileStorage)
        logger.info(f"[èšç±»] MongoProfileStorage è·å–æˆåŠŸ: {type(profile_storage)}")

        llm_provider = LLMProvider(
            provider_type=os.getenv("LLM_PROVIDER", "openai"),
            model=os.getenv("LLM_MODEL", "gpt-4"),
            base_url=os.getenv("LLM_BASE_URL"),
            api_key=os.getenv("LLM_API_KEY"),
            temperature=float(os.getenv("LLM_TEMPERATURE", "0.3")),
            max_tokens=int(os.getenv("LLM_MAX_TOKENS", "16384")),
        )

        # æ ¹æ® scene å†³å®š Profile æå–åœºæ™¯
        # assistant/companion -> assistant åœºæ™¯ï¼ˆæå–å…´è¶£ã€åå¥½ã€ç”Ÿæ´»ä¹ æƒ¯ï¼‰
        # å…¶ä»– -> group_chat åœºæ™¯ï¼ˆæå–å·¥ä½œè§’è‰²ã€æŠ€èƒ½ã€é¡¹ç›®ç»éªŒï¼‰
        profile_scenario = (
            "assistant"
            if scene and scene.lower() in ["assistant", "companion"]
            else "group_chat"
        )

        profile_config = ProfileManagerConfig(
            scenario=profile_scenario,
            min_confidence=0.6,
            enable_versioning=True,
            auto_extract=True,
        )

        profile_manager = ProfileManager(
            llm_provider=llm_provider,
            config=profile_config,
            storage=profile_storage,  # ä½¿ç”¨ MongoDB å­˜å‚¨
            group_id=group_id,
            group_name=None,  # å¯ä»¥ä» memcell ä¸­è·å–
        )

        # è¿æ¥ ProfileManager åˆ° ClusterManager
        profile_manager.attach_to_cluster_manager(cluster_manager)
        logger.info(
            f"[èšç±»] ProfileManager å·²è¿æ¥åˆ° ClusterManager (åœºæ™¯: {profile_scenario}, ä½¿ç”¨ MongoDB å­˜å‚¨)"
        )
        print(
            f"[èšç±»] ProfileManager å·²è¿æ¥ï¼Œé˜ˆå€¼: {profile_manager._min_memcells_threshold}"
        )

        # å°† MemCell è½¬æ¢ä¸ºèšç±»æ‰€éœ€çš„å­—å…¸æ ¼å¼
        memcell_dict = {
            "event_id": str(memcell.event_id),
            "episode": memcell.episode,
            "timestamp": memcell.timestamp.timestamp() if memcell.timestamp else None,
            "participants": memcell.participants or [],
            "group_id": group_id,
        }

        logger.info(f"[èšç±»] å¼€å§‹æ‰§è¡Œèšç±»: {memcell_dict['event_id']}")
        print(f"[èšç±»] å¼€å§‹æ‰§è¡Œèšç±»: event_id={memcell_dict['event_id']}")

        # æ‰§è¡Œèšç±»ï¼ˆä¼šè‡ªåŠ¨è§¦å‘ ProfileManager çš„å›è°ƒï¼‰
        cluster_id = await cluster_manager.cluster_memcell(
            group_id=group_id, memcell=memcell_dict
        )

        print(f"[èšç±»] èšç±»å®Œæˆ: cluster_id={cluster_id}")

        if cluster_id:
            logger.info(
                f"[èšç±»] âœ… MemCell {memcell.event_id} -> Cluster {cluster_id} (group: {group_id})"
            )
            print(f"[èšç±»] âœ… MemCell {memcell.event_id} -> Cluster {cluster_id}")
        else:
            logger.warning(
                f"[èšç±»] âš ï¸ MemCell {memcell.event_id} èšç±»è¿”å› None (group: {group_id})"
            )
            print(f"[èšç±»] âš ï¸ èšç±»è¿”å› None")

    except Exception as e:
        # èšç±»å¤±è´¥ï¼Œæ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯å¹¶é‡æ–°æŠ›å‡º
        import traceback

        error_msg = f"[èšç±»] âŒ è§¦å‘èšç±»å¤±è´¥: {e}"
        logger.error(error_msg, exc_info=True)
        print(error_msg)  # ç¡®ä¿åœ¨æ§åˆ¶å°èƒ½çœ‹åˆ°
        print(traceback.format_exc())
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…çŸ¥é“å¤±è´¥äº†


def _convert_data_type_to_raw_data_type(data_type) -> RawDataType:
    """
    å°†ä¸åŒçš„æ•°æ®ç±»å‹æšä¸¾è½¬æ¢ä¸ºç»Ÿä¸€çš„RawDataType

    Args:
        data_type: å¯èƒ½æ˜¯DataTypeEnumã€RawDataTypeæˆ–å­—ç¬¦ä¸²

    Returns:
        RawDataType: è½¬æ¢åçš„ç»Ÿä¸€æ•°æ®ç±»å‹
    """
    if isinstance(data_type, RawDataType):
        return data_type

    # è·å–å­—ç¬¦ä¸²å€¼
    if hasattr(data_type, 'value'):
        type_str = data_type.value
    else:
        type_str = str(data_type)

    # æ˜ å°„è½¬æ¢
    type_mapping = {
        "Conversation": RawDataType.CONVERSATION,
        "CONVERSATION": RawDataType.CONVERSATION,
        # å…¶ä»–ç±»å‹æ˜ å°„åˆ°CONVERSATIONä½œä¸ºé»˜è®¤å€¼
    }

    return type_mapping.get(type_str, RawDataType.CONVERSATION)


from biz_layer.mem_db_operations import (
    _convert_timestamp_to_time,
    _convert_episode_memory_to_doc,
    _convert_semantic_memory_to_doc,
    _convert_event_log_to_docs,
    _save_memcell_to_database,
    _save_profile_memory_to_core,
    ConversationStatus,
    _update_status_for_new_conversation,
    _update_status_for_continuing_conversation,
    _update_status_after_memcell_extraction,
    _convert_original_data_for_profile_extractor,
    _save_group_profile_memory,
    _save_profile_memory_to_group_user_profile_memory,
    _convert_document_to_group_importance_evidence,
    _normalize_datetime_for_storage,
    _convert_projects_participated_list,
    _convert_group_profile_raw_to_memory_format,
)


def if_memorize(memcell: MemCell) -> bool:
    return True


def extract_message_time(raw_data):
    """
    ä»RawDataå¯¹è±¡ä¸­æå–æ¶ˆæ¯æ—¶é—´

    Args:
        raw_data: RawDataå¯¹è±¡

    Returns:
        datetime: æ¶ˆæ¯æ—¶é—´ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
    """
    # ä¼˜å…ˆä»timestampå­—æ®µè·å–
    if hasattr(raw_data, 'timestamp') and raw_data.timestamp:
        try:
            return _normalize_datetime_for_storage(raw_data.timestamp)
        except Exception as e:
            logger.debug(f"Failed to parse timestamp from raw_data.timestamp: {e}")
            pass

    # ä»extendå­—æ®µè·å–
    if (
        hasattr(raw_data, 'extend')
        and raw_data.extend
        and isinstance(raw_data.extend, dict)
    ):
        timestamp_val = raw_data.extend.get('timestamp')
        if timestamp_val:
            try:
                return _normalize_datetime_for_storage(timestamp_val)
            except Exception as e:
                logger.debug(f"Failed to parse timestamp from extend field: {e}")
                pass

    return None


from core.observation.tracing.decorators import trace_logger


@trace_logger(operation_name="mem_memorize preprocess_conv_request", log_level="info")
async def preprocess_conv_request(
    request: MemorizeRequest, current_time: datetime
) -> MemorizeRequest:
    """
    ç®€åŒ–ç‰ˆçš„è¯·æ±‚é¢„å¤„ç†ï¼š
    1. ä» Redis è¯»å–æ‰€æœ‰å†å²æ¶ˆæ¯
    2. å°†å†å²æ¶ˆæ¯ä½œä¸º history_raw_data_list
    3. å°†å½“å‰æ–°æ¶ˆæ¯ä½œä¸º new_raw_data_list
    4. è¾¹ç•Œæ£€æµ‹ç”±åç»­é€»è¾‘å¤„ç†ï¼ˆæ£€æµ‹åä¼šæ¸…ç©ºæˆ–ä¿ç•™ Redisï¼‰
    """

    logger.info(f"[preprocess] å¼€å§‹å¤„ç†: group_id={request.group_id}")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®
    if not request.new_raw_data_list:
        logger.info("[preprocess] æ²¡æœ‰æ–°æ•°æ®ï¼Œè·³è¿‡å¤„ç†")
        return None

    # ä½¿ç”¨ conversation_data_repo è¿›è¡Œå…ˆå–åå­˜æ“ä½œ
    conversation_data_repo = get_bean_by_type(ConversationDataRepository)

    try:
        # ç¬¬ä¸€æ­¥ï¼šå…ˆä» conversation_data_repo è·å–å†å²æ¶ˆæ¯
        # è¿™é‡Œä¸é™åˆ¶æ—¶é—´èŒƒå›´ï¼Œè·å–æœ€è¿‘1000æ¡å†å²æ¶ˆæ¯ï¼ˆç”±ç¼“å­˜ç®¡ç†å™¨çš„max_lengthæ§åˆ¶ï¼‰
        history_raw_data_list = await conversation_data_repo.get_conversation_data(
            group_id=request.group_id, start_time=None, end_time=None, limit=1000
        )

        logger.info(
            f"[preprocess] ä» conversation_data_repo è¯»å– {len(history_raw_data_list)} æ¡å†å²æ¶ˆæ¯"
        )

        # æ›´æ–° request
        request.history_raw_data_list = history_raw_data_list
        # new_raw_data_list ä¿æŒä¸å˜ï¼ˆå°±æ˜¯æ–°ä¼ å…¥çš„æ¶ˆæ¯ï¼‰

        logger.info(
            f"[preprocess] å®Œæˆ: å†å² {len(history_raw_data_list)} æ¡, æ–°æ¶ˆæ¯ {len(request.new_raw_data_list)} æ¡"
        )

        return request

    except Exception as e:
        logger.error(f"[preprocess] Redis è¯»å–å¤±è´¥: {e}")
        traceback.print_exc()
        # Redis å¤±è´¥æ—¶ï¼Œä½¿ç”¨åŸå§‹ request
        return request


async def update_status_when_no_memcell(
    request: MemorizeRequest,
    status_result: StatusResult,
    current_time: datetime,
    data_type: RawDataType,
):
    if data_type == RawDataType.CONVERSATION:
        # å°è¯•æ›´æ–°çŠ¶æ€è¡¨
        try:
            status_repo = get_bean_by_type(ConversationStatusRawRepository)

            if status_result.should_wait:
                logger.info(f"[mem_memorize] åˆ¤æ–­ä¸ºæ— æ³•åˆ¤æ–­è¾¹ç•Œç»§ç»­ç­‰å¾…ï¼Œä¸æ›´æ–°çŠ¶æ€è¡¨")
                return
            else:
                logger.info(f"[mem_memorize] åˆ¤æ–­ä¸ºéè¾¹ç•Œï¼Œç»§ç»­ç´¯ç§¯msgï¼Œæ›´æ–°çŠ¶æ€è¡¨")
                # è·å–æœ€æ–°æ¶ˆæ¯æ—¶é—´æˆ³
                latest_time = _convert_timestamp_to_time(current_time, current_time)
                if request.new_raw_data_list:
                    last_msg = request.new_raw_data_list[-1]
                    if hasattr(last_msg, 'content') and isinstance(
                        last_msg.content, dict
                    ):
                        latest_time = last_msg.content.get('timestamp', latest_time)
                    elif hasattr(last_msg, 'timestamp'):
                        latest_time = last_msg.timestamp

                if not latest_time:
                    latest_time = min(latest_time, current_time)

                # ä½¿ç”¨å°è£…å‡½æ•°æ›´æ–°å¯¹è¯å»¶ç»­çŠ¶æ€
                await _update_status_for_continuing_conversation(
                    status_repo, request, latest_time, current_time
                )

        except Exception as e:
            logger.error(f"æ›´æ–°çŠ¶æ€è¡¨å¤±è´¥: {e}")
    else:
        pass


async def update_status_after_memcell(
    request: MemorizeRequest,
    memcell: MemCell,
    current_time: datetime,
    data_type: RawDataType,
):
    if data_type == RawDataType.CONVERSATION:
        # æ›´æ–°çŠ¶æ€è¡¨ä¸­çš„last_memcell_timeè‡³memcellçš„æ—¶é—´æˆ³
        try:
            status_repo = get_bean_by_type(ConversationStatusRawRepository)

            # è·å–MemCellçš„æ—¶é—´æˆ³
            memcell_time = None
            if memcell and hasattr(memcell, 'timestamp'):
                memcell_time = memcell.timestamp
            else:
                memcell_time = current_time

            # ä½¿ç”¨å°è£…å‡½æ•°æ›´æ–°MemCellæå–åçš„çŠ¶æ€
            await _update_status_after_memcell_extraction(
                status_repo, request, memcell_time, current_time
            )

            logger.info(f"[mem_memorize] è®°å¿†æå–å®Œæˆï¼ŒçŠ¶æ€è¡¨å·²æ›´æ–°")

        except Exception as e:
            logger.error(f"æœ€ç»ˆçŠ¶æ€è¡¨æ›´æ–°å¤±è´¥: {e}")
    else:
        pass


async def save_personal_profile_memory(
    profile_memories: List[ProfileMemory], version: Optional[str] = None
):
    logger.info(f"[mem_memorize] ä¿å­˜ {len(profile_memories)} ä¸ªä¸ªäººæ¡£æ¡ˆè®°å¿†åˆ°æ•°æ®åº“")
    # åˆå§‹åŒ–Repositoryå®ä¾‹
    core_memory_repo = get_bean_by_type(CoreMemoryRawRepository)

    # ä¿å­˜ä¸ªäººæ¡£æ¡ˆè®°å¿†åˆ°GroupUserProfileMemoryRawRepository
    for profile_mem in profile_memories:
        await _save_profile_memory_to_core(profile_mem, core_memory_repo, version)
        # ç§»é™¤å•ä¸ªæ“ä½œæˆåŠŸæ—¥å¿—


async def save_memory_docs(
    doc_payloads: List[MemoryDocPayload], version: Optional[str] = None
) -> Dict[MemoryType, List[Any]]:
    """
    é€šç”¨ Doc ä¿å­˜å‡½æ•°ï¼ŒæŒ‰ MemoryType æšä¸¾è‡ªåŠ¨ä¿å­˜å¹¶åŒæ­¥
    """

    grouped_docs: Dict[MemoryType, List[Any]] = defaultdict(list)
    for payload in doc_payloads:
        if payload and payload.doc:
            grouped_docs[payload.memory_type].append(payload.doc)

    saved_result: Dict[MemoryType, List[Any]] = {}

    # Episodic
    episodic_docs = grouped_docs.get(MemoryType.EPISODIC_MEMORY, [])
    if episodic_docs:
        episodic_repo = get_bean_by_type(EpisodicMemoryRawRepository)
        episodic_milvus_repo = get_bean_by_type(EpisodicMemoryMilvusRepository)
        saved_episodic: List[Any] = []

        for doc in episodic_docs:
            saved_doc = await episodic_repo.append_episodic_memory(doc)
            saved_episodic.append(saved_doc)

            es_doc = EpisodicMemoryConverter.from_mongo(saved_doc)
            await es_doc.save()

            milvus_entity = EpisodicMemoryMilvusConverter.from_mongo(saved_doc)
            vector = (
                milvus_entity.get("vector") if isinstance(milvus_entity, dict) else None
            )
            if vector and len(vector) > 0:
                await episodic_milvus_repo.insert(milvus_entity, flush=False)
            else:
                logger.warning(
                    "[mem_memorize] è·³è¿‡å†™å…¥Milvusï¼šå‘é‡ä¸ºç©ºæˆ–ç¼ºå¤±ï¼Œevent_id=%s",
                    getattr(saved_doc, "event_id", None),
                )

        saved_result[MemoryType.EPISODIC_MEMORY] = saved_episodic

    # Semantic
    semantic_docs = grouped_docs.get(MemoryType.SEMANTIC_MEMORY, [])
    if semantic_docs:
        semantic_repo = get_bean_by_type(SemanticMemoryRecordRawRepository)
        saved_semantic = await semantic_repo.create_batch(semantic_docs)
        saved_result[MemoryType.SEMANTIC_MEMORY] = saved_semantic

        sync_service = get_bean_by_type(MemorySyncService)
        await sync_service.sync_batch_semantic_memories(
            saved_semantic, sync_to_es=True, sync_to_milvus=True
        )

    # Event Log
    event_log_docs = grouped_docs.get(MemoryType.PERSONAL_EVENT_LOG, [])
    if event_log_docs:
        event_log_repo = get_bean_by_type(EventLogRecordRawRepository)
        saved_event_logs = await event_log_repo.create_batch(event_log_docs)
        saved_result[MemoryType.PERSONAL_EVENT_LOG] = saved_event_logs

        sync_service = get_bean_by_type(MemorySyncService)
        await sync_service.sync_batch_event_logs(
            saved_event_logs, sync_to_es=True, sync_to_milvus=True
        )

    # Profile
    profile_docs = grouped_docs.get(MemoryType.PROFILE, [])
    if profile_docs:
        group_user_profile_repo = get_bean_by_type(GroupUserProfileMemoryRawRepository)
        saved_profiles = []
        for profile_mem in profile_docs:
            try:
                await _save_profile_memory_to_group_user_profile_memory(
                    profile_mem, group_user_profile_repo, version
                )
                saved_profiles.append(profile_mem)
            except Exception as exc:
                logger.error(f"ä¿å­˜Profileè®°å¿†å¤±è´¥: {exc}")
        if saved_profiles:
            saved_result[MemoryType.PROFILE] = saved_profiles

    group_profile_docs = grouped_docs.get(MemoryType.GROUP_PROFILE, [])
    if group_profile_docs:
        group_profile_repo = get_bean_by_type(GroupProfileRawRepository)
        saved_group_profiles = []
        for mem in group_profile_docs:
            try:
                await _save_group_profile_memory(mem, group_profile_repo, version)
                saved_group_profiles.append(mem)
            except Exception as exc:
                logger.error(f"ä¿å­˜Group Profileè®°å¿†å¤±è´¥: {exc}")
        if saved_group_profiles:
            saved_result[MemoryType.GROUP_PROFILE] = saved_group_profiles

    return saved_result


async def load_core_memories(
    request: MemorizeRequest, participants: List[str], current_time: datetime
):
    logger.info(f"[mem_memorize] è¯»å–ç”¨æˆ·æ•°æ®: {participants}")
    # åˆå§‹åŒ–Repositoryå®ä¾‹
    core_memory_repo = get_bean_by_type(CoreMemoryRawRepository)

    # è¯»å–ç”¨æˆ·CoreMemoryæ•°æ®
    user_core_memories = {}
    for user_id in participants:
        try:
            core_memory = await core_memory_repo.get_by_user_id(user_id)
            if core_memory:
                user_core_memories[user_id] = core_memory
            # ç§»é™¤å•ä¸ªç”¨æˆ·çš„æˆåŠŸ/å¤±è´¥æ—¥å¿—
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ· {user_id} CoreMemoryå¤±è´¥: {e}")

    logger.info(f"[mem_memorize] è·å–åˆ° {len(user_core_memories)} ä¸ªç”¨æˆ·CoreMemory")

    # ç›´æ¥ä»CoreMemoryè½¬æ¢ä¸ºProfileMemoryå¯¹è±¡åˆ—è¡¨
    old_memory_list = []
    if user_core_memories:
        for user_id, core_memory in user_core_memories.items():
            if core_memory:
                # ç›´æ¥åˆ›å»ºProfileMemoryå¯¹è±¡
                profile_memory = ProfileMemory(
                    # Memory åŸºç±»å¿…éœ€å­—æ®µ
                    memory_type=MemoryType.CORE,
                    user_id=user_id,
                    timestamp=to_iso_format(current_time),
                    ori_event_id_list=[],
                    # Memory åŸºç±»å¯é€‰å­—æ®µ
                    subject=f"{getattr(core_memory, 'user_name', user_id)}çš„ä¸ªäººæ¡£æ¡ˆ",
                    summary=f"ç”¨æˆ·{user_id}çš„åŸºæœ¬ä¿¡æ¯ï¼š{getattr(core_memory, 'position', 'æœªçŸ¥è§’è‰²')}",
                    group_id=request.group_id,
                    participants=[user_id],
                    type=RawDataType.CONVERSATION,
                    # ProfileMemory ç‰¹æœ‰å­—æ®µ - ç›´æ¥ä½¿ç”¨åŸå§‹å­—å…¸æ ¼å¼
                    hard_skills=getattr(core_memory, 'hard_skills', None),
                    soft_skills=getattr(core_memory, 'soft_skills', None),
                    output_reasoning=getattr(core_memory, 'output_reasoning', None),
                    motivation_system=getattr(core_memory, 'motivation_system', None),
                    fear_system=getattr(core_memory, 'fear_system', None),
                    value_system=getattr(core_memory, 'value_system', None),
                    humor_use=getattr(core_memory, 'humor_use', None),
                    colloquialism=getattr(core_memory, 'colloquialism', None),
                    projects_participated=_convert_projects_participated_list(
                        getattr(core_memory, 'projects_participated', None)
                    ),
                )
                old_memory_list.append(profile_memory)

        logger.info(
            f"[mem_memorize] ç›´æ¥è½¬æ¢äº† {len(old_memory_list)} ä¸ªCoreMemoryä¸ºProfileMemory"
        )
    else:
        logger.info(f"[mem_memorize] æ²¡æœ‰ç”¨æˆ·CoreMemoryæ•°æ®ï¼Œold_memory_listä¸ºç©º")


async def memorize(request: MemorizeRequest) -> List[Memory]:
    """
    è®°å¿†æå–ä¸»æµç¨‹ (å…¨å±€é˜Ÿåˆ—ç‰ˆ)
    
    æµç¨‹:
    1. æå– MemCell
    2. ä¿å­˜ MemCell åˆ°æ•°æ®åº“
    3. æäº¤åˆ°å…¨å±€é˜Ÿåˆ—ç”± Worker å¼‚æ­¥å¤„ç†
    4. ç«‹å³è¿”å›ï¼Œä¸ç­‰å¾…åç»­å¤„ç†å®Œæˆ
    """
    logger.info(f"[mem_memorize] request.current_time: {request.current_time}")
    
    # è·å–å½“å‰æ—¶é—´
    if request.current_time:
        current_time = request.current_time
    else:
        current_time = get_now_with_timezone() + timedelta(seconds=1)
    logger.info(f"[mem_memorize] å½“å‰æ—¶é—´: {current_time}")

    memory_manager = MemoryManager()
    
    # ===== MemCell æå–é˜¶æ®µ =====
    if request.raw_data_type == RawDataType.CONVERSATION:
        request = await preprocess_conv_request(request, current_time)
        if request == None:
            logger.warning(f"[mem_memorize] preprocess_conv_request è¿”å› None")
            return None

    # è¾¹ç•Œæ£€æµ‹
    now = time.time()
    logger.info("=" * 80)
    logger.info(f"[è¾¹ç•Œæ£€æµ‹] å¼€å§‹æ£€æµ‹: group_id={request.group_id}")
    logger.info(f"[è¾¹ç•Œæ£€æµ‹] å†å²æ¶ˆæ¯: {len(request.history_raw_data_list)} æ¡")
    logger.info(f"[è¾¹ç•Œæ£€æµ‹] æ–°æ¶ˆæ¯: {len(request.new_raw_data_list)} æ¡")
    logger.info("=" * 80)

    memcell_result = await memory_manager.extract_memcell(
        request.history_raw_data_list,
        request.new_raw_data_list,
        request.raw_data_type,
        request.group_id,
        request.group_name,
        request.user_id_list,
    )
    logger.debug(f"[mem_memorize] æå– MemCell è€—æ—¶: {time.time() - now}ç§’")

    if memcell_result == None:
        logger.warning(f"[mem_memorize] è·³è¿‡æå–MemCell")
        return None

    memcell, status_result = memcell_result

    # æ£€æŸ¥è¾¹ç•Œæ£€æµ‹ç»“æœ
    logger.info("=" * 80)
    logger.info(f"[è¾¹ç•Œæ£€æµ‹ç»“æœ] memcell is None: {memcell is None}")
    if memcell is None:
        logger.info(
            f"[è¾¹ç•Œæ£€æµ‹ç»“æœ] åˆ¤æ–­: {'éœ€è¦ç­‰å¾…æ›´å¤šæ¶ˆæ¯' if status_result.should_wait else 'éè¾¹ç•Œï¼Œç»§ç»­ç´¯ç§¯'}"
        )
    else:
        logger.info(f"[è¾¹ç•Œæ£€æµ‹ç»“æœ] åˆ¤æ–­: æ˜¯è¾¹ç•Œï¼event_id={memcell.event_id}")
    logger.info("=" * 80)

    if memcell == None:
        # ä¿å­˜æ–°æ¶ˆæ¯åˆ° conversation_data_repo
        await conversation_data_repo.save_conversation_data(
            request.new_raw_data_list, request.group_id
        )
        await update_status_when_no_memcell(
            request, status_result, current_time, request.raw_data_type
        )
        logger.warning(f"[mem_memorize] æœªæ£€æµ‹åˆ°è¾¹ç•Œï¼Œè¿”å›")
        return None

        # åˆ¤æ–­ä¸ºè¾¹ç•Œï¼Œæ¸…ç©ºå¯¹è¯å†å²æ•°æ®ï¼ˆé‡æ–°å¼€å§‹ç´¯ç§¯ï¼‰
        try:
            conversation_data_repo = get_bean_by_type(ConversationDataRepository)
            delete_success = await conversation_data_repo.delete_conversation_data(
                request.group_id
            )
            if delete_success:
                logger.info(
                    f"[mem_memorize] åˆ¤æ–­ä¸ºè¾¹ç•Œï¼Œå·²æ¸…ç©ºå¯¹è¯å†å²: group_id={request.group_id}"
                )
            else:
                logger.warning(
                    f"[mem_memorize] æ¸…ç©ºå¯¹è¯å†å²å¤±è´¥: group_id={request.group_id}"
                )
            # ä¿å­˜æ–°æ¶ˆæ¯åˆ° conversation_data_repo
            await conversation_data_repo.save_conversation_data(
                request.new_raw_data_list, request.group_id
            )
        except Exception as e:
            logger.error(f"[mem_memorize] æ¸…ç©ºå¯¹è¯å†å²å¼‚å¸¸: {e}")
            traceback.print_exc()
    # TODO: è¯»çŠ¶æ€è¡¨ï¼Œè¯»å–ç´¯ç§¯çš„MemCellæ•°æ®è¡¨ï¼Œåˆ¤æ–­æ˜¯å¦è¦åšmemorizeè®¡ç®—

    # MemCellå­˜è¡¨
    memcell = await _save_memcell_to_database(memcell, current_time)
    logger.info(f"[mem_memorize] æˆåŠŸä¿å­˜ MemCell: {memcell.event_id}")

    # ğŸ”¥ æäº¤åˆ°å…¨å±€ Worker é˜Ÿåˆ—ï¼Œå¼‚æ­¥å¤„ç†
    from biz_layer.memorize_worker_service import MemorizeWorkerService
    
    try:
        worker_service = await MemorizeWorkerService.get_instance()
        await worker_service.submit_memcell(
            memcell=memcell,
            request=request,
            current_time=current_time,
        )
        logger.info(f"[mem_memorize] âœ… MemCell å·²æäº¤åˆ° Worker é˜Ÿåˆ—ï¼Œç«‹å³è¿”å›")
    except Exception as e:
        logger.error(f"[mem_memorize] âŒ æäº¤åˆ° Worker é˜Ÿåˆ—å¤±è´¥: {e}")
        traceback.print_exc()
    
    # ç«‹å³è¿”å›ç©ºåˆ—è¡¨ï¼ˆè®°å¿†å°†å¼‚æ­¥ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
    return []


def get_version_from_request(request: MemorizeOfflineRequest) -> str:
    # 1. è·å– memorize_to æ—¥æœŸ
    target_date = request.memorize_to

    # 2. å€’é€€ä¸€å¤©
    previous_day = target_date - timedelta(days=1)

    # 3. æ ¼å¼åŒ–ä¸º "YYYY-MM" å­—ç¬¦ä¸²
    return previous_day.strftime("%Y-%m")

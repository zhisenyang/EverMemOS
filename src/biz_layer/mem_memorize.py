

from dataclasses import dataclass
import random
import time
import json
import traceback
from memory_layer.memory_manager import MemorizeRequest, MemorizeOfflineRequest
from memory_layer.memory_manager import MemoryManager
from memory_layer.types import (
    MemoryType,
    MemCell,
    Memory,
    RawDataType,
    SemanticMemoryItem,
)
from memory_layer.memory_extractor.event_log_extractor import EventLog
from memory_layer.memcell_extractor.base_memcell_extractor import RawData
from infra_layer.adapters.out.persistence.document.memory.memcell import DataTypeEnum
from memory_layer.memory_extractor.profile_memory_extractor import (
    ProfileMemory,
    ProfileMemoryExtractor,
    ProfileMemoryExtractRequest,
    ProfileMemoryMerger,
    ProjectInfo,
)
from memory_layer.memory_extractor.group_profile_memory_extractor import (
    GroupProfileMemoryExtractor,
    GroupProfileMemoryExtractRequest,
    GroupProfileMemory,
)
from core.di import get_bean_by_type
from component.redis_provider import RedisProvider
from infra_layer.adapters.out.persistence.repository.episodic_memory_raw_repository import (
    EpisodicMemoryRawRepository,
)
from infra_layer.adapters.out.persistence.repository.semantic_memory_record_repository import (
    SemanticMemoryRecordRawRepository,
)
from infra_layer.adapters.out.persistence.repository.event_log_record_repository import (
    EventLogRecordRawRepository,
)
from infra_layer.adapters.out.persistence.repository.conversation_status_raw_repository import (
    ConversationStatusRawRepository,
)
from infra_layer.adapters.out.persistence.repository.conversation_meta_raw_repository import (
    ConversationMetaRawRepository,
)
from infra_layer.adapters.out.persistence.repository.core_memory_raw_repository import (
    CoreMemoryRawRepository,
)
from infra_layer.adapters.out.persistence.repository.group_user_profile_memory_raw_repository import (
    GroupUserProfileMemoryRawRepository,
)
from infra_layer.adapters.out.persistence.repository.group_profile_raw_repository import (
    GroupProfileRawRepository,
)
from biz_layer.conversation_data_repo import ConversationDataRepository
from memory_layer.types import RawDataType
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
import uuid
from datetime import datetime, timedelta
import os
import asyncio
from collections import defaultdict
from common_utils.datetime_utils import (
    get_now_with_timezone,
    to_iso_format,
    from_iso_format,
)
from memory_layer.memcell_extractor.base_memcell_extractor import StatusResult
import traceback

from core.lock.redis_distributed_lock import distributed_lock
from core.observation.logger import get_logger
from infra_layer.adapters.out.search.elasticsearch.converter.episodic_memory_converter import (
    EpisodicMemoryConverter,
)
from infra_layer.adapters.out.search.milvus.converter.episodic_memory_milvus_converter import (
    EpisodicMemoryMilvusConverter,
)
from infra_layer.adapters.out.search.elasticsearch.converter.semantic_memory_converter import (
    SemanticMemoryConverter,
)
from infra_layer.adapters.out.search.milvus.converter.semantic_memory_milvus_converter import (
    SemanticMemoryMilvusConverter,
)
from infra_layer.adapters.out.search.elasticsearch.converter.event_log_converter import (
    EventLogConverter,
)
from infra_layer.adapters.out.search.milvus.converter.event_log_milvus_converter import (
    EventLogMilvusConverter,
)
from infra_layer.adapters.out.search.repository.episodic_memory_milvus_repository import (
    EpisodicMemoryMilvusRepository,
)
from infra_layer.adapters.out.search.repository.episodic_memory_es_repository import (
    EpisodicMemoryEsRepository,
)
from infra_layer.adapters.out.search.repository.semantic_memory_milvus_repository import (
    SemanticMemoryMilvusRepository,
)
from infra_layer.adapters.out.search.repository.event_log_milvus_repository import (
    EventLogMilvusRepository,
)
from biz_layer.mem_sync import MemorySyncService

logger = get_logger(__name__)

@dataclass
class MemoryDocPayload:
    memory_type: MemoryType
    doc: Any


def _clone_semantic_memory_item(raw_item: Any) -> Optional[SemanticMemoryItem]:
    """å°†ä»»æ„ç»“æ„çš„è¯­ä¹‰è®°å¿†æ¡ç›®è½¬æ¢ä¸º SemanticMemoryItem å®ä¾‹"""
    if raw_item is None:
        return None

    if isinstance(raw_item, SemanticMemoryItem):
        return SemanticMemoryItem(
            content=raw_item.content,
            evidence=getattr(raw_item, "evidence", None),
            start_time=getattr(raw_item, "start_time", None),
            end_time=getattr(raw_item, "end_time", None),
            duration_days=getattr(raw_item, "duration_days", None),
            source_episode_id=getattr(raw_item, "source_episode_id", None),
            embedding=getattr(raw_item, "embedding", None),
        )

    if isinstance(raw_item, dict):
        return SemanticMemoryItem(
            content=raw_item.get("content", ""),
            evidence=raw_item.get("evidence"),
            start_time=raw_item.get("start_time"),
            end_time=raw_item.get("end_time"),
            duration_days=raw_item.get("duration_days"),
            source_episode_id=raw_item.get("source_episode_id"),
            embedding=raw_item.get("embedding"),
        )

    return None


def _clone_event_log(raw_event_log: Any) -> Optional[EventLog]:
    """å°†ä»»æ„ç»“æ„çš„äº‹ä»¶æ—¥å¿—è½¬æ¢ä¸º EventLog å®ä¾‹"""
    if raw_event_log is None:
        return None

    if isinstance(raw_event_log, EventLog):
        return EventLog(
            time=getattr(raw_event_log, "time", ""),
            atomic_fact=list(getattr(raw_event_log, "atomic_fact", []) or []),
            fact_embeddings=getattr(raw_event_log, "fact_embeddings", None),
        )

    if isinstance(raw_event_log, dict):
        return EventLog.from_dict(raw_event_log)

    return None

async def _trigger_clustering(
    group_id: str, memcell: MemCell, scene: Optional[str] = None
) -> None:
    """å¼‚æ­¥è§¦å‘ MemCell èšç±»ï¼ˆåå°ä»»åŠ¡ï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰

    Args:
        group_id: ç¾¤ç»„ID
        memcell: åˆšä¿å­˜çš„ MemCell
        scene: å¯¹è¯åœºæ™¯ï¼ˆç”¨äºå†³å®š Profile æå–ç­–ç•¥ï¼‰
            - None/"work"/"company" ç­‰ï¼šä½¿ç”¨ group_chat åœºæ™¯
            - "assistant"/"companion" ç­‰ï¼šä½¿ç”¨ assistant åœºæ™¯
    """
    logger.info(
        f"[èšç±»] å¼€å§‹è§¦å‘èšç±»: group_id={group_id}, event_id={memcell.event_id}, scene={scene}"
    )

    try:
        from memory_layer.cluster_manager import (
            ClusterManager,
            ClusterManagerConfig,
            MongoClusterStorage,
        )
        from memory_layer.profile_manager import (
            ProfileManager,
            ProfileManagerConfig,
            MongoProfileStorage,
        )
        from memory_layer.llm.llm_provider import LLMProvider
        from core.di import get_bean_by_type
        import os

        logger.info(f"[èšç±»] æ­£åœ¨è·å– MongoClusterStorage...")
        # è·å– MongoDB å­˜å‚¨
        mongo_storage = get_bean_by_type(MongoClusterStorage)
        logger.info(f"[èšç±»] MongoClusterStorage è·å–æˆåŠŸ: {type(mongo_storage)}")

        # åˆ›å»º ClusterManagerï¼ˆä½¿ç”¨ MongoDB å­˜å‚¨ï¼‰
        config = ClusterManagerConfig(
            similarity_threshold=0.65,
            max_time_gap_days=7,
            enable_persistence=False,  # MongoDB ä¸éœ€è¦æ–‡ä»¶æŒä¹…åŒ–
        )
        cluster_manager = ClusterManager(config=config, storage=mongo_storage)
        logger.info(f"[èšç±»] ClusterManager åˆ›å»ºæˆåŠŸ")

        # åˆ›å»º ProfileManager å¹¶è¿æ¥åˆ° ClusterManager
        # è·å– MongoDB Profile å­˜å‚¨
        profile_storage = get_bean_by_type(MongoProfileStorage)
        logger.info(f"[èšç±»] MongoProfileStorage è·å–æˆåŠŸ: {type(profile_storage)}")

        llm_provider = LLMProvider(
            provider_type=os.getenv("LLM_PROVIDER", "openai"),
            model=os.getenv("LLM_MODEL", "gpt-4"),
            base_url=os.getenv("LLM_BASE_URL"),
            api_key=os.getenv("LLM_API_KEY"),
            temperature=float(os.getenv("LLM_TEMPERATURE", "0.3")),
            max_tokens=int(os.getenv("LLM_MAX_TOKENS", "16384")),
        )

        # æ ¹æ® scene å†³å®š Profile æå–åœºæ™¯
        # assistant/companion -> assistant åœºæ™¯ï¼ˆæå–å…´è¶£ã€åå¥½ã€ç”Ÿæ´»ä¹ æƒ¯ï¼‰
        # å…¶ä»– -> group_chat åœºæ™¯ï¼ˆæå–å·¥ä½œè§’è‰²ã€æŠ€èƒ½ã€é¡¹ç›®ç»éªŒï¼‰
        profile_scenario = (
            "assistant"
            if scene and scene.lower() in ["assistant", "companion"]
            else "group_chat"
        )

        profile_config = ProfileManagerConfig(
            scenario=profile_scenario,
            min_confidence=0.6,
            enable_versioning=True,
            auto_extract=True,
        )

        profile_manager = ProfileManager(
            llm_provider=llm_provider,
            config=profile_config,
            storage=profile_storage,  # ä½¿ç”¨ MongoDB å­˜å‚¨
            group_id=group_id,
            group_name=None,  # å¯ä»¥ä» memcell ä¸­è·å–
        )

        # è¿æ¥ ProfileManager åˆ° ClusterManager
        profile_manager.attach_to_cluster_manager(cluster_manager)
        logger.info(
            f"[èšç±»] ProfileManager å·²è¿æ¥åˆ° ClusterManager (åœºæ™¯: {profile_scenario}, ä½¿ç”¨ MongoDB å­˜å‚¨)"
        )
        print(
            f"[èšç±»] ProfileManager å·²è¿æ¥ï¼Œé˜ˆå€¼: {profile_manager._min_memcells_threshold}"
        )

        # å°† MemCell è½¬æ¢ä¸ºèšç±»æ‰€éœ€çš„å­—å…¸æ ¼å¼
        memcell_dict = {
            "event_id": str(memcell.event_id),
            "episode": memcell.episode,
            "timestamp": memcell.timestamp.timestamp() if memcell.timestamp else None,
            "participants": memcell.participants or [],
            "group_id": group_id,
        }

        logger.info(f"[èšç±»] å¼€å§‹æ‰§è¡Œèšç±»: {memcell_dict['event_id']}")
        print(f"[èšç±»] å¼€å§‹æ‰§è¡Œèšç±»: event_id={memcell_dict['event_id']}")

        # æ‰§è¡Œèšç±»ï¼ˆä¼šè‡ªåŠ¨è§¦å‘ ProfileManager çš„å›è°ƒï¼‰
        cluster_id = await cluster_manager.cluster_memcell(
            group_id=group_id, memcell=memcell_dict
        )

        print(f"[èšç±»] èšç±»å®Œæˆ: cluster_id={cluster_id}")

        if cluster_id:
            logger.info(
                f"[èšç±»] âœ… MemCell {memcell.event_id} -> Cluster {cluster_id} (group: {group_id})"
            )
            print(f"[èšç±»] âœ… MemCell {memcell.event_id} -> Cluster {cluster_id}")
        else:
            logger.warning(
                f"[èšç±»] âš ï¸ MemCell {memcell.event_id} èšç±»è¿”å› None (group: {group_id})"
            )
            print(f"[èšç±»] âš ï¸ èšç±»è¿”å› None")

    except Exception as e:
        # èšç±»å¤±è´¥ï¼Œæ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯å¹¶é‡æ–°æŠ›å‡º
        import traceback

        error_msg = f"[èšç±»] âŒ è§¦å‘èšç±»å¤±è´¥: {e}"
        logger.error(error_msg, exc_info=True)
        print(error_msg)  # ç¡®ä¿åœ¨æ§åˆ¶å°èƒ½çœ‹åˆ°
        print(traceback.format_exc())
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…çŸ¥é“å¤±è´¥äº†


def _convert_data_type_to_raw_data_type(data_type) -> RawDataType:
    """
    å°†ä¸åŒçš„æ•°æ®ç±»å‹æšä¸¾è½¬æ¢ä¸ºç»Ÿä¸€çš„RawDataType

    Args:
        data_type: å¯èƒ½æ˜¯DataTypeEnumã€RawDataTypeæˆ–å­—ç¬¦ä¸²

    Returns:
        RawDataType: è½¬æ¢åçš„ç»Ÿä¸€æ•°æ®ç±»å‹
    """
    if isinstance(data_type, RawDataType):
        return data_type

    # è·å–å­—ç¬¦ä¸²å€¼
    if hasattr(data_type, 'value'):
        type_str = data_type.value
    else:
        type_str = str(data_type)

    # æ˜ å°„è½¬æ¢
    type_mapping = {
        "Conversation": RawDataType.CONVERSATION,
        "CONVERSATION": RawDataType.CONVERSATION,
        # å…¶ä»–ç±»å‹æ˜ å°„åˆ°CONVERSATIONä½œä¸ºé»˜è®¤å€¼
    }

    return type_mapping.get(type_str, RawDataType.CONVERSATION)


from biz_layer.mem_db_operations import (
    _convert_timestamp_to_time,
    _convert_episode_memory_to_doc,
    _convert_semantic_memory_to_doc,
    _convert_event_log_to_docs,
    _save_memcell_to_database,
    _save_profile_memory_to_core,
    ConversationStatus,
    _update_status_for_new_conversation,
    _update_status_for_continuing_conversation,
    _update_status_after_memcell_extraction,
    _convert_original_data_for_profile_extractor,
    _save_group_profile_memory,
    _save_profile_memory_to_group_user_profile_memory,
    _convert_document_to_group_importance_evidence,
    _normalize_datetime_for_storage,
    _convert_projects_participated_list,
    _convert_group_profile_raw_to_memory_format,
)


def if_memorize(memcells: List[MemCell]) -> bool:
    return True


def extract_message_time(raw_data):
    """
    ä»RawDataå¯¹è±¡ä¸­æå–æ¶ˆæ¯æ—¶é—´

    Args:
        raw_data: RawDataå¯¹è±¡

    Returns:
        datetime: æ¶ˆæ¯æ—¶é—´ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
    """
    # ä¼˜å…ˆä»timestampå­—æ®µè·å–
    if hasattr(raw_data, 'timestamp') and raw_data.timestamp:
        try:
            return _normalize_datetime_for_storage(raw_data.timestamp)
        except Exception as e:
            logger.debug(f"Failed to parse timestamp from raw_data.timestamp: {e}")
            pass

    # ä»extendå­—æ®µè·å–
    if (
        hasattr(raw_data, 'extend')
        and raw_data.extend
        and isinstance(raw_data.extend, dict)
    ):
        timestamp_val = raw_data.extend.get('timestamp')
        if timestamp_val:
            try:
                return _normalize_datetime_for_storage(timestamp_val)
            except Exception as e:
                logger.debug(f"Failed to parse timestamp from extend field: {e}")
                pass

    return None


from core.observation.tracing.decorators import trace_logger


@trace_logger(operation_name="mem_memorize preprocess_conv_request", log_level="info")
async def preprocess_conv_request(
    request: MemorizeRequest, current_time: datetime
) -> MemorizeRequest:
    """
    ç®€åŒ–ç‰ˆçš„è¯·æ±‚é¢„å¤„ç†ï¼š
    1. ä» Redis è¯»å–æ‰€æœ‰å†å²æ¶ˆæ¯
    2. å°†å†å²æ¶ˆæ¯ä½œä¸º history_raw_data_list
    3. å°†å½“å‰æ–°æ¶ˆæ¯ä½œä¸º new_raw_data_list
    4. è¾¹ç•Œæ£€æµ‹ç”±åç»­é€»è¾‘å¤„ç†ï¼ˆæ£€æµ‹åä¼šæ¸…ç©ºæˆ–ä¿ç•™ Redisï¼‰
    """

    logger.info(f"[preprocess] å¼€å§‹å¤„ç†: group_id={request.group_id}")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®
    if not request.new_raw_data_list:
        logger.info("[preprocess] æ²¡æœ‰æ–°æ•°æ®ï¼Œè·³è¿‡å¤„ç†")
        return None

    # ä½¿ç”¨ conversation_data_repo è¿›è¡Œå…ˆå–åå­˜æ“ä½œ
    conversation_data_repo = get_bean_by_type(ConversationDataRepository)

    try:
        # ç¬¬ä¸€æ­¥ï¼šå…ˆä» conversation_data_repo è·å–å†å²æ¶ˆæ¯
        # è¿™é‡Œä¸é™åˆ¶æ—¶é—´èŒƒå›´ï¼Œè·å–æœ€è¿‘1000æ¡å†å²æ¶ˆæ¯ï¼ˆç”±ç¼“å­˜ç®¡ç†å™¨çš„max_lengthæ§åˆ¶ï¼‰
        history_raw_data_list = await conversation_data_repo.get_conversation_data(
            group_id=request.group_id, start_time=None, end_time=None, limit=1000
        )

        logger.info(
            f"[preprocess] ä» conversation_data_repo è¯»å– {len(history_raw_data_list)} æ¡å†å²æ¶ˆæ¯"
        )

        # ç¬¬äºŒæ­¥ï¼šä¿å­˜æ–°æ¶ˆæ¯åˆ° conversation_data_repo
        save_success = await conversation_data_repo.save_conversation_data(
            request.new_raw_data_list, request.group_id
        )

        if save_success:
            logger.info(
                f"[preprocess] æˆåŠŸä¿å­˜ {len(request.new_raw_data_list)} æ¡æ–°æ¶ˆæ¯"
            )
        else:
            logger.warning(f"[preprocess] ä¿å­˜æ–°æ¶ˆæ¯å¤±è´¥")

        # æ›´æ–° request
        request.history_raw_data_list = history_raw_data_list
        # new_raw_data_list ä¿æŒä¸å˜ï¼ˆå°±æ˜¯æ–°ä¼ å…¥çš„æ¶ˆæ¯ï¼‰

        logger.info(
            f"[preprocess] å®Œæˆ: å†å² {len(history_raw_data_list)} æ¡, æ–°æ¶ˆæ¯ {len(request.new_raw_data_list)} æ¡"
        )

        return request

    except Exception as e:
        logger.error(f"[preprocess] Redis è¯»å–å¤±è´¥: {e}")
        traceback.print_exc()
        # Redis å¤±è´¥æ—¶ï¼Œä½¿ç”¨åŸå§‹ request
        return request


async def update_status_when_no_memcell(
    request: MemorizeRequest,
    status_result: StatusResult,
    current_time: datetime,
    data_type: RawDataType,
):
    if data_type == RawDataType.CONVERSATION:
        # å°è¯•æ›´æ–°çŠ¶æ€è¡¨
        try:
            status_repo = get_bean_by_type(ConversationStatusRawRepository)

            if status_result.should_wait:
                logger.info(f"[mem_memorize] åˆ¤æ–­ä¸ºæ— æ³•åˆ¤æ–­è¾¹ç•Œç»§ç»­ç­‰å¾…ï¼Œä¸æ›´æ–°çŠ¶æ€è¡¨")
                return
            else:
                logger.info(f"[mem_memorize] åˆ¤æ–­ä¸ºéè¾¹ç•Œï¼Œç»§ç»­ç´¯ç§¯msgï¼Œæ›´æ–°çŠ¶æ€è¡¨")
                # è·å–æœ€æ–°æ¶ˆæ¯æ—¶é—´æˆ³
                latest_time = _convert_timestamp_to_time(current_time, current_time)
                if request.new_raw_data_list:
                    last_msg = request.new_raw_data_list[-1]
                    if hasattr(last_msg, 'content') and isinstance(
                        last_msg.content, dict
                    ):
                        latest_time = last_msg.content.get('timestamp', latest_time)
                    elif hasattr(last_msg, 'timestamp'):
                        latest_time = last_msg.timestamp

                if not latest_time:
                    latest_time = min(latest_time, current_time)

                # ä½¿ç”¨å°è£…å‡½æ•°æ›´æ–°å¯¹è¯å»¶ç»­çŠ¶æ€
                await _update_status_for_continuing_conversation(
                    status_repo, request, latest_time, current_time
                )

        except Exception as e:
            logger.error(f"æ›´æ–°çŠ¶æ€è¡¨å¤±è´¥: {e}")
    else:
        pass


async def update_status_after_memcell(
    request: MemorizeRequest,
    memcells: List[MemCell],
    current_time: datetime,
    data_type: RawDataType,
):
    if data_type == RawDataType.CONVERSATION:
        # æ›´æ–°çŠ¶æ€è¡¨ä¸­çš„last_memcell_timeè‡³memcellsæœ€åä¸€ä¸ªæ—¶é—´æˆ³
        try:
            status_repo = get_bean_by_type(ConversationStatusRawRepository)

            # è·å–MemCellçš„æ—¶é—´æˆ³
            memcell_time = None
            if memcells and hasattr(memcells[-1], 'timestamp'):
                memcell_time = memcells[-1].timestamp
            else:
                memcell_time = current_time

            # ä½¿ç”¨å°è£…å‡½æ•°æ›´æ–°MemCellæå–åçš„çŠ¶æ€
            await _update_status_after_memcell_extraction(
                status_repo, request, memcell_time, current_time
            )

            logger.info(f"[mem_memorize] è®°å¿†æå–å®Œæˆï¼ŒçŠ¶æ€è¡¨å·²æ›´æ–°")

        except Exception as e:
            logger.error(f"æœ€ç»ˆçŠ¶æ€è¡¨æ›´æ–°å¤±è´¥: {e}")
    else:
        pass


async def save_personal_profile_memory(
    profile_memories: List[ProfileMemory], version: Optional[str] = None
):
    logger.info(f"[mem_memorize] ä¿å­˜ {len(profile_memories)} ä¸ªä¸ªäººæ¡£æ¡ˆè®°å¿†åˆ°æ•°æ®åº“")
    # åˆå§‹åŒ–Repositoryå®ä¾‹
    core_memory_repo = get_bean_by_type(CoreMemoryRawRepository)

    # ä¿å­˜ä¸ªäººæ¡£æ¡ˆè®°å¿†åˆ°GroupUserProfileMemoryRawRepository
    for profile_mem in profile_memories:
        await _save_profile_memory_to_core(profile_mem, core_memory_repo, version)
        # ç§»é™¤å•ä¸ªæ“ä½œæˆåŠŸæ—¥å¿—


async def save_memory_docs(
    doc_payloads: List[MemoryDocPayload], version: Optional[str] = None
) -> Dict[MemoryType, List[Any]]:
    """
    é€šç”¨ Doc ä¿å­˜å‡½æ•°ï¼ŒæŒ‰ MemoryType æšä¸¾è‡ªåŠ¨ä¿å­˜å¹¶åŒæ­¥
    """

    grouped_docs: Dict[MemoryType, List[Any]] = defaultdict(list)
    for payload in doc_payloads:
        if payload and payload.doc:
            grouped_docs[payload.memory_type].append(payload.doc)

    saved_result: Dict[MemoryType, List[Any]] = {}

    # Episodic
    episodic_docs = grouped_docs.get(MemoryType.EPISODIC_MEMORY, [])
    if episodic_docs:
        episodic_repo = get_bean_by_type(EpisodicMemoryRawRepository)
        episodic_milvus_repo = get_bean_by_type(EpisodicMemoryMilvusRepository)
        saved_episodic: List[Any] = []

        for doc in episodic_docs:
            saved_doc = await episodic_repo.append_episodic_memory(doc)
            saved_episodic.append(saved_doc)

            es_doc = EpisodicMemoryConverter.from_mongo(saved_doc)
            await es_doc.save()

            milvus_entity = EpisodicMemoryMilvusConverter.from_mongo(saved_doc)
            vector = (
                milvus_entity.get("vector") if isinstance(milvus_entity, dict) else None
            )
            if vector and len(vector) > 0:
                await episodic_milvus_repo.insert(milvus_entity, flush=False)
            else:
                logger.warning(
                    "[mem_memorize] è·³è¿‡å†™å…¥Milvusï¼šå‘é‡ä¸ºç©ºæˆ–ç¼ºå¤±ï¼Œevent_id=%s",
                    getattr(saved_doc, "event_id", None),
                )

    
        saved_result[MemoryType.EPISODIC_MEMORY] = saved_episodic

    # Semantic
    semantic_docs = grouped_docs.get(MemoryType.SEMANTIC_MEMORY, [])
    if semantic_docs:
        semantic_repo = get_bean_by_type(SemanticMemoryRecordRawRepository)
        saved_semantic = await semantic_repo.create_batch(semantic_docs)
        saved_result[MemoryType.SEMANTIC_MEMORY] = saved_semantic

        sync_service = get_bean_by_type(MemorySyncService)
        await sync_service.sync_batch_semantic_memories(
            saved_semantic, sync_to_es=True, sync_to_milvus=True
        )

    # Event Log
    event_log_docs = grouped_docs.get(MemoryType.PERSONAL_EVENT_LOG, [])
    if event_log_docs:
        event_log_repo = get_bean_by_type(EventLogRecordRawRepository)
        saved_event_logs = await event_log_repo.create_batch(event_log_docs)
        saved_result[MemoryType.PERSONAL_EVENT_LOG] = saved_event_logs

        sync_service = get_bean_by_type(MemorySyncService)
        await sync_service.sync_batch_event_logs(
            saved_event_logs, sync_to_es=True, sync_to_milvus=True
        )

    # Profile
    profile_docs = grouped_docs.get(MemoryType.PROFILE, [])
    if profile_docs:
        group_user_profile_repo = get_bean_by_type(
            GroupUserProfileMemoryRawRepository
        )
        saved_profiles = []
        for profile_mem in profile_docs:
            try:
                await _save_profile_memory_to_group_user_profile_memory(
                    profile_mem, group_user_profile_repo, version
                )
                saved_profiles.append(profile_mem)
            except Exception as exc:
                logger.error(f"ä¿å­˜Profileè®°å¿†å¤±è´¥: {exc}")
        if saved_profiles:
            saved_result[MemoryType.PROFILE] = saved_profiles

    group_profile_docs = grouped_docs.get(MemoryType.GROUP_PROFILE, [])
    if group_profile_docs:
        group_profile_repo = get_bean_by_type(GroupProfileRawRepository)
        saved_group_profiles = []
        for mem in group_profile_docs:
            try:
                await _save_group_profile_memory(mem, group_profile_repo, version)
                saved_group_profiles.append(mem)
            except Exception as exc:
                logger.error(f"ä¿å­˜Group Profileè®°å¿†å¤±è´¥: {exc}")
        if saved_group_profiles:
            saved_result[MemoryType.GROUP_PROFILE] = saved_group_profiles

    return saved_result


async def load_core_memories(
    request: MemorizeRequest, participants: List[str], current_time: datetime
):
    logger.info(f"[mem_memorize] è¯»å–ç”¨æˆ·æ•°æ®: {participants}")
    # åˆå§‹åŒ–Repositoryå®ä¾‹
    core_memory_repo = get_bean_by_type(CoreMemoryRawRepository)

    # è¯»å–ç”¨æˆ·CoreMemoryæ•°æ®
    user_core_memories = {}
    for user_id in participants:
        try:
            core_memory = await core_memory_repo.get_by_user_id(user_id)
            if core_memory:
                user_core_memories[user_id] = core_memory
            # ç§»é™¤å•ä¸ªç”¨æˆ·çš„æˆåŠŸ/å¤±è´¥æ—¥å¿—
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ· {user_id} CoreMemoryå¤±è´¥: {e}")

    logger.info(f"[mem_memorize] è·å–åˆ° {len(user_core_memories)} ä¸ªç”¨æˆ·CoreMemory")

    # ç›´æ¥ä»CoreMemoryè½¬æ¢ä¸ºProfileMemoryå¯¹è±¡åˆ—è¡¨
    old_memory_list = []
    if user_core_memories:
        for user_id, core_memory in user_core_memories.items():
            if core_memory:
                # ç›´æ¥åˆ›å»ºProfileMemoryå¯¹è±¡
                profile_memory = ProfileMemory(
                    # Memory åŸºç±»å¿…éœ€å­—æ®µ
                    memory_type=MemoryType.CORE,
                    user_id=user_id,
                    timestamp=to_iso_format(current_time),
                    ori_event_id_list=[],
                    # Memory åŸºç±»å¯é€‰å­—æ®µ
                    subject=f"{getattr(core_memory, 'user_name', user_id)}çš„ä¸ªäººæ¡£æ¡ˆ",
                    summary=f"ç”¨æˆ·{user_id}çš„åŸºæœ¬ä¿¡æ¯ï¼š{getattr(core_memory, 'position', 'æœªçŸ¥è§’è‰²')}",
                    group_id=request.group_id,
                    participants=[user_id],
                    type=RawDataType.CONVERSATION,
                    # ProfileMemory ç‰¹æœ‰å­—æ®µ - ç›´æ¥ä½¿ç”¨åŸå§‹å­—å…¸æ ¼å¼
                    hard_skills=getattr(core_memory, 'hard_skills', None),
                    soft_skills=getattr(core_memory, 'soft_skills', None),
                    output_reasoning=getattr(core_memory, 'output_reasoning', None),
                    motivation_system=getattr(core_memory, 'motivation_system', None),
                    fear_system=getattr(core_memory, 'fear_system', None),
                    value_system=getattr(core_memory, 'value_system', None),
                    humor_use=getattr(core_memory, 'humor_use', None),
                    colloquialism=getattr(core_memory, 'colloquialism', None),
                    projects_participated=_convert_projects_participated_list(
                        getattr(core_memory, 'projects_participated', None)
                    ),
                )
                old_memory_list.append(profile_memory)

        logger.info(
            f"[mem_memorize] ç›´æ¥è½¬æ¢äº† {len(old_memory_list)} ä¸ªCoreMemoryä¸ºProfileMemory"
        )
    else:
        logger.info(f"[mem_memorize] æ²¡æœ‰ç”¨æˆ·CoreMemoryæ•°æ®ï¼Œold_memory_listä¸ºç©º")


async def memorize(request: MemorizeRequest) -> List[Memory]:

    # logger.info(f"[mem_memorize] request: {request}")

    # logger.info(f"[mem_memorize] memorize request: {request}")
    logger.info(f"[mem_memorize] request.current_time: {request.current_time}")
    # è·å–å½“å‰æ—¶é—´ï¼Œç”¨äºæ‰€æœ‰æ—¶é—´ç›¸å…³æ“ä½œ
    if request.current_time:
        current_time = request.current_time
    else:
        current_time = get_now_with_timezone() + timedelta(seconds=1)
    logger.info(f"[mem_memorize] å½“å‰æ—¶é—´: {current_time}")

    memory_manager = MemoryManager()

    # å®šä¹‰éœ€è¦æå–çš„è®°å¿†ç±»å‹ï¼šå…ˆæå–ä¸ªäºº episodeï¼Œå†åŸºäº episode æå–è¯­ä¹‰è®°å¿†å’Œäº‹ä»¶æ—¥å¿—
    memory_types = [
        MemoryType.EPISODIC_MEMORY,
        MemoryType.SEMANTIC_MEMORY,
        MemoryType.PERSONAL_EVENT_LOG,
    ]
    if request.raw_data_type == RawDataType.CONVERSATION:
        request = await preprocess_conv_request(request, current_time)
        if request == None:
            return None

    if request.raw_data_type == RawDataType.CONVERSATION:
        # async with distributed_lock(f"memcell_extract_{request.group_id}") as acquired:
        #     # 120sç­‰å¾…ï¼Œè·å–ä¸åˆ°
        #     if not acquired:
        #         logger.warning(f"[mem_memorize] è·å–åˆ†å¸ƒå¼é”å¤±è´¥: {request.group_id}")
        now = time.time()

        # æ·»åŠ è¯¦ç»†è°ƒè¯•æ—¥å¿—
        logger.info(f"=" * 80)
        logger.info(f"[è¾¹ç•Œæ£€æµ‹] å¼€å§‹æ£€æµ‹: group_id={request.group_id}")
        logger.info(f"[è¾¹ç•Œæ£€æµ‹] å†å²æ¶ˆæ¯: {len(request.history_raw_data_list)} æ¡")
        logger.info(f"[è¾¹ç•Œæ£€æµ‹] æ–°æ¶ˆæ¯: {len(request.new_raw_data_list)} æ¡")
        if request.history_raw_data_list:
            logger.info(
                f"[è¾¹ç•Œæ£€æµ‹] å†å²æ¶ˆæ¯èŒƒå›´: {request.history_raw_data_list[0].content.get('timestamp')} ~ {request.history_raw_data_list[-1].content.get('timestamp')}"
            )
        if request.new_raw_data_list:
            for idx, raw in enumerate(request.new_raw_data_list):
                logger.info(
                    f"[è¾¹ç•Œæ£€æµ‹] æ–°æ¶ˆæ¯[{idx}]: {raw.content.get('speaker_id')} - {raw.content.get('content')[:50]}... @ {raw.content.get('timestamp')}"
                )
        logger.info(f"=" * 80)

        logger.debug(
            f"[memorize memorize] æå–MemCellå¼€å§‹: group_id={request.group_id}, group_name={request.group_name}, "
            f"semantic_extraction={request.enable_semantic_extraction}"
        )
        memcell_result = await memory_manager.extract_memcell(
            request.history_raw_data_list,
            request.new_raw_data_list,
            request.raw_data_type,
            request.group_id,
            request.group_name,
            request.user_id_list,
            enable_semantic_extraction=request.enable_semantic_extraction,
            enable_event_log_extraction=request.enable_event_log_extraction,
        )
        logger.debug(f"[memorize memorize] æå–MemCellè€—æ—¶: {time.time() - now}ç§’")
    else:
        now = time.time()
        logger.debug(
            f"[memorize memorize] æå–MemCellå¼€å§‹: group_id={request.group_id}, group_name={request.group_name}, "
            f"semantic_extraction={request.enable_semantic_extraction}, "
            f"event_log_extraction={request.enable_event_log_extraction}"
        )
        memcell_result = await memory_manager.extract_memcell(
            request.history_raw_data_list,
            request.new_raw_data_list,
            request.raw_data_type,
            request.group_id,
            request.group_name,
            request.user_id_list,
            enable_semantic_extraction=request.enable_semantic_extraction,
            enable_event_log_extraction=request.enable_event_log_extraction,
        )
        logger.debug(f"[memorize memorize] æå–MemCellè€—æ—¶: {time.time() - now}ç§’")

    if memcell_result == None:
        logger.warning(f"[mem_memorize] è·³è¿‡æå–MemCell")
        return None

    logger.debug(f"[mem_memorize] memcell_result: {memcell_result}")
    memcell, status_result = memcell_result

    # æ·»åŠ è¾¹ç•Œæ£€æµ‹ç»“æœæ—¥å¿—
    logger.info(f"=" * 80)
    logger.info(f"[è¾¹ç•Œæ£€æµ‹ç»“æœ] memcell is None: {memcell is None}")
    logger.info(
        f"[è¾¹ç•Œæ£€æµ‹ç»“æœ] should_wait: {status_result.should_wait if status_result else 'N/A'}"
    )
    if memcell is None:
        logger.info(
            f"[è¾¹ç•Œæ£€æµ‹ç»“æœ] åˆ¤æ–­: {'éœ€è¦ç­‰å¾…æ›´å¤šæ¶ˆæ¯' if status_result.should_wait else 'éè¾¹ç•Œï¼Œç»§ç»­ç´¯ç§¯'}"
        )
    else:
        logger.info(f"[è¾¹ç•Œæ£€æµ‹ç»“æœ] åˆ¤æ–­: æ˜¯è¾¹ç•Œï¼æˆåŠŸæå–MemCell")
        logger.info(f"[è¾¹ç•Œæ£€æµ‹ç»“æœ] MemCell event_id: {memcell.event_id}")
        logger.info(
            f"[è¾¹ç•Œæ£€æµ‹ç»“æœ] Episode: {memcell.episode[:100] if memcell.episode else 'None'}..."
        )
    logger.info(f"=" * 80)

    if memcell == None:
        await update_status_when_no_memcell(
            request, status_result, current_time, request.raw_data_type
        )
        logger.warning(f"[mem_memorize] è·³è¿‡æå–MemCell")
        return None
    else:
        logger.info(f"[mem_memorize] æˆåŠŸæå–MemCell")

        # åˆ¤æ–­ä¸ºè¾¹ç•Œï¼Œæ¸…ç©ºå¯¹è¯å†å²æ•°æ®ï¼ˆé‡æ–°å¼€å§‹ç´¯ç§¯ï¼‰
        try:
            conversation_data_repo = get_bean_by_type(ConversationDataRepository)
            delete_success = await conversation_data_repo.delete_conversation_data(
                request.group_id
            )
            if delete_success:
                logger.info(
                    f"[mem_memorize] åˆ¤æ–­ä¸ºè¾¹ç•Œï¼Œå·²æ¸…ç©ºå¯¹è¯å†å²: group_id={request.group_id}"
                )
            else:
                logger.warning(
                    f"[mem_memorize] æ¸…ç©ºå¯¹è¯å†å²å¤±è´¥: group_id={request.group_id}"
                )
        except Exception as e:
            logger.error(f"[mem_memorize] æ¸…ç©ºå¯¹è¯å†å²å¼‚å¸¸: {e}")
            traceback.print_exc()

    # TODO: è¯»çŠ¶æ€è¡¨ï¼Œè¯»å–ç´¯ç§¯çš„MemCellæ•°æ®è¡¨ï¼Œåˆ¤æ–­æ˜¯å¦è¦åšmemorizeè®¡ç®—

    # MemCellå­˜è¡¨
    memcell = await _save_memcell_to_database(memcell, current_time)

    # print_memory = random.random() < 0.1

    logger.info(f"[mem_memorize] æˆåŠŸä¿å­˜MemCell: {memcell.event_id}")

    # if print_memory:
    #     logger.info(f"[mem_memorize] æ‰“å°MemCell: {memcell}")

    memcells = [memcell]

    group_episode_memories: List[Memory] = [
        Memory(
            memory_type=MemoryType.EPISODIC_MEMORY,
            user_id=None,  # ç¾¤ç»„è®°å¿†çš„ user_id ä¸º None
            timestamp=memcell.timestamp or current_time,
            ori_event_id_list=[memcell.event_id],
            subject=memcell.subject,
            summary=memcell.summary,
            episode=memcell.episode,
            group_id=memcell.group_id,
            group_name=memcell.group_name or request.group_name,
            participants=memcell.participants,
            type=memcell.type,
            keywords=memcell.keywords,
            linked_entities=memcell.linked_entities,
            memcell_event_id_list=[memcell.event_id],
            user_name=memcell.group_name or request.group_name,
        )
    ]

    # åŒæ­¥è§¦å‘èšç±»ï¼ˆç­‰å¾…å®Œæˆï¼Œç¡®ä¿ Profile æå–æˆåŠŸï¼‰
    if request.group_id:
        # ä» conversation_meta_raw_repository è·å– scene
        conversation_meta_repo = get_bean_by_type(ConversationMetaRawRepository)
        conversation_meta = await conversation_meta_repo.get_by_group_id(
            request.group_id
        )

        # å¦‚æœæ‰¾åˆ° conversation_metaï¼Œä½¿ç”¨å…¶ä¸­çš„ sceneï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ "assistant"
        if conversation_meta and conversation_meta.scene:
            scene = conversation_meta.scene
            logger.info(f"[mem_memorize] ä» conversation_meta è·å– scene: {scene}")
        else:
            scene = "assistant"  # é»˜è®¤åœºæ™¯ï¼Œå¯é€‰å€¼: ["assistant", "companion"]
            logger.warning(
                f"[mem_memorize] æœªæ‰¾åˆ° conversation_meta æˆ– scene ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ scene: {scene}"
            )

        await _trigger_clustering(request.group_id, memcell, scene)

    # è¯»å–è®°å¿†çš„æµç¨‹
    participants = []
    for memcell in memcells:
        if memcell.participants:
            participants.extend(memcell.participants)

    if if_memorize(memcells):
        # åŠ é”
        # ä½¿ç”¨çœŸå®Repositoryè¯»å–ç”¨æˆ·æ•°æ®
        old_memory_list = await load_core_memories(request, participants, current_time)

        episode_memories: List[Memory] = []
        semantic_memories: List[SemanticMemoryItem] = []
        event_logs: List[EventLog] = []

        # ç¬¬ä¸€é˜¶æ®µï¼šæå–ä¸ªäºº episode
        for memory_type in memory_types:
            if memory_type == MemoryType.EPISODIC_MEMORY:
                extracted_memories = await memory_manager.extract_memory(
                    memcell_list=memcells,
                    memory_type=memory_type,
                    user_ids=participants,
                    group_id=request.group_id,
                    group_name=request.group_name,
                    old_memory_list=old_memory_list,
                )
                if extracted_memories:
                    episode_memories = extracted_memories

        # å°† Episode è½¬æ¢ä¸º Doc å¹¶ä¿å­˜ï¼Œè·å– parent_docs_map
        parent_docs_map: Dict[str, Any] = {}
        episodic_source_memories: List[Memory] = (
            group_episode_memories + episode_memories
        )
        group_parent_event_id: Optional[str] = None

        if episodic_source_memories:
            for episode_mem in episodic_source_memories:
                if getattr(episode_mem, "group_name", None) is None:
                    episode_mem.group_name = request.group_name
                if getattr(episode_mem, "user_name", None) is None:
                    episode_mem.user_name = episode_mem.user_id
            episodic_docs = [
                _convert_episode_memory_to_doc(episode_mem, current_time)
                for episode_mem in episodic_source_memories
            ]
            episodic_payloads = [
                MemoryDocPayload(MemoryType.EPISODIC_MEMORY, doc)
                for doc in episodic_docs
            ]
            saved_docs_map = await save_memory_docs(episodic_payloads)
            saved_episode_docs = saved_docs_map.get(
                MemoryType.EPISODIC_MEMORY, []
            )
            for idx, (episode_mem, saved_doc) in enumerate(
                zip(episodic_source_memories, saved_episode_docs)
            ):
                episode_mem.event_id = str(saved_doc.event_id)
                parent_docs_map[str(saved_doc.event_id)] = saved_doc
                if group_parent_event_id is None and idx < len(
                    group_episode_memories
                ):
                    group_parent_event_id = str(saved_doc.event_id)
        else:
            group_parent_event_id = None

        # ç¬¬äºŒé˜¶æ®µï¼šåŸºäºå·²ä¿å­˜çš„ episode æå–è¯­ä¹‰è®°å¿†å’Œäº‹ä»¶æ—¥å¿—
        for memory_type in memory_types:
            if memory_type in [
                MemoryType.SEMANTIC_MEMORY,
                MemoryType.PERSONAL_EVENT_LOG,
            ]:
                # éå†æ‰€æœ‰å·²ä¿å­˜çš„ Episode (åŒ…æ‹¬ä¸ªäººå’Œç¾¤ç»„)
                for episode_mem in episodic_source_memories:
                    if not episode_mem.event_id:
                        continue
                    # è·³è¿‡ç¾¤ç»„ Episode (user_id=None),å› ä¸ºç¾¤ç»„çš„ semantic/eventlog ç›´æ¥ä» MemCell æå–
                    if episode_mem.user_id is None or episode_mem.user_id == "":
                        continue
                    
                    logger.info(f"ğŸ” ä¸º user_id={episode_mem.user_id} æå– {memory_type}")
                    extracted_memories = await memory_manager.extract_memory(
                        memcell_list=[],
                        memory_type=memory_type,
                        user_ids=[episode_mem.user_id],
                        episode_memory=episode_mem,
                    )
                    if not extracted_memories:
                        logger.warning(f"âš ï¸  æå–å¤±è´¥æˆ–ä¸ºç©º: user_id={episode_mem.user_id}, memory_type={memory_type}")
                        continue
                    logger.info(f"âœ… æˆåŠŸæå–: user_id={episode_mem.user_id}, memory_type={memory_type}, æ•°é‡={len(extracted_memories) if isinstance(extracted_memories, list) else 1}")

                    if memory_type == MemoryType.SEMANTIC_MEMORY:
                        for mem in extracted_memories:
                            mem.parent_event_id = episode_mem.event_id
                            mem.user_id = episode_mem.user_id
                            mem.group_id = episode_mem.group_id
                            mem.group_name = episode_mem.group_name
                            #TODO:æ·»åŠ  username
                            if getattr(mem, "user_name", None) is None:
                                mem.user_name = episode_mem.user_name
                            semantic_memories.append(mem)
                    elif memory_type == MemoryType.PERSONAL_EVENT_LOG:
                        extracted_memories.parent_event_id = episode_mem.event_id
                        extracted_memories.user_id = episode_mem.user_id
                        extracted_memories.group_id = episode_mem.group_id
                        extracted_memories.group_name = episode_mem.group_name
                        #TODO:æ·»åŠ  username
                        if getattr(extracted_memories, "user_name", None) is None:
                            extracted_memories.user_name = episode_mem.user_name
                        event_logs.append(extracted_memories)

        # è¿½åŠ ç¾¤ç»„å±‚é¢çš„è¯­ä¹‰è®°å¿†ä¸äº‹ä»¶æ—¥å¿—ï¼ˆç›´æ¥æ¥è‡ª MemCellï¼‰
        if group_parent_event_id:
            group_parent_doc = parent_docs_map.get(group_parent_event_id)
            if memcell.semantic_memories and group_parent_doc:
                for raw_sem in memcell.semantic_memories:
                    sem_item = _clone_semantic_memory_item(raw_sem)
                    sem_item.parent_event_id = group_parent_event_id
                    sem_item.user_id = None  # ç¾¤ç»„è¯­ä¹‰è®°å¿†çš„ user_id ä¸º None
                    sem_item.group_id = memcell.group_id
                    sem_item.group_name = memcell.group_name or request.group_name
                    sem_item.user_name = sem_item.group_name
                    semantic_memories.append(sem_item)

            if memcell.event_log:
                event_log_obj = _clone_event_log(memcell.event_log)
                if event_log_obj and event_log_obj.atomic_fact:
                    event_log_obj.parent_event_id = group_parent_event_id
                    event_log_obj.user_id = None  # ç¾¤ç»„äº‹ä»¶æ—¥å¿—çš„ user_id ä¸º None
                    event_log_obj.group_id = memcell.group_id
                    event_log_obj.group_name = memcell.group_name or request.group_name
                    event_log_obj.user_name = event_log_obj.group_name
                    event_logs.append(event_log_obj)

        # å°†è¯­ä¹‰è®°å¿†å’Œäº‹ä»¶æ—¥å¿—è½¬æ¢ä¸º Doc
        semantic_docs = []
        for sem_mem in semantic_memories:
            parent_doc = parent_docs_map.get(str(sem_mem.parent_event_id))
            if not parent_doc:
                logger.warning(
                    f"âš ï¸  æœªæ‰¾åˆ° parent_event_id={sem_mem.parent_event_id} å¯¹åº”çš„ episodic_memory"
                )
                continue
            doc = _convert_semantic_memory_to_doc(sem_mem, parent_doc, current_time)
            semantic_docs.append(doc)

        event_log_docs = []
        for event_log in event_logs:
            parent_doc = parent_docs_map.get(str(event_log.parent_event_id))
            if not parent_doc:
                logger.warning(
                    f"âš ï¸  æœªæ‰¾åˆ° parent_event_id={event_log.parent_event_id} å¯¹åº”çš„ episodic_memory"
                )
                continue
            docs = _convert_event_log_to_docs(event_log, parent_doc, current_time)
            event_log_docs.extend(docs)

        payloads: List[MemoryDocPayload] = []
        if semantic_docs:
            payloads.extend(
                MemoryDocPayload(MemoryType.SEMANTIC_MEMORY, doc)
                for doc in semantic_docs
            )
        if event_log_docs:
            payloads.extend(
                MemoryDocPayload(MemoryType.PERSONAL_EVENT_LOG, doc)
                for doc in event_log_docs
            )
        if payloads:
            await save_memory_docs(payloads)

        await update_status_after_memcell(
            request, memcells, current_time, request.raw_data_type
        )
        # TODO: å®é™…é¡¹ç›®ä¸­åº”è¯¥åŠ é”é¿å…å¹¶å‘é—®é¢˜
        # é‡Šæ”¾é”
        return episode_memories + semantic_memories + event_logs
       
    else:
        return None


def get_version_from_request(request: MemorizeOfflineRequest) -> str:
    # 1. è·å– memorize_to æ—¥æœŸ
    target_date = request.memorize_to

    # 2. å€’é€€ä¸€å¤©
    previous_day = target_date - timedelta(days=1)

    # 3. æ ¼å¼åŒ–ä¸º "YYYY-MM" å­—ç¬¦ä¸²
    return previous_day.strftime("%Y-%m")
